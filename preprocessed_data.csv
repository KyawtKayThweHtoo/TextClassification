Title,Abstract,Category,Cleaned_Title,Cleaned_Abstract
AI in health and medicine,"Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human–AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI’s potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide. AI has the potential to reshape medicine and make healthcare more accurate, efficient and accessible; this Review discusses recent progress, opportunities and challenges toward achieving this goal.",Artificial Intelligence,ai health medicin,artifici intellig ai pois broadli reshap medicin potenti improv experi clinician patient discuss key find 2year weekli effort track share key develop medic ai cover prospect studi advanc medic imag analysi reduc gap research deploy also address sever promis avenu novel medic ai research includ nonimag data sourc unconvent problem formul humanai collabor final consid seriou technic ethic challeng issu span data scarciti racial bia challeng address ai potenti may realiz make healthcar accur effici access patient worldwid ai potenti reshap medicin make healthcar accur effici access review discuss recent progress opportun challeng toward achiev goal
Learning Deep Architectures for AI,"Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",Artificial Intelligence,learn deep architectur ai,theoret result strongli suggest order learn kind complic function repres highlevel abstract eg vision languag ailevel task one need deep architectur deep architectur compos multipl level nonlinear oper neural net mani hidden layer complic proposit formula reus mani subformula search paramet space deep architectur difficult optim task learn algorithm deep belief network recent propos tackl problem notabl success beat stateoftheart certain area paper discuss motiv principl regard learn algorithm deep architectur particular exploit build block unsupervis learn singlelay model restrict boltzmann machin use construct deeper model deep belief network
"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed
appropriately, may deliver the best of expectations over many application sectors across the field. For this
to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability,
an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural
Networks) that were not present in the last hype of AI (namely, expert systems and rule based models).
Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely
acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in
this article examines the existing literature and contributions already done in the field of XAI, including a
prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define
explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that
covers such prior conceptual propositions with a major focus on the audience for which the explainability
is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including those aimed at explaining
Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This
critical literature analysis serves as the motivating background for a series of challenges faced by XAI,
such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept
of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI
methods in real organizations with fairness, model explainability and accountability at its core. Our
ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve
as reference material in order to stimulate future research advances, but also to encourage experts and
professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any
prior bias for its lack of interpretability",Artificial Intelligence,explain artifici intellig xai concept taxonomi opportun challeng toward respons ai,last year artifici intellig ai achiev notabl momentum har appropri may deliv best expect mani applic sector across field occur shortli machin learn entir commun stand front barrier explain inher problem latest techniqu brought subsymbol eg ensembl deep neural network present last hype ai name expert system rule base model paradigm underli problem fall within socal explain ai xai field wide acknowledg crucial featur practic deploy ai model overview present articl examin exist literatur contribut alreadi done field xai includ prospect toward yet reach purpos summar previou effort made defin explain machin learn establish novel definit explain machin learn cover prior conceptu proposit major focu audienc explain sought depart definit propos discuss taxonomi recent contribut relat explain differ machin learn model includ aim explain deep learn method second dedic taxonomi built examin detail critic literatur analysi serv motiv background seri challeng face xai interest crossroad data fusion explain prospect lead toward concept respons artifici intellig name methodolog largescal implement ai method real organ fair model explain account core ultim goal provid newcom field xai thorough taxonomi serv refer materi order stimul futur research advanc also encourag expert profession disciplin embrac benefit ai activ sector without prior bia lack interpret
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",Artificial Intelligence,hugginggpt solv ai task chatgpt friend hug face,solv complic ai task differ domain modal key step toward artifici gener intellig numer ai model avail variou domain modal handl complic ai task autonom consid larg languag model llm exhibit except abil languag understand gener interact reason advoc llm could act control manag exist ai model solv complic ai task languag serv gener interfac empow base philosophi present hugginggpt llmpower agent leverag llm eg chatgpt connect variou ai model machin learn commun eg hug face solv ai task specif use chatgpt conduct task plan receiv user request select model accord function descript avail hug face execut subtask select ai model summar respons accord execut result leverag strong languag capabl chatgpt abund ai model hug face hugginggpt tackl wide rang sophist ai task span differ modal domain achiev impress result languag vision speech challeng task pave new way toward realiz artifici gener intellig
Can AI-Generated Text be Reliably Detected?,"The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via human studies, and metrics such as perplexity scores and accuracy on text benchmarks. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",Artificial Intelligence,aigener text reliabl detect,unregul use llm potenti lead malici consequ plagiar gener fake news spam etc therefor reliabl detect aigener text critic ensur respons use llm recent work attempt tackl problem either use certain model signatur present gener text output appli watermark techniqu imprint specif pattern onto paper show detector reliabl practic scenario particular develop recurs paraphras attack appli ai text break whole rang detector includ one use watermark scheme well neural networkbas detector zeroshot classifi retrievalbas detector experi includ passag around 300 token length show sensit detector even case rel long passag also observ recurs paraphras degrad text qualiti slightli measur via human studi metric perplex score accuraci text benchmark addit show even llm protect watermark scheme vulner spoof attack aim mislead detector classifi humanwritten text aigener potenti caus reput damag develop particular show adversari infer hidden ai text signatur llm output without whitebox access detect method final provid theoret connect auroc best possibl detector total variat distanc human ai text distribut use studi fundament hard reliabl detect problem advanc languag model code publicli avail httpsgithubcomvinusankarsreliabilityofaitextdetector
From local explanations to global understanding with explainable AI for trees,"Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains. Tree-based machine learning models are widely used in domains such as healthcare, finance and public services. The authors present an explanation method for trees that enables the computation of optimal local explanations for individual predictions, and demonstrate their method on three medical datasets.",Artificial Intelligence,local explan global understand explain ai tree,treebas machin learn model random forest decis tree gradient boost tree popular nonlinear predict model yet compar littl attent paid explain predict improv interpret treebas model three main contribut 1 polynomi time algorithm comput optim explan base game theori 2 new type explan directli measur local featur interact effect 3 new set tool understand global model structur base combin mani local explan predict appli tool three medic machin learn problem show combin mani highqual local explan allow us repres global structur retain local faith origin model tool enabl us 1 identifi highmagnitud lowfrequ nonlinear mortal risk factor us popul 2 highlight distinct popul subgroup share risk characterist 3 identifi nonlinear interact effect among risk factor chronic kidney diseas 4 monitor machin learn model deploy hospit identifi featur degrad model perform time given popular treebas machin learn model improv interpret implic across broad set domain treebas machin learn model wide use domain healthcar financ public servic author present explan method tree enabl comput optim local explan individu predict demonstr method three medic dataset
Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning,"Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.",Artificial Intelligence,educ era gener artifici intellig ai understand potenti benefit chatgpt promot teach learn,sinc maiden releas public domain novemb 30 2022 chatgpt garner one million subscrib within week gener ai tool chatgpt took world surpris sophist capac carri remark complex task extraordinari abil chatgpt perform complex task within field educ caus mix feel among educ advanc ai seem revolution exist educ praxi exploratori studi synthes recent extant literatur offer potenti benefit drawback chatgpt promot teach learn benefit chatgpt includ limit promot person interact learn gener prompt form assess activ provid ongo feedback inform teach learn etc paper also highlight inher limit chatgpt gener wrong inform bias data train may augment exist bias privaci issu etc studi offer recommend chatgpt could leverag maxim teach learn polici maker research educ technolog expert could work togeth start convers evolv gener ai tool could use safe construct improv educ support student learn
Constitutional AI: Harmlessness from AI Feedback,"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",Artificial Intelligence,constitut ai harmless ai feedback,ai system becom capabl would like enlist help supervis ai experi method train harmless ai assist selfimprov without human label identifi harm output human oversight provid list rule principl refer method constitut ai process involv supervis learn reinforc learn phase supervis phase sampl initi model gener selfcritiqu revis finetun origin model revis respons rl phase sampl finetun model use model evalu two sampl better train prefer model dataset ai prefer train rl use prefer model reward signal ie use rl ai feedback rlaif result abl train harmless nonevas ai assist engag harm queri explain object sl rl method leverag chainofthought style reason improv humanjudg perform transpar ai decis make method make possibl control ai behavior precis far fewer human label
Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts,"Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",Artificial Intelligence,johnni cant prompt nonai expert tri fail design llm prompt,pretrain larg languag model llm like gpt3 engag fluent multiturn instructiontak outofthebox make attract materi design natur languag interact use natur languag steer llm output prompt emerg import design techniqu potenti access nonaiexpert craft effect prompt challeng howev promptbas interact brittl explor whether nonaiexpert success engag endus prompt engin use design probea prototyp llmbase chatbot design tool support develop systemat evalu prompt strategi ultim probe particip explor prompt design opportunist systemat struggl way echo endus program system interact machin learn system expect stem humantohuman instruct experi tendenc overgener barrier effect prompt design find implic nonaiexpertfac llmbase tool design improv llmandprompt literaci among programm public present opportun research
Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,"Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.",Artificial Intelligence,engin educ era chatgpt promis pitfal gener ai educ,engin educ constantli evolv keep latest technolog develop meet chang need engin industri one promis develop field use gener artifici intellig technolog chatgpt convers agent chatgpt potenti offer person effect learn experi provid student custom feedback explan well creat realist virtual simul handson learn howev import also consid limit technolog chatgpt gener ai system good train data may perpetu bias even gener spread misinform addit use gener ai educ rais ethic concern potenti uneth dishonest use student potenti unemploy human made redund technolog current state gener ai technolog repres chatgpt impress flaw preview come import engin educ understand implic technolog studi adapt engin educ ecosystem ensur next gener engin take advantag benefit offer gener ai minim neg consequ
Large Language Models can be Guided to Evade AI-Generated Text Detection,"Large language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation show that the SICO-generated text achieves human-level readability and task completion rates, while preserving high imperceptibility. Finally, we propose an ensemble approach to enhance the robustness of detectors against SICO attack. The code is publicly available at https://github.com/ColinLu50/Evade-GPT-Detector.",Artificial Intelligence,larg languag model guid evad aigener text detect,larg languag model llm shown remark perform variou task extens util public howev increas concern regard misus llm plagiar spam led develop multipl detector includ finetun classifi statist method studi equip llm prompt rather reli extern paraphras evalu vulner detector propos novel substitutionbas incontext exampl optim method sico automat construct prompt evad detector sico costeffici requir 40 humanwritten exampl limit number llm infer gener prompt moreov taskspecif prompt construct univers use wide rang detector extens experi across three realworld task demonstr sico significantli outperform paraphras baselin enabl gpt35 success evad six detector decreas auc 05 averag furthermor comprehens human evalu show sicogener text achiev humanlevel readabl task complet rate preserv high impercept final propos ensembl approach enhanc robust detector sico attack code publicli avail httpsgithubcomcolinlu50evadegptdetector
MEGA: Multilingual Evaluation of Generative AI,"Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",Artificial Intelligence,mega multilingu evalu gener ai,gener ai model shown impress perform mani natur languag process task languag understand reason languag gener import question ask ai commun today capabl limit model clear evalu gener ai challeng studi gener llm restrict english unclear capabl model understand gener text languag present first comprehens benchmark gener llm mega evalu model standard nlp benchmark cover 16 nlp dataset across 70 typolog divers languag compar perform gener llm includ chatgpt gpt4 state art sota nonautoregress model task determin well gener model perform compar previou gener llm present thorough analysi perform model across languag task discuss challeng improv perform gener llm lowresourc languag creat framework evalu gener llm multilingu set provid direct futur progress field
On the Reliability of Watermarks for Large Language Models,"As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",Artificial Intelligence,reliabl watermark larg languag model,llm becom commonplac machinegener text potenti flood internet spam social media bot valueless content watermark simpl effect strategi mitig harm enabl detect document llmgener text yet crucial question remain reliabl watermark realist set wild watermark text may modifi suit user need entir rewritten avoid detect studi robust watermark text rewritten human paraphras nonwatermark llm mix longer handwritten document find watermark remain detect even human machin paraphras attack dilut strength watermark paraphras statist like leak ngram even longer fragment origin text result highconfid detect enough token observ exampl strong human paraphras watermark detect observ 800 token averag set 1e5 fals posit rate also consid rang new detect scheme sensit short span watermark text embed insid larg document compar robust watermark kind detector
A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT,"Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.",Artificial Intelligence,comprehens survey aigener content aigc histori gener ai gan chatgpt,recent chatgpt along dalle2 codexha gain signific attent societi result mani individu becom interest relat resourc seek uncov background secret behind impress perform fact chatgpt gener ai gai techniqu belong categori artifici intellig gener content aigc involv creation digit content imag music natur languag ai model goal aigc make content creation process effici access allow product highqual content faster pace aigc achiev extract understand intent inform instruct provid human gener content accord knowledg intent inform recent year largescal model becom increasingli import aigc provid better intent extract thu improv gener result growth data size model distribut model learn becom comprehens closer realiti lead realist highqual content gener survey provid comprehens review histori gener model basic compon recent advanc aigc unimod interact multimod interact perspect unimod introduc gener task rel model text imag perspect multimod introduc crossappl modal mention final discuss exist open problem futur challeng aigc
The Impact of AI on Developer Productivity: Evidence from GitHub Copilot,"Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.",Artificial Intelligence,impact ai develop product evid github copilot,gener ai tool hold promis increas human product paper present result control experi github copilot ai pair programm recruit softwar develop ask implement http server javascript quickli possibl treatment group access ai pair programm complet task 558 faster control group observ heterogen effect show promis ai pair programm help peopl transit softwar develop career
Regulating ChatGPT and other Large Generative AI Models,"Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",Artificial Intelligence,regul chatgpt larg gener ai model,larg gener ai model lgaim chatgpt gpt4 stabl diffus rapidli transform way commun illustr creat howev ai regul eu beyond primarili focus convent ai model lgaim paper situat new gener model current debat trustworthi ai regul ask law tailor capabl lay technic foundat legal part paper proce four step cover 1 direct regul 2 data protect 3 content moder 4 polici propos suggest novel terminolog captur ai valu chain lgaim set differenti lgaim develop deploy profession nonprofession user well recipi lgaim output tailor regulatori duti differ actor along valu chain suggest strategi ensur lgaim trustworthi deploy benefit societi larg rule ai act direct regul must match specif pretrain model paper argu three layer oblig concern lgaim minimum standard lgaim highrisk oblig highrisk use case collabor along ai valu chain gener regul focu concret highrisk applic pretrain model includ oblig regard transpar ii risk manag nondiscrimin provis iii may howev appli lgaim develop lastli iv core dsa content moder rule expand cover lgaim includ notic action mechan trust flagger
Generative AI,"Recent advancements in generative artificial intelligence (AI) have made it possible for machines to independently produce a variety of creative content. In the context of producing creative content, this essay examines the developments, difficulties, and ethical issues relating to generative AI. It looks into how generative models, such Generative Adversarial Networks (GANs) and Variational Auto encoders (VAEs), can produce realistic artwork like music, literature, and visuals. However, it is frequently discovered that GAN training is extremely unstable and frequently experiences non-convergence, mode collapse, and hyperparameter sensitivity [1]. The technical details of developing and optimizing generative models to produce desired results are covered in detail in this work. It also looks at the difficulties in guaranteeing the variety, creativity, and coherence of generated content. Additionally, the use of generative AI in the creation of original material raises ethical questions. Included in this are concerns about intellectual property, plagiarism, and possible effects on the creative industries. In specifically, the article explores the consequences of employing generative AI for content production in terms of authorship, human creativity, and the possible disruption of traditional creative practices. It also covers issues with fairness, bias, and appropriate application of generative models.",Artificial Intelligence,gener ai,recent advanc gener artifici intellig ai made possibl machin independ produc varieti creativ content context produc creativ content essay examin develop difficulti ethic issu relat gener ai look gener model gener adversari network gan variat auto encod vae produc realist artwork like music literatur visual howev frequent discov gan train extrem unstabl frequent experi nonconverg mode collaps hyperparamet sensit 1 technic detail develop optim gener model produc desir result cover detail work also look difficulti guarante varieti creativ coher gener content addit use gener ai creation origin materi rais ethic question includ concern intellectu properti plagiar possibl effect creativ industri specif articl explor consequ employ gener ai content product term authorship human creativ possibl disrupt tradit creativ practic also cover issu fair bia appropri applic gener model
"Generative AI at Work
","We study the staggered introduction of a generative AI-based conversational assistant using data from 5,000 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14 percent on average, with the greatest impact on novice and low-skilled workers, and minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the potentially tacit knowledge of more able workers and helps newer workers move down the experience curve. In addition, we show that AI assistance improves customer sentiment, reduces requests for managerial intervention, and improves employee retention.",Artificial Intelligence,gener ai work,studi stagger introduct gener aibas convers assist use data 5000 custom support agent access tool increas product measur issu resolv per hour 14 percent averag greatest impact novic lowskil worker minim impact experienc highli skill worker provid suggest evid ai model dissemin potenti tacit knowledg abl worker help newer worker move experi curv addit show ai assist improv custom sentiment reduc request manageri intervent improv employe retent
A comprehensive AI policy education framework for university teaching and learning,"This study aims to develop an AI education policy for higher education by examining the perceptions and implications of text generative AI technologies. Data was collected from 457 students and 180 teachers and staff across various disciplines in Hong Kong universities, using both quantitative and qualitative research methods. Based on the findings, the study proposes an AI Ecological Education Policy Framework to address the multifaceted implications of AI integration in university teaching and learning. This framework is organized into three dimensions: Pedagogical, Governance, and Operational. The Pedagogical dimension concentrates on using AI to improve teaching and learning outcomes, while the Governance dimension tackles issues related to privacy, security, and accountability. The Operational dimension addresses matters concerning infrastructure and training. The framework fosters a nuanced understanding of the implications of AI integration in academic settings, ensuring that stakeholders are aware of their responsibilities and can take appropriate actions accordingly. Proposed AI Ecological Education Policy Framework for university teaching and learning. Three dimensions: Pedagogical, Governance, and Operational AI Policy Framework. Qualitative and quantitative data collected from students, teachers, and staff. Ten key areas identified for planning an AI policy in universities. Students should play an active role in drafting and implementing the policy. Proposed AI Ecological Education Policy Framework for university teaching and learning. Three dimensions: Pedagogical, Governance, and Operational AI Policy Framework. Qualitative and quantitative data collected from students, teachers, and staff. Ten key areas identified for planning an AI policy in universities. Students should play an active role in drafting and implementing the policy.",Artificial Intelligence,comprehens ai polici educ framework univers teach learn,studi aim develop ai educ polici higher educ examin percept implic text gener ai technolog data collect 457 student 180 teacher staff across variou disciplin hong kong univers use quantit qualit research method base find studi propos ai ecolog educ polici framework address multifacet implic ai integr univers teach learn framework organ three dimens pedagog govern oper pedagog dimens concentr use ai improv teach learn outcom govern dimens tackl issu relat privaci secur account oper dimens address matter concern infrastructur train framework foster nuanc understand implic ai integr academ set ensur stakehold awar respons take appropri action accordingli propos ai ecolog educ polici framework univers teach learn three dimens pedagog govern oper ai polici framework qualit quantit data collect student teacher staff ten key area identifi plan ai polici univers student play activ role draft implement polici propos ai ecolog educ polici framework univers teach learn three dimens pedagog govern oper ai polici framework qualit quantit data collect student teacher staff ten key area identifi plan ai polici univers student play activ role draft implement polici
Art and the science of generative AI,"Understanding shifts in creative work will help guide AI’s impact on the media ecosystem The capabilities of a new class of tools, colloquially known as generative artificial intelligence (AI), is a topic of much debate. One prominent application thus far is the production of high-quality artistic media for visual arts, concept art, music, and literature, as well as video and animation. For example, diffusion models can synthesize high-quality images (1), and large language models (LLMs) can produce sensible-sounding and impressive prose and verse in a wide range of contexts (2). The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI—and making policy decisions around it—requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity.",Artificial Intelligence,art scienc gener ai,understand shift creativ work help guid ai impact media ecosystem capabl new class tool colloqui known gener artifici intellig ai topic much debat one promin applic thu far product highqual artist media visual art concept art music literatur well video anim exampl diffus model synthes highqual imag 1 larg languag model llm produc sensiblesound impress prose vers wide rang context 2 gener capabl tool like fundament alter creativ process creator formul idea put product creativ reimagin may mani sector societi understand impact gener aiand make polici decis around itrequir new interdisciplinari scientif inquiri cultur econom law algorithm interact technolog creativ
Unlocking the Power of ChatGPT: A Framework for Applying Generative AI in Education,"Purpose Artificial intelligence (AI) chatbots, such as ChatGPT and GPT-4, developed by OpenAI, have the potential to revolutionize education. This study explores the potential benefits and challenges of using ChatGPT in education (or “educative AI”). Design/Approach/Methods This paper proposes a theoretical framework called “IDEE” for educative AI such as using ChatGPT and other generative AI in education, which includes identifying the desired outcomes, determining the appropriate level of automation, ensuring ethical considerations, and evaluating effectiveness. Findings The benefits of using ChatGPT in education or more generally, educative AI, include a more personalized and efficient learning experience for students as well as easier and faster feedback for teachers. However, challenges such as the untested effectiveness of the technology, limitations in the quality of data, and ethical and safety concerns must also be considered. Originality/Value This study explored the opportunities and challenges of using ChatGPT in education within the proposed theoretical framework.",Artificial Intelligence,unlock power chatgpt framework appli gener ai educ,purpos artifici intellig ai chatbot chatgpt gpt4 develop openai potenti revolution educ studi explor potenti benefit challeng use chatgpt educ educ ai designapproachmethod paper propos theoret framework call ide educ ai use chatgpt gener ai educ includ identifi desir outcom determin appropri level autom ensur ethic consider evalu effect find benefit use chatgpt educ gener educ ai includ person effici learn experi student well easier faster feedback teacher howev challeng untest effect technolog limit qualiti data ethic safeti concern must also consid originalityvalu studi explor opportun challeng use chatgpt educ within propos theoret framework
Revolutionizing education with AI: Exploring the transformative potential of ChatGPT,"Artificial intelligence (AI) introduces new tools to the educational environment with the potential to transform conventional teaching and learning processes. This study offers a comprehensive overview of AI technologies, their potential applications in education, and the difficulties involved. Chatbots and related algorithms that can simulate human interactions and generate human-like text based on input from natural language are discussed. In addition to the advantages of cutting-edge chatbots like ChatGPT, their use in education raises important ethical and practical challenges. The authors aim to provide insightful information on how AI may be successfully incorporated into the educational setting to benefit teachers and students, while promoting responsible and ethical use.",Artificial Intelligence,revolution educ ai explor transform potenti chatgpt,artifici intellig ai introduc new tool educ environ potenti transform convent teach learn process studi offer comprehens overview ai technolog potenti applic educ difficulti involv chatbot relat algorithm simul human interact gener humanlik text base input natur languag discuss addit advantag cuttingedg chatbot like chatgpt use educ rais import ethic practic challeng author aim provid insight inform ai may success incorpor educ set benefit teacher student promot respons ethic use
Towards Generalist Biomedical AI,"Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more. Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret this data at scale can potentially enable impactful applications ranging from scientific discovery to care delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering, mammography and dermatology image interpretation, radiology report generation and summarization, and genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights. Med-PaLM M reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin. We also report examples of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and emergent zero-shot medical reasoning. To further probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe encouraging performance across model scales. In a side-by-side ranking on 246 retrospective chest X-rays, clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases, suggesting potential clinical utility. While considerable work is needed to validate these models in real-world use cases, our results represent a milestone towards the development of generalist biomedical AI systems.",Artificial Intelligence,toward generalist biomed ai,medicin inher multimod rich data modal span text imag genom generalist biomed artifici intellig ai system flexibl encod integr interpret data scale potenti enabl impact applic rang scientif discoveri care deliveri enabl develop model first curat multimedbench new multimod biomed benchmark multimedbench encompass 14 divers task medic question answer mammographi dermatolog imag interpret radiolog report gener summar genom variant call introduc medpalm multimod medpalm proof concept generalist biomed ai system medpalm larg multimod gener model flexibl encod interpret biomed data includ clinic languag imag genom set model weight medpalm reach perform competit exceed state art multimedbench task often surpass specialist model wide margin also report exampl zeroshot gener novel medic concept task posit transfer learn across task emerg zeroshot medic reason probe capabl limit medpalm conduct radiologist evalu modelgener human chest xray report observ encourag perform across model scale sidebysid rank 246 retrospect chest xray clinician express pairwis prefer medpalm report produc radiologist 4050 case suggest potenti clinic util consider work need valid model realworld use case result repres mileston toward develop generalist biomed ai system
Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality,"The public release of Large Language Models (LLMs) has sparked tremendous interest in how humans will use Artificial Intelligence (AI) to accomplish a variety of tasks. In our study conducted with Boston Consulting Group, a global management consulting firm, we examine the performance implications of AI on realistic, complex, and knowledge-intensive tasks. The pre-registered experiment involved 758 consultants comprising about 7% of the individual contributor-level consultants at the company. After establishing a performance baseline on a similar task, subjects were randomly assigned to one of three conditions: no AI access, GPT-4 AI access, or GPT-4 AI access with a prompt engineering overview. We suggest that the capabilities of AI create a “jagged technological frontier” where some tasks are easily done by AI, while others, though seemingly similar in difficulty level, are outside the current capability of AI. For each one of a set of 18 realistic consulting tasks within the frontier of AI capabilities, consultants using AI were significantly more productive (they completed 12.2% more tasks on average, and completed tasks 25.1% more quickly), and produced significantly higher quality results (more than 40% higher quality compared to a control group). Consultants across the skills distribution benefited significantly from having AI augmentation, with those below the average performance threshold increasing by 43% and those above increasing by 17% compared to their own scores. For a task selected to be outside the frontier, however, consultants using AI were 19 percentage points less likely to produce correct solutions compared to those without AI. Further, our analysis shows the emergence of two distinctive patterns of successful AI use by humans along a spectrum of humanAI integration. One set of consultants acted as “Centaurs,” like the mythical halfhorse/half-human creature, dividing and delegating their solution-creation activities to the AI or to themselves. Another set of consultants acted more like “Cyborgs,” completely integrating their task flow with the AI and continually interacting with the technology.",Artificial Intelligence,navig jag technolog frontier field experiment evid effect ai knowledg worker product qualiti,public releas larg languag model llm spark tremend interest human use artifici intellig ai accomplish varieti task studi conduct boston consult group global manag consult firm examin perform implic ai realist complex knowledgeintens task preregist experi involv 758 consult compris 7 individu contributorlevel consult compani establish perform baselin similar task subject randomli assign one three condit ai access gpt4 ai access gpt4 ai access prompt engin overview suggest capabl ai creat jag technolog frontier task easili done ai other though seemingli similar difficulti level outsid current capabl ai one set 18 realist consult task within frontier ai capabl consult use ai significantli product complet 122 task averag complet task 251 quickli produc significantli higher qualiti result 40 higher qualiti compar control group consult across skill distribut benefit significantli ai augment averag perform threshold increas 43 increas 17 compar score task select outsid frontier howev consult use ai 19 percentag point less like produc correct solut compar without ai analysi show emerg two distinct pattern success ai use human along spectrum humanai integr one set consult act centaur like mythic halfhorsehalfhuman creatur divid deleg solutioncr activ ai anoth set consult act like cyborg complet integr task flow ai continu interact technolog
International evaluation of an AI system for breast cancer screening,"Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7% and 1.2% (USA and UK) in false positives and 9.4% and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening. An artificial intelligence (AI) system performs as well as or better than radiologists at detecting breast cancer from mammograms, and using a combination of AI and human inputs could help to improve screening efficiency.",Artificial Intelligence,intern evalu ai system breast cancer screen,screen mammographi aim identifi breast cancer earlier stage diseas treatment successful1 despit exist screen programm worldwid interpret mammogram affect high rate fals posit fals negatives2 present artifici intellig ai system capabl surpass human expert breast cancer predict assess perform clinic set curat larg repres dataset uk larg enrich dataset usa show absolut reduct 57 12 usa uk fals posit 94 27 fals neg provid evid abil system gener uk usa independ studi six radiologist ai system outperform human reader area receiv oper characterist curv aucroc ai system greater aucroc averag radiologist absolut margin 115 ran simul ai system particip doubleread process use uk found ai system maintain noninferior perform reduc workload second reader 88 robust assess ai system pave way clinic trial improv accuraci effici breast cancer screen artifici intellig ai system perform well better radiologist detect breast cancer mammogram use combin ai human input could help improv screen effici
Teachers’ AI digital competencies and twenty-first century skills in the post-pandemic world,"The pandemic has catalyzed a significant shift to online/blended teaching and learning where teachers apply emerging technologies to enhance their students’ learning outcomes. Artificial intelligence (AI) technology has gained its popularity in online learning environments during the pandemic to assist students’ learning. However, many of these AI tools are new to teachers. They may not have rich technical knowledge to use AI educational applications to facilitate their teaching, not to mention developing students’ AI digital capabilities. As such, there is a growing need for teachers to equip themselves with adequate digital competencies so as to use and teach AI in their teaching environments. There are few existing frameworks informing teachers of necessary AI competencies. This study first explores the opportunities and challenges of employing AI systems and how they can enhance teaching, learning and assessment. Then, aligning with generic digital competency frameworks, the DigCompEdu framework and P21’s framework for twenty-first century learning were adapted and revised to accommodate AI technologies. Recommendations are proposed to support educators and researchers to promote AI education in their classrooms and academia.",Artificial Intelligence,teacher ai digit compet twentyfirst centuri skill postpandem world,pandem catalyz signific shift onlineblend teach learn teacher appli emerg technolog enhanc student learn outcom artifici intellig ai technolog gain popular onlin learn environ pandem assist student learn howev mani ai tool new teacher may rich technic knowledg use ai educ applic facilit teach mention develop student ai digit capabl grow need teacher equip adequ digit compet use teach ai teach environ exist framework inform teacher necessari ai compet studi first explor opportun challeng employ ai system enhanc teach learn assess align gener digit compet framework digcompedu framework p21 framework twentyfirst centuri learn adapt revis accommod ai technolog recommend propos support educ research promot ai educ classroom academia
"AI in teacher education: Unlocking new dimensions in teaching support, inclusive learning, and digital literacy","AI can positively influence teaching by offering support for classroom management, creating inclusive learning environments, enhancing digital skills, personalizing teaching methods, and strengthening teacher‐student relationships.This quantitative research study investigates the opportunities, difficulties, and consequences of incorporating AI into teacher education.Data were collected through structured questionnaires from 202 college students and 68 staff members. The analysis was conducted using SPSS software.The study provides a novel contribution by its thorough investigation of the diverse effects of AI on teacher education. It offers beneficial perspectives on the possible benefits and challenges, illuminating the far‐reaching changes that AI could bring to the terrain of learning and instruction and teaching methods in the time yet to come. The research sought to assess the effect of AI adoption in teacher education across five main dimensions: (i) its influence on teaching support and classroom management, (ii) its role in creating inclusive and accessible learning environments, (iii) its contribution to improving teachers' digital literacy and computer skills, and enhancing access to digital teaching resources, (iv) its positive influence on identifying students' learning styles and facilitating the adoption of diverse teaching methods, and (v) its role in strengthening teacher‐student relationships through improved interactions.The findings elucidate the promising opportunities that AI presents in the field of teacher education, along with the obstacles that require resolution for the effective fusion of AI educational settings.",Artificial Intelligence,ai teacher educ unlock new dimens teach support inclus learn digit literaci,ai posit influenc teach offer support classroom manag creat inclus learn environ enhanc digit skill person teach method strengthen teacherstud relationshipsthi quantit research studi investig opportun difficulti consequ incorpor ai teacher educationdata collect structur questionnair 202 colleg student 68 staff member analysi conduct use spss softwareth studi provid novel contribut thorough investig divers effect ai teacher educ offer benefici perspect possibl benefit challeng illumin farreach chang ai could bring terrain learn instruct teach method time yet come research sought assess effect ai adopt teacher educ across five main dimens influenc teach support classroom manag ii role creat inclus access learn environ iii contribut improv teacher digit literaci comput skill enhanc access digit teach resourc iv posit influenc identifi student learn style facilit adopt divers teach method v role strengthen teacherstud relationship improv interactionsth find elucid promis opportun ai present field teacher educ along obstacl requir resolut effect fusion ai educ set
A systematic review of Green AI,"With the ever‐growing adoption of artificial intelligence (AI)‐based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this article, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm‐agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115%, with savings over 50% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.",Artificial Intelligence,systemat review green ai,evergrow adopt artifici intellig aibas system carbon footprint ai longer neglig ai research practition therefor urg hold account carbon emiss ai model design use led recent year appear research tackl ai environment sustain field refer green ai despit rapid growth interest topic comprehens overview green ai research date still miss address gap articl present systemat review green ai literatur analysi 98 primari studi differ pattern emerg topic experienc consider growth 2020 onward studi consid monitor ai model footprint tune hyperparamet improv model sustain benchmark model mix posit paper observ studi solut paper present paper focu train phase algorithmagnost studi neural network use imag data laboratori experi common research strategi report green ai energi save go 115 save 50 rather common industri parti involv green ai studi albeit target academ reader green ai tool provis scarc conclus green ai research field result reach consider level matur therefor review emerg time suitabl adopt green ai research strategi port numer promis academ result industri practic
AI Art and its Impact on Artists,"The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial “generative AI Art” products have entered the market, making generative AI an estimated $48B industry [125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.",Artificial Intelligence,ai art impact artist,last 3 year result machin learn mlbase imag gener abil output consist higher qualiti imag base natur languag prompt input result mani popular commerci gener ai art product enter market make gener ai estim 48b industri 125 howev mani profession artist spoken harm experienc due prolifer larg scale imag gener train imagetext pair internet paper review harm includ reput damag econom loss plagiar copyright infring guard issu reap potenti benefit imag gener provid recommend regul forc organ disclos train data tool help artist prevent use content train data without consent
Can AI Help in Screening Viral and COVID-19 Pneumonia?,"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.",Artificial Intelligence,ai help screen viral covid19 pneumonia,coronaviru diseas covid19 pandem diseas alreadi caus thousand causal infect sever million peopl worldwid technolog tool enabl rapid screen covid19 infect high accuraci crucial help healthcar profession main clinic tool current use diagnosi covid19 revers transcript polymeras chain reaction rtpcr expens lesssensit requir special medic personnel xray imag easili access tool excel altern covid19 diagnosi research taken investig util artifici intellig ai rapid accur detect covid19 chest xray imag aim paper propos robust techniqu automat detect covid19 pneumonia digit chest xray imag appli pretrain deeplearn algorithm maxim detect accuraci public databas creat author combin sever public databas also collect imag recent publish articl databas contain mixtur 423 covid19 1485 viral pneumonia 1579 normal chest xray imag transfer learn techniqu use help imag augment train valid sever pretrain deep convolut neural network cnn network train classifi two differ scheme normal covid19 pneumonia ii normal viral covid19 pneumonia without imag augment classif accuraci precis sensit specif scheme 997 997 997 9955 979 9795 979 988 respect high accuraci computeraid diagnost tool significantli improv speed accuraci covid19 diagnosi would extrem use pandem diseas burden need prevent measur odd avail resourc
"Democratising AI: Multiple Meanings, Goals, and Methods","Numerous parties are calling for “the democratisation of AI”, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of “AI democratisation” that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to “democratising AI”, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.",Artificial Intelligence,democratis ai multipl mean goal method,numer parti call democratis ai phrase use refer varieti goal pursuit sometim conflict paper identifi four kind ai democratis commonli discuss 1 democratis ai use 2 democratis ai develop 3 democratis ai profit 4 democratis ai govern numer goal method achiev form democratis discuss main takeaway paper ai democratis multifari sometim conflict concept conflat improv ai access want move beyond ambigu commit democratis ai product discuss concret polici tradeoff need recognis princip role democratis ai govern navig tradeoff risk across decis around use develop profit
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making,"The current literature on AI‐advised decision making—involving explainable AI systems advising human decision makers—presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. In contrast to other common desiderata, for example, interpretability or spelling out the AI's reasoning process, we argue that explanations are only useful to the extent that they allow a human decision maker to verify the correctness of the AI's prediction. Prior studies find in many decision making contexts that AI explanations do not facilitate such verification. Moreover, most tasks fundamentally do not allow easy verification, regardless of explanation method, limiting the potential benefit of any type of explanation. We also compare the objective of complementary performance with that of appropriate reliance, decomposing the latter into the notions of outcome‐graded and strategy‐graded reliance.",Artificial Intelligence,search verifi explan rare enabl complementari perform aiadvis decis make,current literatur aiadvis decis makinginvolv explain ai system advis human decis makerspres seri inconclus confound result synthes find propos simpl theori elucid frequent failur ai explan engend appropri relianc complementari decis make perform contrast common desiderata exampl interpret spell ai reason process argu explan use extent allow human decis maker verifi correct ai predict prior studi find mani decis make context ai explan facilit verif moreov task fundament allow easi verif regardless explan method limit potenti benefit type explan also compar object complementari perform appropri relianc decompos latter notion outcomegrad strategygrad relianc
Habitat: A Platform for Embodied AI Research,"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",Artificial Intelligence,habitat platform embodi ai research,present habitat platform research embodi artifici intellig ai habitat enabl train embodi agent virtual robot highli effici photorealist 3d simul specif habitat consist habitatsim flexibl highperform 3d simul configur agent sensor gener 3d dataset handl habitatsim fast render scene matterport3d achiev sever thousand frame per second fp run singlethread reach 10000 fp multiprocess singl gpu ii habitatapi modular highlevel librari endtoend develop embodi ai algorithm defin task eg navig instruct follow question answer configur train benchmark embodi agent largescal engin contribut enabl us answer scientif question requir experi till impractic mere impract specif context pointgoal navig 1 revisit comparison learn slam approach two recent work find evid opposit conclus learn outperform slam scale order magnitud experi previou investig 2 conduct first crossdataset gener experi train test x matterport3d gibson multipl sensor blind rgb rgbd find agent depth sensor gener across dataset hope opensourc platform find advanc research embodi ai
Fusing Blockchain and AI With Metaverse: A Survey,"Metaverse as the latest buzzword has attracted great attention from both industry and academia. Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social networking, and trading. Thus, it is promising to build an exciting digital world and to transform a better physical world through the exploration of the metaverse. In this survey, we dive into the metaverse by discussing how Blockchain and Artificial Intelligence (AI) fuse with it through investigating the state-of-the-art studies across the metaverse components, digital currencies, AI applications in the virtual world, and blockchain-empowered technologies. Further exploitation and interdisciplinary research on the fusion of AI and Blockchain towards metaverse will definitely require collaboration from both academia and industries. We wish that our survey can help researchers, engineers, and educators build an open, fair, and rational future metaverse.",Artificial Intelligence,fuse blockchain ai metavers survey,metavers latest buzzword attract great attent industri academia metavers seamlessli integr real world virtual world allow avatar carri rich activ includ creation display entertain social network trade thu promis build excit digit world transform better physic world explor metavers survey dive metavers discuss blockchain artifici intellig ai fuse investig stateoftheart studi across metavers compon digit currenc ai applic virtual world blockchainempow technolog exploit interdisciplinari research fusion ai blockchain toward metavers definit requir collabor academia industri wish survey help research engin educ build open fair ration futur metavers
"AI-Based Modeling: Techniques, Applications and Research Issues Towards Automation, Intelligent and Smart Systems","Artificial intelligence (AI) is a leading technology of the current age of the Fourth Industrial Revolution (Industry 4.0 or 4IR), with the capability of incorporating human behavior and intelligence into machines or systems. Thus, AI-based modeling is the key to build automated, intelligent, and smart systems according to today’s needs. To solve real-world issues, various types of AI such as analytical, functional, interactive, textual, and visual AI can be applied to enhance the intelligence and capabilities of an application. However, developing an effective AI model is a challenging task due to the dynamic nature and variation in real-world problems and data. In this paper, we present a comprehensive view on “AI-based Modeling” with the principles and capabilities of potential AI techniques that can play an important role in developing intelligent and smart systems in various real-world application areas including business, finance, healthcare, agriculture, smart cities, cybersecurity and many more. We also emphasize and highlight the research issues within the scope of our study. Overall, the goal of this paper is to provide a broad overview of AI-based modeling that can be used as a reference guide by academics and industry people as well as decision-makers in various real-world scenarios and application domains.",Artificial Intelligence,aibas model techniqu applic research issu toward autom intellig smart system,artifici intellig ai lead technolog current age fourth industri revolut industri 40 4ir capabl incorpor human behavior intellig machin system thu aibas model key build autom intellig smart system accord today need solv realworld issu variou type ai analyt function interact textual visual ai appli enhanc intellig capabl applic howev develop effect ai model challeng task due dynam natur variat realworld problem data paper present comprehens view aibas model principl capabl potenti ai techniqu play import role develop intellig smart system variou realworld applic area includ busi financ healthcar agricultur smart citi cybersecur mani also emphas highlight research issu within scope studi overal goal paper provid broad overview aibas model use refer guid academ industri peopl well decisionmak variou realworld scenario applic domain
"Advances, challenges and opportunities in creating data for trustworthy AI","As artificial intelligence (AI) transitions from research to deployment, creating the appropriate datasets and data pipelines to develop and evaluate AI models is increasingly the biggest challenge. Automated AI model builders that are publicly available can now achieve top performance in many applications. In contrast, the design and sculpting of the data used to develop AI often rely on bespoke manual work, and they critically affect the trustworthiness of the model. This Perspective discusses key considerations for each stage of the data-for-AI pipeline—starting from data design to data sculpting (for example, cleaning, valuation and annotation) and data evaluation—to make AI more reliable. We highlight technical advances that help to make the data-for-AI pipeline more scalable and rigorous. Furthermore, we discuss how recent data regulations and policies can impact AI. It has become rapidly clear in the past few years that the creation, use and maintenance of high-quality annotated datasets for robust and reliable AI applications requires careful attention. This Perspective discusses challenges, considerations and best practices for various stages in the data-to-AI pipeline, to encourage a more data-centric approach.",Artificial Intelligence,advanc challeng opportun creat data trustworthi ai,artifici intellig ai transit research deploy creat appropri dataset data pipelin develop evalu ai model increasingli biggest challeng autom ai model builder publicli avail achiev top perform mani applic contrast design sculpt data use develop ai often reli bespok manual work critic affect trustworthi model perspect discuss key consider stage dataforai pipelinestart data design data sculpt exampl clean valuat annot data evaluationto make ai reliabl highlight technic advanc help make dataforai pipelin scalabl rigor furthermor discuss recent data regul polici impact ai becom rapidli clear past year creation use mainten highqual annot dataset robust reliabl ai applic requir care attent perspect discuss challeng consider best practic variou stage datatoai pipelin encourag datacentr approach
Artificial Intelligence (AI) and Internet of Medical Things (IoMT) Assisted Biomedical Systems for Intelligent Healthcare,"Artificial intelligence (AI) is a modern approach based on computer science that develops programs and algorithms to make devices intelligent and efficient for performing tasks that usually require skilled human intelligence. AI involves various subsets, including machine learning (ML), deep learning (DL), conventional neural networks, fuzzy logic, and speech recognition, with unique capabilities and functionalities that can improve the performances of modern medical sciences. Such intelligent systems simplify human intervention in clinical diagnosis, medical imaging, and decision-making ability. In the same era, the Internet of Medical Things (IoMT) emerges as a next-generation bio-analytical tool that combines network-linked biomedical devices with a software application for advancing human health. In this review, we discuss the importance of AI in improving the capabilities of IoMT and point-of-care (POC) devices used in advanced healthcare sectors such as cardiac measurement, cancer diagnosis, and diabetes management. The role of AI in supporting advanced robotic surgeries developed for advanced biomedical applications is also discussed in this article. The position and importance of AI in improving the functionality, detection accuracy, decision-making ability of IoMT devices, and evaluation of associated risks assessment is discussed carefully and critically in this review. This review also encompasses the technological and engineering challenges and prospects for AI-based cloud-integrated personalized IoMT devices for designing efficient POC biomedical systems suitable for next-generation intelligent healthcare.",Artificial Intelligence,artifici intellig ai internet medic thing iomt assist biomed system intellig healthcar,artifici intellig ai modern approach base comput scienc develop program algorithm make devic intellig effici perform task usual requir skill human intellig ai involv variou subset includ machin learn ml deep learn dl convent neural network fuzzi logic speech recognit uniqu capabl function improv perform modern medic scienc intellig system simplifi human intervent clinic diagnosi medic imag decisionmak abil era internet medic thing iomt emerg nextgener bioanalyt tool combin networklink biomed devic softwar applic advanc human health review discuss import ai improv capabl iomt pointofcar poc devic use advanc healthcar sector cardiac measur cancer diagnosi diabet manag role ai support advanc robot surgeri develop advanc biomed applic also discuss articl posit import ai improv function detect accuraci decisionmak abil iomt devic evalu associ risk assess discuss care critic review review also encompass technolog engin challeng prospect aibas cloudintegr person iomt devic design effici poc biomed system suitabl nextgener intellig healthcar
"Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI","AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.",Artificial Intelligence,everyon want model work data work data cascad highstak ai,ai model increasingli appli highstak domain like health conserv data qualiti carri elev signific highstak ai due heighten downstream impact impact predict like cancer detect wildlif poach loan alloc paradox data undervalu deglamoris aspect ai paper report data practic highstak ai interview 53 ai practition india east west african countri usa defin identifi present empir evid data cascadescompound event caus neg downstream effect data issuestrigg convent aiml practic undervalu data qualiti data cascad pervas 92 preval invis delay often avoid discuss hci opportun design incentiv data excel firstclass citizen ai result safer robust system
Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",Artificial Intelligence,explain ai review machin learn interpret method,recent advanc artifici intellig ai led widespread industri adopt machin learn system demonstr superhuman perform signific number task howev surg perform often achiev increas model complex turn system black box approach caus uncertainti regard way oper ultim way come decis ambigu made problemat machin learn system adopt sensit yet critic domain valu could immens healthcar result scientif interest field explain artifici intellig xai field concern develop new method explain interpret machin learn model tremend reignit recent year studi focus machin learn interpret method specif literatur review taxonomi method present well link program implement hope survey would serv refer point theorist practition
Reporting guideline for the early stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI,"A growing number of artificial intelligence (AI)-based clinical decision support systems are showing promising performance in preclinical, in silico, evaluation, but few have yet demonstrated real benefit to patient care. Early stage clinical evaluation is important to assess an AI system’s actual clinical performance at small scale, ensure its safety, evaluate the human factors surrounding its use, and pave the way to further large scale trials. However, the reporting of these early studies remains inadequate. The present statement provides a multistakeholder, consensus-based reporting guideline for the Developmental and Exploratory Clinical Investigations of DEcision support systems driven by Artificial Intelligence (DECIDE-AI). We conducted a two round, modified Delphi process to collect and analyse expert opinion on the reporting of early clinical evaluation of AI systems. Experts were recruited from 20 predefined stakeholder categories. The final composition and wording of the guideline was determined at a virtual consensus meeting. The checklist and the Explanation & Elaboration (E&E) sections were refined based on feedback from a qualitative evaluation process. 123 experts participated in the first round of Delphi, 138 in the second, 16 in the consensus meeting, and 16 in the qualitative evaluation. The DECIDE-AI reporting guideline comprises 17 AI specific reporting items (made of 28 subitems) and 10 generic reporting items, with an E&E paragraph provided for each. Through consultation and consensus with a range of stakeholders, we have developed a guideline comprising key items that should be reported in early stage clinical studies of AI-based decision support systems in healthcare. By providing an actionable checklist of minimal reporting items, the DECIDE-AI guideline will facilitate the appraisal of these studies and replicability of their findings.",Artificial Intelligence,report guidelin earli stage clinic evalu decis support system driven artifici intellig decideai,grow number artifici intellig aibas clinic decis support system show promis perform preclin silico evalu yet demonstr real benefit patient care earli stage clinic evalu import assess ai system actual clinic perform small scale ensur safeti evalu human factor surround use pave way larg scale trial howev report earli studi remain inadequ present statement provid multistakehold consensusbas report guidelin development exploratori clinic investig decis support system driven artifici intellig decideai conduct two round modifi delphi process collect analys expert opinion report earli clinic evalu ai system expert recruit 20 predefin stakehold categori final composit word guidelin determin virtual consensu meet checklist explan elabor ee section refin base feedback qualit evalu process 123 expert particip first round delphi 138 second 16 consensu meet 16 qualit evalu decideai report guidelin compris 17 ai specif report item made 28 subitem 10 gener report item ee paragraph provid consult consensu rang stakehold develop guidelin compris key item report earli stage clinic studi aibas decis support system healthcar provid action checklist minim report item decideai guidelin facilit apprais studi replic find
Multimodal biomedical AI,"The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health. Multimodal artificial intelligence models could unlock many exciting applications in health an",Artificial Intelligence,multimod biomed ai,increas avail biomed data larg biobank electron health record medic imag wearabl ambient biosensor lower cost genom microbiom sequenc set stage develop multimod artifici intellig solut captur complex human health diseas review outlin key applic enabl along technic analyt challeng explor opportun person medicin digit clinic trial remot monitor care pandem surveil digit twin technolog virtual health assist survey data model privaci challeng must overcom realiz full potenti multimod artifici intellig health multimod artifici intellig model could unlock mani excit applic health
Opportunities and Adoption Challenges of AI in the Construction Industry: A PRISMA Review,"Artificial intelligence (AI) is a powerful technology with a range of capabilities, which are beginning to become apparent in all industries nowadays. The increased popularity of AI in the construction industry, however, is rather limited in comparison to other industry sectors. Moreover, despite AI being a hot topic in built environment research, there are limited review studies that investigate the reasons for the low-level AI adoption in the construction industry. This study aims to reduce this gap by identifying the adoption challenges of AI, along with the opportunities offered, for the construction industry. To achieve the aim, the study adopts a systematic literature review approach using the PRISMA protocol. In addition, the systematic review of the literature focuses on the planning, design, and construction stages of the construction project lifecycle. The results of the review reveal that (a) AI is particularly beneficial in the planning stage as the success of construction projects depends on accurate events, risks, and cost forecasting; (b) the major opportunity in adopting AI is to reduce the time spent on repetitive tasks by using big data analytics and improving the work processes; and (c) the biggest challenge to incorporate AI on a construction site is the fragmented nature of the industry, which has resulted in issues of data acquisition and retention. The findings of the study inform a range of parties that operate in the construction industry concerning the opportunities and challenges of AI adaptability and help increase the market acceptance of AI practices",Artificial Intelligence,opportun adopt challeng ai construct industri prisma review,artifici intellig ai power technolog rang capabl begin becom appar industri nowaday increas popular ai construct industri howev rather limit comparison industri sector moreov despit ai hot topic built environ research limit review studi investig reason lowlevel ai adopt construct industri studi aim reduc gap identifi adopt challeng ai along opportun offer construct industri achiev aim studi adopt systemat literatur review approach use prisma protocol addit systemat review literatur focus plan design construct stage construct project lifecycl result review reveal ai particularli benefici plan stage success construct project depend accur event risk cost forecast b major opportun adopt ai reduc time spent repetit task use big data analyt improv work process c biggest challeng incorpor ai construct site fragment natur industri result issu data acquisit retent find studi inform rang parti oper construct industri concern opportun challeng ai adapt help increas market accept ai practic
The global landscape of AI ethics guidelines,"In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies. As AI technology develops rapidly, it is widely recognized that ethical guidelines are required for safe and fair implementation in society. But is it possible to agree on what is ‘ethical AI’? A detailed analysis of 84 AI ethics reports around the world, from national and international organizations, companies and institutes, explores this question, finding a convergence around core principles but substantial divergence on practical implementation.",Artificial Intelligence,global landscap ai ethic guidelin,past five year privat compani research institut public sector organ issu principl guidelin ethic artifici intellig ai howev despit appar agreement ai ethic debat constitut ethic ai ethic requir technic standard best practic need realiz investig whether global agreement question emerg map analys current corpu principl guidelin ethic ai result reveal global converg emerg around five ethic principl transpar justic fair nonmalefic respons privaci substant diverg relat principl interpret deem import issu domain actor pertain implement find highlight import integr guidelinedevelop effort substant ethic analysi adequ implement strategi ai technolog develop rapidli wide recogn ethic guidelin requir safe fair implement societi possibl agre ethic ai detail analysi 84 ai ethic report around world nation intern organ compani institut explor question find converg around core principl substanti diverg practic implement
A Survey on the Convergence of Edge Computing and AI for UAVs: Opportunities and Challenges,"The latest 5G mobile networks have enabled many exciting Internet of Things (IoT) applications that employ unmanned aerial vehicles (UAVs/drones). The success of most UAV-based IoT applications is heavily dependent on artificial intelligence (AI) technologies, for instance, computer vision and path planning. These AI methods must process data and provide decisions while ensuring low latency and low energy consumption. However, the existing cloud-based AI paradigm finds it difficult to meet these strict UAV requirements. Edge AI, which runs AI on-device or on edge servers close to users, can be suitable for improving UAV-based IoT services. This article provides a comprehensive analysis of the impact of edge AI on key UAV technical aspects (i.e., autonomous navigation, formation control, power management, security and privacy, computer vision, and communication) and applications (i.e., delivery systems, civil infrastructure inspection, precision agriculture, search and rescue (SAR) operations, acting as aerial wireless base stations (BSs), and drone light shows). As guidance for researchers and practitioners, this article also explores UAV-based edge AI implementation challenges, lessons learned, and future research directions.",Artificial Intelligence,survey converg edg comput ai uav opportun challeng,latest 5g mobil network enabl mani excit internet thing iot applic employ unman aerial vehicl uavsdron success uavbas iot applic heavili depend artifici intellig ai technolog instanc comput vision path plan ai method must process data provid decis ensur low latenc low energi consumpt howev exist cloudbas ai paradigm find difficult meet strict uav requir edg ai run ai ondevic edg server close user suitabl improv uavbas iot servic articl provid comprehens analysi impact edg ai key uav technic aspect ie autonom navig format control power manag secur privaci comput vision commun applic ie deliveri system civil infrastructur inspect precis agricultur search rescu sar oper act aerial wireless base station bss drone light show guidanc research practition articl also explor uavbas edg ai implement challeng lesson learn futur research direct
Queer In AI: A Case Study in Community-Led Participatory AI,"Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.",Artificial Intelligence,queer ai case studi communityl participatori ai,queer queer peopl face uncertain futur face ever wide deploy invas artifici intellig ai technolog caus numer harm queer peopl includ privaci violat censor downrank queer content expos queer peopl space harass make hypervis deadnam outing queer peopl broadli violat core tenet queer classifi control queer ident respons queer commun ai organ queer ai global decentr volunteerrun grassroot organ employ intersect communityl participatori design build inclus equit ai futur paper present queer ai case studi communityl participatori design ai examin participatori design intersect tenet start shape commun program year discuss differ challeng emerg process look way organ fallen short operation participatori intersect principl assess organ impact queer ai provid import lesson insight practition theorist participatori method broadli reject hierarchi favor decentr success build aid program queer commun effort chang actor institut outsid queer commun final theoriz commun like queer ai contribut participatori design ai broadli foster cultur particip ai welcom empow margin particip critiqu poor exploit participatori practic bring particip institut outsid individu research project queer ai work serv case studi grassroot activ participatori method within ai demonstr potenti communityl participatori method intersect praxi also provid challeng case studi nuanc insight research develop use participatori method
What is AI Literacy? Competencies and Design Considerations,"Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.",Artificial Intelligence,ai literaci compet design consider,artifici intellig ai becom increasingli integr userfac technolog public understand technolog often limit need addit hci research investig compet user need order effect interact critic evalu ai b design learnercent ai technolog foster increas user understand ai paper take step toward realiz goal provid concret definit ai literaci base exist research synthes varieti interdisciplinari literatur set core compet ai literaci suggest sever design consider support ai develop educ creat learnercent ai compet design consider organ conceptu framework themat deriv literatur paper contribut use start convers guid futur research ai literaci within hci commun
"AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations","This article reports the findings of AI4People, an Atomium—EISMD initiative designed to lay the foundations for a “Good AI Society”. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations—to assess, to develop, to incentivise, and to support good AI—which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.",Artificial Intelligence,ai4peoplean ethic framework good ai societi opportun risk principl recommend,articl report find ai4peopl atomiumeismd initi design lay foundat good ai societi introduc core opportun risk ai societi present synthesi five ethic principl undergird develop adopt offer 20 concret recommendationsto assess develop incentivis support good aiwhich case may undertaken directli nation supran polici maker other may led stakehold adopt recommend would serv firm foundat establish good ai societi
Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation,"Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.",Artificial Intelligence,intellig clinic document har gener ai patientcentr clinic note gener,comprehens clinic document crucial effect healthcar deliveri yet pose signific burden healthcar profession lead burnout increas medic error compromis patient safeti paper explor potenti gener ai artifici intellig streamlin clinic document process specif focus gener soap subject object assess plan birp behavior intervent respons plan note present case studi demonstr applic natur languag process nlp automat speech recognit asr technolog transcrib patientclinician interact coupl advanc prompt techniqu gener draft clinic note use larg languag model llm studi highlight benefit approach includ time save improv document qualiti enhanc patientcent care addit discuss ethic consider maintain patient confidenti address model bias underscor need respons deploy gener ai healthcar set find suggest gener ai potenti revolution clinic document practic allevi administr burden enabl healthcar profession focu direct patient care
Trust in AI and Its Role in the Acceptance of AI Technologies,"Abstract As AI-enhanced technologies become common in a variety of domains, there is an increasing need to define and examine the trust that users have in such technologies. Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required. This paper addresses this need by explaining the role of trust in the intention to use AI technologies. Study 1 examined the role of trust in the use of AI voice assistants based on survey responses from college students. A path analysis confirmed that trust had a significant effect the on intention to use AI, which operated through perceived usefulness and participants’ attitude toward voice assistants. In Study 2, using data from a representative sample of the U.S. population, different dimensions of trust were examined using exploratory factor analysis, which yielded two dimensions: human-like trust and functionality trust. The results of the path analyses from Study 1 were replicated in Study 2, confirming the indirect effect of trust and the effects of perceived usefulness, ease of use, and attitude on intention to use. Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust. Overall, the role of trust in the acceptance of AI technologies was significant across both studies. This research contributes to the advancement and application of the TAM in AI-related applications and",Artificial Intelligence,trust ai role accept ai technolog,abstract aienhanc technolog becom common varieti domain increas need defin examin trust user technolog given progress develop ai correspondingli sophist understand trust technolog requir paper address need explain role trust intent use ai technolog studi 1 examin role trust use ai voic assist base survey respons colleg student path analysi confirm trust signific effect intent use ai oper perceiv use particip attitud toward voic assist studi 2 use data repres sampl us popul differ dimens trust examin use exploratori factor analysi yield two dimens humanlik trust function trust result path analys studi 1 replic studi 2 confirm indirect effect trust effect perceiv use eas use attitud intent use dimens trust share similar pattern effect within model functionalityrel trust exhibit greater total impact usag intent humanlik trust overal role trust accept ai technolog signific across studi research contribut advanc applic tam airel applic
"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies","Artificial intelligence (AI) has the potential to revolutionize the drug discovery process, offering improved efficiency, accuracy, and speed. However, the successful application of AI is dependent on the availability of high-quality data, the addressing of ethical concerns, and the recognition of the limitations of AI-based approaches. In this article, the benefits, challenges, and drawbacks of AI in this field are reviewed, and possible strategies and approaches for overcoming the present obstacles are proposed. The use of data augmentation, explainable AI, and the integration of AI with traditional experimental methods, as well as the potential advantages of AI in pharmaceutical research, are also discussed. Overall, this review highlights the potential of AI in drug discovery and provides insights into the challenges and opportunities for realizing its potential in this field. Note from the human authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, in terms of assisting human authors in writing review articles. The text generated by the AI following our instructions (see Supporting Information) was used as a starting point, and its ability to automatically generate content was evaluated. After conducting a thorough review, the human authors practically rewrote the manuscript, striving to maintain a balance between the original proposal and the scientific criteria. The advantages and limitations of using AI for this purpose are discussed in the last section.",Artificial Intelligence,role ai drug discoveri challeng opportun strategi,artifici intellig ai potenti revolution drug discoveri process offer improv effici accuraci speed howev success applic ai depend avail highqual data address ethic concern recognit limit aibas approach articl benefit challeng drawback ai field review possibl strategi approach overcom present obstacl propos use data augment explain ai integr ai tradit experiment method well potenti advantag ai pharmaceut research also discuss overal review highlight potenti ai drug discoveri provid insight challeng opportun realiz potenti field note human author articl creat test abil chatgpt chatbot base gpt35 languag model term assist human author write review articl text gener ai follow instruct see support inform use start point abil automat gener content evalu conduct thorough review human author practic rewrot manuscript strive maintain balanc origin propos scientif criteria advantag limit use ai purpos discuss last section
Mobile ad hoc network access authentication mechanism based on rotation election and two-factor aggregation,"With the continuous development and progress of computer network communication technology, mobile ad hoc networks (MANET) with strong indestructibility and no fixed infrastructure have become one of the new hot research areas in the academic community. However, due to the open wireless channel and unpredictable changes of network topology, mobile ad hoc networks are susceptible to security threats such as identity spoofing, identity tracing, and DoS attacks, and face security problems that cannot be ignored. Therefore, it is necessary to design a trusted access authentication scheme to authenticate the identity of unknown nodes and ensure the security of connected nodes. In this paper, the Certificate Authority rotation election algorithm based on improved DPoS and access authentication mechanism based on dual identity factor aggregation are proposed, which improve the authentication efficiency and the anti-attack ability of the authentication system, and ensure the security and stability of the network. At the same time, the authentication information is uploaded to the blockchain to ensure the security and immutability of the data. The safety analysis and performance simulation experiments show that the proposed scheme has high safety and good performance.",Networking,mobil ad hoc network access authent mechan base rotat elect twofactor aggreg,continu develop progress comput network commun technolog mobil ad hoc network manet strong indestruct fix infrastructur becom one new hot research area academ commun howev due open wireless channel unpredict chang network topolog mobil ad hoc network suscept secur threat ident spoof ident trace do attack face secur problem ignor therefor necessari design trust access authent scheme authent ident unknown node ensur secur connect node paper certif author rotat elect algorithm base improv dpo access authent mechan base dual ident factor aggreg propos improv authent effici antiattack abil authent system ensur secur stabil network time authent inform upload blockchain ensur secur immut data safeti analysi perform simul experi show propos scheme high safeti good perform
INCS: Intent-driven network-wide configuration synthesis based on deep reinforcement learning,"Configuring modern networks is a complex task due to the intricate low-level configuration languages and diverse routing protocols used across numerous interconnected devices. Network configuration synthesis has been proposed to solve these challenges. However, existing synthesizers encounter several limitations: (i) They cannot scale to large topologies with more than a few tens of routers. (ii) They only support specific routing protocols. (iii) They require prolonged synthesis times. (iv) They lack configuration optimization.
To address these limitations, this paper introduces INCS, an end-to-end network configuration synthesizer designed to generate configurations that comply with any hard and soft specifications. It comprises a fact base graph constructor using graph embedding techniques, a Graph Neural Network (GNN)-based predictor employing a Graph Attention Network (GAT) model, and a Deep Reinforcement Learning (DRL)-based optimizer trained with the Deep Deterministic Policy Gradient (DDPG) algorithm. The graph constructor translates network topology, hard specifications, and configuration sketches into a fact base graph. The predictor then generates initial configurations, which are further optimized by the optimizer to meet predefined soft specifications.
We compare INCS with two benchmark synthesizers: NetComplete and a neural algorithmic reasoning (NAR)-based synthesizer. The evaluation shows that INCS has superior synthesis speed and specification consistency. It is 27.9x faster than the benchmarks. Even in the presence of unsatisfiable hard specifications, INCS maintains 100% specification consistency, whereas Netcomplete fails to synthesize, and the NAR-based synthesizer achieves only 90% consistency.",Networking,inc intentdriven networkwid configur synthesi base deep reinforc learn,configur modern network complex task due intric lowlevel configur languag divers rout protocol use across numer interconnect devic network configur synthesi propos solv challeng howev exist synthes encount sever limit scale larg topolog ten router ii support specif rout protocol iii requir prolong synthesi time iv lack configur optim address limit paper introduc inc endtoend network configur synthes design gener configur compli hard soft specif compris fact base graph constructor use graph embed techniqu graph neural network gnnbase predictor employ graph attent network gat model deep reinforc learn drlbase optim train deep determinist polici gradient ddpg algorithm graph constructor translat network topolog hard specif configur sketch fact base graph predictor gener initi configur optim optim meet predefin soft specif compar inc two benchmark synthes netcomplet neural algorithm reason narbas synthes evalu show inc superior synthesi speed specif consist 279x faster benchmark even presenc unsatisfi hard specif inc maintain 100 specif consist wherea netcomplet fail synthes narbas synthes achiev 90 consist
Adaptive and low-cost resource synchronization based on data distribution service in high dynamic networks,"In disaster-stricken areas monitoring, management, search, and rescue operations, unmanned aerial vehicles (UAVs) play a crucial role in disaster management and emergency communication due to their high mobility. To efficiently coordinate and plan UAVs and their carried sensor and base station resources, synchronization is essential to establish consistency, laying the foundation for high-level demands. In such scenarios, synchronization relies on request–response (RR) or publish–subscribe (PS) forms of information exchange. Existing research in the field typically focuses on higher-level applications and selects either RR or PS synchronization, thereby overlooking the potential advantages that could be gained from combining both methods to meet synchronization requirements. We propose a resource synchronization method based on the Data Distribution Service (DDS) and a linear time complexity subscription mechanism tailored to specific query demands, which considers the pros and cons of the above two information exchange forms and the bottom-layer network topology. Experimental results using open-source simulation tools demonstrate that the proposed method adapts to scene requirements and decreases bandwidth by at least 21.2% and packet rate by at least 3.7% compared to different baseline methods across three topologies, while satisfying delay and query success rate requirements. Furthermore, the method maintains robust performance in the face of dynamic changes in network topology, showcasing its robustness.",Networking,adapt lowcost resourc synchron base data distribut servic high dynam network,disasterstricken area monitor manag search rescu oper unman aerial vehicl uav play crucial role disast manag emerg commun due high mobil effici coordin plan uav carri sensor base station resourc synchron essenti establish consist lay foundat highlevel demand scenario synchron reli requestrespons rr publishsubscrib ps form inform exchang exist research field typic focus higherlevel applic select either rr ps synchron therebi overlook potenti advantag could gain combin method meet synchron requir propos resourc synchron method base data distribut servic dd linear time complex subscript mechan tailor specif queri demand consid pro con two inform exchang form bottomlay network topolog experiment result use opensourc simul tool demonstr propos method adapt scene requir decreas bandwidth least 212 packet rate least 37 compar differ baselin method across three topolog satisfi delay queri success rate requir furthermor method maintain robust perform face dynam chang network topolog showcas robust
A congestion-aware user–cell association scheme to assist with traffic offloading in a three-tier heterogeneous network,"In this work, we aim to expand the cell range in congestion areas. To achieve this goal, we integrate UAV network into macro–micro cellular network to enhance coverage. However, the gain of deploying heterogeneous nodes is limit if there are no CRE (cell range expanding)-based method to steer users towards low-power nodes. Therefore, in order to further improve system access capacity, we propose a congestion-aware UA (user–cell association) scheme to offload queued traffic of cellular network into UAV network. To verify the superiority of proposed scheme, we utilize the low-complexity SG (stochastic-geometry) methods. Then in simulation analysis phase, we obtain five primary results: . The SG-based methods have lower computation complexity, and the convergence of derived expressions are verified. . The SG-based methods do not need the exact users’ location but the users’ density, which have improved the operability and robustness in infrastructure construction of wireless network. The congestion-aware UA scheme can prompt network to adjust association tier flexibly, which is very suitable for traffic unevenly-distributed regions. 
",Networking,congestionawar usercel associ scheme assist traffic offload threetier heterogen network,work aim expand cell rang congest area achiev goal integr uav network macromicro cellular network enhanc coverag howev gain deploy heterogen node limit cre cell rang expandingbas method steer user toward lowpow node therefor order improv system access capac propos congestionawar ua usercel associ scheme offload queu traffic cellular network uav network verifi superior propos scheme util lowcomplex sg stochasticgeometri method simul analysi phase obtain five primari result sgbase method lower comput complex converg deriv express verifi sgbase method need exact user locat user densiti improv oper robust infrastructur construct wireless network congestionawar ua scheme prompt network adjust associ tier flexibl suitabl traffic unevenlydistribut region
Reliable edge-to-core optical networks: An optimal algorithm for maximal path diversity,"With the emergence of IoT applications, 5G, and edge computing, network resource allocation has shifted toward the edge, bringing services closer to the end users. These applications often require communication with the core network for purposes that include cloud storage, compute offloading, 5G-and-Beyond transport communication between centralized unit (CU), distributed unit (DU) and core network, centralized network monitoring and management, etc. As the number of these services increases, efficient and reliable connectivity between the edge and core networks is of the essence. Wavelength Division Multiplexing (WDM) is a well-suited technology for transferring large amounts of data by simultaneously transmitting several wavelength-multiplexed data streams over each single fiber optics link. WDM is the technology of choice in mid-haul and long-haul transmission networks, including edge-to-core networks, to offer increased transport capacity.
Optical networks are prone to failures of components such as network fiber links, sites, and transmission ports. A single network element failure alone can cause significant traffic loss due to the disruption of many active data flows. Thus, fault-tolerant and reliable network designs remain a priority. The architecture called “dual-hub and dual-spoke” is often used in metro area networks (MANs). A dual-hub, or in general a multi-hub network, consists of a set of designated destination nodes (hubs) in which the data traffic from all other nodes (the peripherals) should be directed to the hubs. Multiple hubs offer redundant connectivity to and from the core or wide area network (WAN) through geographical diversity. The routing of the connections (also known as lightpaths) between the peripheral node and the hubs has to be carefully computed to maximize path diversity across the edge-to-core network. This means that whenever possible the established redundant lightpaths must not contain a common Shared Risk Link Group (SRLG).
An algorithm is proposed to compute the most reliable set of SRLG disjoint shortest paths from any peripheral to all hubs. The proposed algorithm can also be used to evaluate the overall edge-to-core network reliability quantified through a newly introduced figure of merit.",Networking,reliabl edgetocor optic network optim algorithm maxim path divers,emerg iot applic 5g edg comput network resourc alloc shift toward edg bring servic closer end user applic often requir commun core network purpos includ cloud storag comput offload 5gandbeyond transport commun central unit cu distribut unit du core network central network monitor manag etc number servic increas effici reliabl connect edg core network essenc wavelength divis multiplex wdm wellsuit technolog transfer larg amount data simultan transmit sever wavelengthmultiplex data stream singl fiber optic link wdm technolog choic midhaul longhaul transmiss network includ edgetocor network offer increas transport capac optic network prone failur compon network fiber link site transmiss port singl network element failur alon caus signific traffic loss due disrupt mani activ data flow thu faulttoler reliabl network design remain prioriti architectur call dualhub dualspok often use metro area network man dualhub gener multihub network consist set design destin node hub data traffic node peripher direct hub multipl hub offer redund connect core wide area network wan geograph divers rout connect also known lightpath peripher node hub care comput maxim path divers across edgetocor network mean whenev possibl establish redund lightpath must contain common share risk link group srlg algorithm propos comput reliabl set srlg disjoint shortest path peripher hub propos algorithm also use evalu overal edgetocor network reliabl quantifi newli introduc figur merit
Secure and Efficient Authentication using Linkage for permissionless Bitcoin network,"The cryptocurrency’s permissionless and large-scale broadcasting requirements prohibit the traditional authentication implementation on the blockchain’s underlying peer-to-peer (P2P) networking. Thus, blockchain networking implementations remain vulnerable to networking integrity threats such as spoofing or hijacking. We design Secure and Efficient Authentication using Linkage (SEAL) to build connection security for permissionless Bitcoin networking. SEAL uses the linkage between the packets for a symmetric operation, in contrast to the traditional authentication approach relying on identity-credential-based trust. To make it appropriate for cryptocurrency networking, SEAL utilizes the packet header, protects the end-to-end connection, and separates the online process and the offline process so that the real-time overhead is minimal for greater efficiency and practicality. We implement SEAL on a functioning Bitcoin node and demonstrate that SEAL operates efficiently with minimal overhead. Specifically, it reduces the hash rate by only 1.3% compared to an unsecured node. Additionally, we use a network simulator to emulate the Bitcoin Mainnet and analyze SEAL’s impact on block propagation delay. SEAL yields 2.04 times smaller delay and 1.25 times smaller delay in block propagation than HMAC and ChaCha20-Poly1305, respectively. The key advantage of SEAL is that it requires fewer hash computations and simpler mixing operations, resulting in significantly lower computational overhead compared to traditional authentication schemes based on message authentication codes (MACs).",Networking,secur effici authent use linkag permissionless bitcoin network,cryptocurr permissionless largescal broadcast requir prohibit tradit authent implement blockchain underli peertop p2p network thu blockchain network implement remain vulner network integr threat spoof hijack design secur effici authent use linkag seal build connect secur permissionless bitcoin network seal use linkag packet symmetr oper contrast tradit authent approach reli identitycredentialbas trust make appropri cryptocurr network seal util packet header protect endtoend connect separ onlin process offlin process realtim overhead minim greater effici practic implement seal function bitcoin node demonstr seal oper effici minim overhead specif reduc hash rate 13 compar unsecur node addit use network simul emul bitcoin mainnet analyz seal impact block propag delay seal yield 204 time smaller delay 125 time smaller delay block propag hmac chacha20poly1305 respect key advantag seal requir fewer hash comput simpler mix oper result significantli lower comput overhead compar tradit authent scheme base messag authent code mac
A two-step linear programming approach for repeater placement in large-scale quantum networks,"Thanks to the applications such as Quantum Key Distribution and Distributed Quantum Computing, the deployment of quantum networks is gaining great momentum. A major component in quantum networks is repeaters, which are essential for reducing the error rate of qubit transmission for long-distance links. However, repeaters are expensive devices, so minimizing the number of repeaters placed in a quantum network while satisfying performance requirements becomes an important problem. Existing solutions typically solve this problem optimally by formulating an Integer Linear Program (ILP). However, the number of variables in their ILPs is , where is the number of nodes in a network. This incurs infeasible running time when the network scale is large. To overcome this drawback, this paper proposes to solve the repeater placement problem by two steps, with each step using a linear program of a much smaller scale with  variables. Although this solution is not optimal, it dramatically reduces the time complexity, making it practical for large-scale networks. Moreover, it constructs networks that have higher node connectivity than those by existing solutions, since it deploys slightly more number of repeaters into networks. Our extensive experiments on both synthetic and real-world network topologies verified our claims.",Networking,twostep linear program approach repeat placement largescal quantum network,thank applic quantum key distribut distribut quantum comput deploy quantum network gain great momentum major compon quantum network repeat essenti reduc error rate qubit transmiss longdist link howev repeat expens devic minim number repeat place quantum network satisfi perform requir becom import problem exist solut typic solv problem optim formul integ linear program ilp howev number variabl ilp number node network incur infeas run time network scale larg overcom drawback paper propos solv repeat placement problem two step step use linear program much smaller scale variabl although solut optim dramat reduc time complex make practic largescal network moreov construct network higher node connect exist solut sinc deploy slightli number repeat network extens experi synthet realworld network topolog verifi claim
Designing the Network Intelligence Stratum for 6G networks,"As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security. A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response. These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance. They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI). Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach. However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective. This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum). This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains. The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management. We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments. The paper also outlines major challenges and open issues in deploying and managing NI.",Networking,design network intellig stratum 6g network,network complex escal increas need sophist method manag oper network focus enhanc effici reliabl secur wide rang artifici intellig aimachin learn ml model develop respons model pivot autom decisionmak conduct predict analys manag network proactiv enhanc secur optim network perform foundat shape futur network collect form known network intellig ni promin standarddefin organ sdo integr ni futur network architectur particularli emphas closedloop approach howev exist method seamlessli integr ni network architectur yet fulli effect paper introduc indepth architectur design network intellig stratum ni stratum stratum support novel endtoend ni orchestr support closedloop ni oper across variou network domain primari goal design streamlin deploy coordin ni throughout entir network infrastructur tackl issu relat scalabl conflict resolut effect data manag detail exhaust workflow manag ni lifecycl demonstr refer implement ni stratum focus compat integr current network system opensourc platform kubernet kubeflow well valid realworld environ paper also outlin major challeng open issu deploy manag ni
Network traffic prediction based on PSO-LightGBM-TM,"Network traffic prediction is critical in wireless network management by allowing a good estimate of the traffic trend, which is also an important approach for detecting traffic anomalies in order to enhance network security. Deep-learning-based method has been widely adopted to predict network traffic matrix (TM) though with the main drawbacks in high complexity and low efficiency. In this paper, we propose a traffic prediction model based on Particle Swarm Optimization (PSO) and LightGBM (PSO-LightGBM-TM), which optimizes the LightGBM parameters for each network flow by PSO so that LightGBM can adapt to each of the network traffic flow. Compared with existing commonly used deep learning models, our model has a more straightforward structure and yet outperforms existing deep learning models. Sufficient comparison tests on three real network traffic datasets, Abilene, GÉANT, and CERNET have been conducted, and the results show that our model provides more accurate results and higher prediction efficiency.",Networking,network traffic predict base psolightgbmtm,network traffic predict critic wireless network manag allow good estim traffic trend also import approach detect traffic anomali order enhanc network secur deeplearningbas method wide adopt predict network traffic matrix tm though main drawback high complex low effici paper propos traffic predict model base particl swarm optim pso lightgbm psolightgbmtm optim lightgbm paramet network flow pso lightgbm adapt network traffic flow compar exist commonli use deep learn model model straightforward structur yet outperform exist deep learn model suffici comparison test three real network traffic dataset abilen géant cernet conduct result show model provid accur result higher predict effici
Towards universal and transferable adversarial attacks against network traffic classification,"In recent years, deep learning technology has shown astonishing potential in many fields, but at the same time, it also hides serious vulnerabilities. In the field of network traffic classification, attackers exploit this vulnerability to add designed perturbations to normal traffic, causing incorrect network traffic classification to implement adversarial attacks. The existing network traffic adversarial attack methods mainly target specific models or sample application scenarios, which have many problems such as poor transferability, high time cost, and low practicality. Therefore, this article proposes a method towards universal and transferable adversarial attacks against network traffic classification, which can not only perform universal adversarial attacks on all samples in the network traffic dataset, but also achieve cross data and cross model transferable adversarial attacks, that is, it has transferable attack effects at both the network traffic data and classification model levels. This method utilizes the geometric characteristics of the network model to design the target loss function and optimize the generation of universal perturbations, resulting in biased learning of features at each layer of the network model, leading to incorrect classification results. Meanwhile, this article conducted universality and transferability adversarial attack verification experiments on standard network traffic datasets of three different classification applications, USTC-TFC2016, ISCX2016, and CICIoT2023, as well as five common network models such as LeNet5. The results show that the proposed method performs universal adversarial attacks on five network models on three datasets, USTC-TFC2016, ISCX2016, and CICIoT2023, with an average attack success rate of over 80 %, 85 %, and 88 %, respectively, and an average time cost of about 0–0.3 ms; And the method proposed in this article has shown good transferable attack performance between five network models and on three network traffic datasets, with transferable attack rates approaching 100 % across different models and datasets, which is more closely related to practical applications.",Networking,toward univers transfer adversari attack network traffic classif,recent year deep learn technolog shown astonish potenti mani field time also hide seriou vulner field network traffic classif attack exploit vulner add design perturb normal traffic caus incorrect network traffic classif implement adversari attack exist network traffic adversari attack method mainli target specif model sampl applic scenario mani problem poor transfer high time cost low practic therefor articl propos method toward univers transfer adversari attack network traffic classif perform univers adversari attack sampl network traffic dataset also achiev cross data cross model transfer adversari attack transfer attack effect network traffic data classif model level method util geometr characterist network model design target loss function optim gener univers perturb result bias learn featur layer network model lead incorrect classif result meanwhil articl conduct univers transfer adversari attack verif experi standard network traffic dataset three differ classif applic ustctfc2016 iscx2016 ciciot2023 well five common network model lenet5 result show propos method perform univers adversari attack five network model three dataset ustctfc2016 iscx2016 ciciot2023 averag attack success rate 80 85 88 respect averag time cost 003 ms method propos articl shown good transfer attack perform five network model three network traffic dataset transfer attack rate approach 100 across differ model dataset close relat practic applic
A fault tolerant node placement algorithm for WSNs and IoT networks,"The operation of Internet of Things (IoT) Network and Wireless Sensor Networks (WSNs) can be often disrupted by a number of factors, such as node faults, security attacks, as well as path disconnections. Despite any disruptions or any failures of network components the functionality and performance of the network should remain unaltered. An approach that can assist in resolving the above issues is the use of mobile nodes. In this work, we present a Fault Tolerant Node Placement Algorithm (FTNPA) that utilizes mobile nodes for addressing failures in the network. We initially propose a Mobile Fault Tolerant (MobileFT) Framework, that supplies the two main functionalities of the Fault Tolerant Node Placement Algorithm (FTNPA), which are the detection and the recovery. Then, we present two variations of the FTNPA algorithm, the Decentralized FTNPA and the Centralized FTNPA. The first variation uses a decentralized detection method, where the detection is performed by the neighboring nodes in the network, as well as a local recovery method, where a mobile node is placed in a certain position to assist the affected area. Whereas, the second variation employs a centralized detection method, where the sink is responsible for the detection, and a recovery method that creates alternatives paths of mobile nodes towards a destination node. Simulation results show that the proposed algorithms can significantly contribute to the detection and recovery of faults in IoT Networks and WSNs.",Networking,fault toler node placement algorithm wsn iot network,oper internet thing iot network wireless sensor network wsn often disrupt number factor node fault secur attack well path disconnect despit disrupt failur network compon function perform network remain unalt approach assist resolv issu use mobil node work present fault toler node placement algorithm ftnpa util mobil node address failur network initi propos mobil fault toler mobileft framework suppli two main function fault toler node placement algorithm ftnpa detect recoveri present two variat ftnpa algorithm decentr ftnpa central ftnpa first variat use decentr detect method detect perform neighbor node network well local recoveri method mobil node place certain posit assist affect area wherea second variat employ central detect method sink respons detect recoveri method creat altern path mobil node toward destin node simul result show propos algorithm significantli contribut detect recoveri fault iot network wsn
Intelligent resource allocation in wireless networks: Predictive models for efficient access point management,"With the significant increase in mobile users connected to the wireless network, coupled with the escalating energy consumption and the risk of network saturation, the search for resource management has become paramount. Managing several access points throughout a whole region is hugely relevant in this context. Moreover, a wireless network must keep its Service Level Agreement, regardless of the number of connected users. With that in mind, in this work, we propose four prediction models that allow one to predict the number of connected users on a wireless network. Once the number of users has been predicted, the network resources can be properly allocated, minimizing the number of active access points. We investigate the use of Particle Swarm Optimization and Genetic Algorithms to hyper-parameterize a Multilayer Perceptron neural network and a Decision Tree. We evaluate our proposal using a campus-based wireless network dataset with more than 20,000 connected users. As a result, our model can considerably improve network performance by intelligently allocating the number of access points, thereby addressing concerns related to energy consumption and network saturation. The results have shown an average accuracy of 95.18%, managing to save network resources effectively.",Networking,intellig resourc alloc wireless network predict model effici access point manag,signific increas mobil user connect wireless network coupl escal energi consumpt risk network satur search resourc manag becom paramount manag sever access point throughout whole region huge relev context moreov wireless network must keep servic level agreement regardless number connect user mind work propos four predict model allow one predict number connect user wireless network number user predict network resourc properli alloc minim number activ access point investig use particl swarm optim genet algorithm hyperparameter multilay perceptron neural network decis tree evalu propos use campusbas wireless network dataset 20000 connect user result model consider improv network perform intellig alloc number access point therebi address concern relat energi consumpt network satur result shown averag accuraci 9518 manag save network resourc effect
Protecting unauthenticated messages in LTE/5G mobile networks: A two-level Hierarchical Identity-Based Signature (HIBS) solution,"As an essential public infrastructure, the security and reliability of mobile networks have a profound impact on people’s production and life. Although the security of LTE/5G networks has been improved a lot with the evolution of standards, there are still some unprotected messages being transmitted between the cellular network and device due to the symmetric key-based security architecture and the trade-off between security and other criteria like network availability. By exploiting these messages, various security attacks have been proposed and demonstrated against commercial mobile networks and devices in existing literature, such as user location tracking, bidding-down, and DoS attacks. To address this security issue, in this paper, we aim to protect these unauthenticated messages in mobile networks using digital signatures. Based on the idea of Hierarchical Identity-Based Signature (HIBS) in existing work, we analyse and design a two-level HIBS solution in detail in terms of different aspects such as keys generation and provisioning procedures, replay mitigation, and cell selection. Unlike previous work, our proposed solution also supports the protection of individual vulnerable RRC and NAS layer signalling in addition to authenticating the base station. We evaluated the efficiency and feasibility of several existing HIBS schemes and implemented the most efficient one in the 5G standalone network setup using open-source software. The implementation results further proved the feasibility of the solution in practice.",Networking,protect unauthent messag lte5g mobil network twolevel hierarch identitybas signatur hib solut,essenti public infrastructur secur reliabl mobil network profound impact peopl product life although secur lte5g network improv lot evolut standard still unprotect messag transmit cellular network devic due symmetr keybas secur architectur tradeoff secur criteria like network avail exploit messag variou secur attack propos demonstr commerci mobil network devic exist literatur user locat track biddingdown do attack address secur issu paper aim protect unauthent messag mobil network use digit signatur base idea hierarch identitybas signatur hib exist work analys design twolevel hib solut detail term differ aspect key gener provis procedur replay mitig cell select unlik previou work propos solut also support protect individu vulner rrc na layer signal addit authent base station evalu effici feasibl sever exist hib scheme implement effici one 5g standalon network setup use opensourc softwar implement result prove feasibl solut practic
NeSt: A QoS differentiating end-to-end networked storage simulator,"The emerging high-speed storage technologies increasingly use Nonvolatile Memory Express (NVMe) protocol to meet their high throughput and low latency needs. In a datacenter environment, applications accessing multiple such devices over the fabric (i.e. the network) tend to have Quality of Service (QoS) requirements pertaining to offered throughput and experienced latency. In this paper we describe a networked storage system simulator called NeSt that supports end-to-end (E2E) QoS differentiation across multiple classes of service. This is done by conveying the class designation end to end and using it to consistently but independently apply the differentiation in each segment of the path. We demonstrate the ability of NeSt to provide end-to-end QoS differentiation under a variety of situations. To the best of our knowledge, NeSt is the first simulator of networked storage (consisting of multiple NVMe SSDs) that supports E2E QoS differentiation.",Networking,nest qo differenti endtoend network storag simul,emerg highspe storag technolog increasingli use nonvolatil memori express nvme protocol meet high throughput low latenc need datacent environ applic access multipl devic fabric ie network tend qualiti servic qo requir pertain offer throughput experienc latenc paper describ network storag system simul call nest support endtoend e2e qo differenti across multipl class servic done convey class design end end use consist independ appli differenti segment path demonstr abil nest provid endtoend qo differenti varieti situat best knowledg nest first simul network storag consist multipl nvme ssd support e2e qo differenti
Performance enhancement of NOMA multi-user networks with aerial reconfigurable intelligent surfaces and aerial relay,"In this article, we propose a combination of multiple aerial reconfigurable intelligent surfaces (ARISs) with an aerial relay (AR) for improving the performance of multiple users adopting the non-orthogonal multiple access (NOMA) scheme. In particular, two groups of ARIS are utilized, one for aiding ground-to-air (G2A) communication and another one for aiding air-to-ground (A2G) communication. Mathematical formulas of outage probability (OP), throughput, and achievable capacity (AC) of every user in the proposed ARISs-AR-NOMA network are derived over the Nakagami-
 channel. The propagation environment recommended for the fifth generation and beyond (5G/B5G) is applied to enhance the practical behavior of the proposed network. To confirm the performance enhancement, we compare the OP, throughput, and AC of the proposed system with the conventional AR-NOMA network (i.e., without ARISs). Numerical illustrations demonstrate the great benefits of the proposed ARISs-AR-NOMA network, its transmit power is dramatically lower than that of the conventional network. Moreover, the throughput and AC of the proposed network are considerably higher than the conventional network’s. Based on the effect of key parameters and network behaviors, various effective solutions are recommended to enhance the performance of the proposed ARISs-AR-NOMA network.",Networking,perform enhanc noma multius network aerial reconfigur intellig surfac aerial relay,articl propos combin multipl aerial reconfigur intellig surfac ariss aerial relay ar improv perform multipl user adopt nonorthogon multipl access noma scheme particular two group ari util one aid groundtoair g2a commun anoth one aid airtoground a2g commun mathemat formula outag probabl op throughput achiev capac ac everi user propos arissarnoma network deriv nakagami channel propag environ recommend fifth gener beyond 5gb5g appli enhanc practic behavior propos network confirm perform enhanc compar op throughput ac propos system convent arnoma network ie without ariss numer illustr demonstr great benefit propos arissarnoma network transmit power dramat lower convent network moreov throughput ac propos network consider higher convent network base effect key paramet network behavior variou effect solut recommend enhanc perform propos arissarnoma network
An intrusion detection method combining variational auto-encoder and generative adversarial networks,"Deep learning is a crucial research area in network security, particularly when it comes to detecting network attacks. While some deep learning algorithms have shown promising results in distinguishing between normal and abnormal traffic, identifying different types of imbalanced anomalous traffic data is still a challenging task at present. To enhance the detection performance of unbalanced anomalous flows, we propose a new intrusion detection architecture based on a variational auto-encoder (VAE) and generative adversarial networks (GAN) in this research. Firstly, we present the VAE-WGAN model, which combines the advantages of VAE and GAN and enables us to generate data with predefined labels to balance the original training dataset. In the intrusion detection phase, we use a hybrid neural network model based on stacked Long Short-Term Memory (LSTM) and Multi-Scale Convolutional Neural Network (MSCNN). Stacked LSTM and MSCNN networks can extract network characteristics at different depths and scales, and subsequent feature fusion is used to increase network attack detection rates. Finally, the results from the NSL-KDD and AWID datasets indicate that the proposed network intrusion detection model improves the accuracy of network attack detection. The model outperforms other existing intrusion detection approaches in terms of accuracy, precision, recall, and f1-score, obtaining 83.45% accuracy and 83.69% f1-score on the NSL-KDD dataset. Moreover, it attains an accuracy and f1-score exceeding 98.9% on the AWID dataset.",Networking,intrus detect method combin variat autoencod gener adversari network,deep learn crucial research area network secur particularli come detect network attack deep learn algorithm shown promis result distinguish normal abnorm traffic identifi differ type imbalanc anomal traffic data still challeng task present enhanc detect perform unbalanc anomal flow propos new intrus detect architectur base variat autoencod vae gener adversari network gan research firstli present vaewgan model combin advantag vae gan enabl us gener data predefin label balanc origin train dataset intrus detect phase use hybrid neural network model base stack long shortterm memori lstm multiscal convolut neural network mscnn stack lstm mscnn network extract network characterist differ depth scale subsequ featur fusion use increas network attack detect rate final result nslkdd awid dataset indic propos network intrus detect model improv accuraci network attack detect model outperform exist intrus detect approach term accuraci precis recal f1score obtain 8345 accuraci 8369 f1score nslkdd dataset moreov attain accuraci f1score exceed 989 awid dataset
"NDN multicast over wireless networks: A survey on fundamentals, challenges, and open issues","Wireless devices have shown remarkable growth in data demand in the last decade due to the proliferation of smart devices and the emergence of bandwidth-hungry applications. This increasing data traffic demand is overcrowding the radio frequency spectrum, leading wireless multicast schemes to become a popular research topic again, as they provide efficient data dissemination. NDN supports multicast communication by design. It is a networked system formed by named entities that adopt a communication model focusing on the content rather than its location. The architecture follows a receiver-driven communication model through which content consumers retrieve data through semantically meaningful names instead of specific destinations. Its properties are essential for multicast communication on ad hoc networks, as its features provide enhanced support for dynamic topologies, decentralized control, and the self-organization of participant nodes that communicate without needing a pre-existing network infrastructure. However, despite the wireless medium being broadcast by nature, NDN multicast is still challenging in wireless scenarios, especially in ad hoc environments, due to the node’s high mobility, link instability, constant handovers, and data transmission over a shared medium. Hence, this survey discusses the benefits of NDN for mobile scenarios through an in-depth analysis of NDN multicast features, focusing on fundamentals, challenges, and open issues when applied to wireless networking.",Networking,ndn multicast wireless network survey fundament challeng open issu,wireless devic shown remark growth data demand last decad due prolifer smart devic emerg bandwidthhungri applic increas data traffic demand overcrowd radio frequenc spectrum lead wireless multicast scheme becom popular research topic provid effici data dissemin ndn support multicast commun design network system form name entiti adopt commun model focus content rather locat architectur follow receiverdriven commun model content consum retriev data semant meaning name instead specif destin properti essenti multicast commun ad hoc network featur provid enhanc support dynam topolog decentr control selforgan particip node commun without need preexist network infrastructur howev despit wireless medium broadcast natur ndn multicast still challeng wireless scenario especi ad hoc environ due node high mobil link instabl constant handov data transmiss share medium henc survey discuss benefit ndn mobil scenario indepth analysi ndn multicast featur focus fundament challeng open issu appli wireless network
DDQN-SFCAG: A service function chain recovery method against network attacks in 6G networks,"Service Function Chain (SFC) as an effective solution can satisfy the diverse service requirements of six application scenarios in Network Function Virtualization (NFV)-enabled 6G networks. Resilience as a new key capability indicator in 6G networks puts forward higher requirements for the Quality of Service (QoS) of SFCs. In this article, we study the resilience recovery method in network attack scenarios. We make full use of the monitoring and early warning capability of the monitoring function and propose a proactive recovery method called Double Deep Q-Network based on SFC Attack Graph (DDQN-SFCAG). We fully consider the characteristics of network attacks to generate SFC attack graphs and determine the recovery strategy of SFC according to the service security requirements to provide guidance for recovery. Among them, we design three recovery modes for the recovered Virtual Network Functions (VNFs) to determine the optimal recovery strategy and avoid resource waste. We formalize the SFC recovery problem, which aims to minimize the recovery cost while meeting the recovery strategy. In order to shorten the interruption time, we use DDQN to quickly solve the recovery solution to ensure optimal recovery performance. Our extensive evaluation shows DDQN-SFCAG has excellent recovery performance in network attack scenarios and can reduce the recovery cost by at least 31% compared to the state-of-the-art methods.",Networking,ddqnsfcag servic function chain recoveri method network attack 6g network,servic function chain sfc effect solut satisfi divers servic requir six applic scenario network function virtual nfvenabl 6g network resili new key capabl indic 6g network put forward higher requir qualiti servic qo sfc articl studi resili recoveri method network attack scenario make full use monitor earli warn capabl monitor function propos proactiv recoveri method call doubl deep qnetwork base sfc attack graph ddqnsfcag fulli consid characterist network attack gener sfc attack graph determin recoveri strategi sfc accord servic secur requir provid guidanc recoveri among design three recoveri mode recov virtual network function vnf determin optim recoveri strategi avoid resourc wast formal sfc recoveri problem aim minim recoveri cost meet recoveri strategi order shorten interrupt time use ddqn quickli solv recoveri solut ensur optim recoveri perform extens evalu show ddqnsfcag excel recoveri perform network attack scenario reduc recoveri cost least 31 compar stateoftheart method
Multi-view multi-label network traffic classification based on MLP-Mixer neural network ,"Network traffic classification is the basis of many network security applications and has received significant attention in the field of cyberspace security. Existing research on deep traffic analysis typically involves converting traffic data into images to extract spatial traffic features using Convolutional Neural Networks (CNNs). However, this approach ignores the semantic differences and details in the various packet structures. In this paper, we propose an MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, one packet is divided into the packet header and the packet payload, together with the flow statistics of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. We conduct experiments on three public datasets, and the experimental results show that our method can achieve superior performance. Code is available at",Networking,multiview multilabel network traffic classif base mlpmixer neural network,network traffic classif basi mani network secur applic receiv signific attent field cyberspac secur exist research deep traffic analysi typic involv convert traffic data imag extract spatial traffic featur use convolut neural network cnn howev approach ignor semant differ detail variou packet structur paper propos mlpmixer base multiview multilabel neural network network traffic classif compar exist cnnbase method method adopt mlpmixer structur line structur packet convent convolut oper method one packet divid packet header packet payload togeth flow statist packet input differ view util multilabel set learn differ scenario simultan improv classif perform exploit correl differ scenario conduct experi three public dataset experiment result show method achiev superior perform code avail
Themis: A passive-active hybrid framework with in-network intelligence for lightweight failure localization,"The fast and efficient failure detection and localization is essential for stable network transmission. Unfortunately, existing schemes suffer from a few drawbacks such as significant resource consumption , lack of support for fast online failure localization, and limited applicable topologies. In this paper, we design Themis, a lightweight learning-based failure localization scheme for general networks. In the data plane, Themis achieves line-speed high performance failure detection using in-network classifiers and fine-grained traffic features. To reduce communication overhead, only coarse-grained traffic features are reported to the control plane for localization when a failure occurs. In the control plane, we propose a two-stage passive-active hybrid failure localization approach to accurately locate the failure without incurring excessive probing traffic. First, passive detection is conducted through the lightweight model XGBoost to infer a Potential Failure Link Set (PFLS). Then, active detection is done by only sending out probing packets to locations in the PFLS for precise failure localization. Comprehensive experiments demonstrate that Themis achieves ms-level failure localization with at least 95.63% accuracy, while saving 87.41% of bandwidth and 41.88% of hardw",Networking,themi passiveact hybrid framework innetwork intellig lightweight failur local,fast effici failur detect local essenti stabl network transmiss unfortun exist scheme suffer drawback signific resourc consumpt lack support fast onlin failur local limit applic topolog paper design themi lightweight learningbas failur local scheme gener network data plane themi achiev linespe high perform failur detect use innetwork classifi finegrain traffic featur reduc commun overhead coarsegrain traffic featur report control plane local failur occur control plane propos twostag passiveact hybrid failur local approach accur locat failur without incur excess probe traffic first passiv detect conduct lightweight model xgboost infer potenti failur link set pfl activ detect done send probe packet locat pfl precis failur local comprehens experi demonstr themi achiev mslevel failur local least 9563 accuraci save 8741 bandwidth 4188 hardw
6TiSCH IIoT network: A review,"Low-power and Lossy Networks (LLN) constitute an interconnected network of numerous resource-constrained nodes, forming a wireless mesh network. The Time slotted Channel Hopping (TSCH) mode, introduced as a revision of the Medium Access Control (MAC) section within the IEEE 802.15.4 standard, stands as an emerging standard for industrial automation and process control. In 2013, the Internet Engineering Task Force (IETF) established the IPv6 over the TSCH mode of IEEE 802.15.4e (6TiSCH) working group (WG), defining the IPv6 deterministic wireless network—6TiSCH. This development is pivotal for advancing the broader adoption of IPv6 in industrial standards and facilitating the convergence of operational technology (OT) and information technology (IT). As of July 2023, the primary documents encompassing architecture, configuration and parameters, and Minimum Scheduling Function for the 6TiSCH protocol stack have been completed, and the status of the WG has transitioned from active to concluded. Over the past decade, the academic community has extensively researched protocol stacks related to 6TiSCH. This paper furnishes a comprehensive survey of the architecture and developmental processes underlying the 6TiSCH network, encapsulating research achievements since its inception, and delineating the challenges and prospective directions for its future development.",Networking,6tisch iiot network review,lowpow lossi network lln constitut interconnect network numer resourceconstrain node form wireless mesh network time slot channel hop tsch mode introduc revis medium access control mac section within ieee 802154 standard stand emerg standard industri autom process control 2013 internet engin task forc ietf establish ipv6 tsch mode ieee 802154e 6tisch work group wg defin ipv6 determinist wireless network6tisch develop pivot advanc broader adopt ipv6 industri standard facilit converg oper technolog ot inform technolog juli 2023 primari document encompass architectur configur paramet minimum schedul function 6tisch protocol stack complet statu wg transit activ conclud past decad academ commun extens research protocol stack relat 6tisch paper furnish comprehens survey architectur development process underli 6tisch network encapsul research achiev sinc incept delin challeng prospect direct futur develop
Achieving scalable capacity in wireless mesh networks,"Wireless mesh networks are critical in enabling key networking scenarios in beyond-5G (B5G) and 6G networks, including integrated access and backhaul (IAB), multi-hop sidelinks, and V2X. However, it still poses a challenge to deliver scalable per-node throughput via mesh networking. As shown in Gupta and Kumar’s seminal research (Gupta and Kumar, 2000), multi-hop transmission results in a per-node throughput of  in a wireless network with  nodes, significantly limiting the potential of large-scale deployment of wireless mesh networks. Follow-up research has achieved per-node throughput in a dense network, but how to achieve scalability remains an unresolved issue for an extended wireless network where the network size increases with a constant node density. This issue prevents a wireless mesh network from large-scale deployment. To this end, this paper aims to develop a theoretical approach to achieving scalable per-node throughput in wireless mesh networks. First, the key factors that limit the per-node throughput of wireless mesh networks are analyzed, through which two major ones are identified, i.e., link sharing and interference. Next, a multi-tier hierarchical architecture is proposed to overcome the link-sharing issue. The inter-tier interference under this architecture is then mitigated by utilizing orthogonal frequency allocation between adjacent tiers, while the intra-tier interference is reduced by considering two specific transmission schemes, one is MIMO spatial multiplexing with time-division, the other is MIMO beamforming. Theoretical analysis shows that the multi-tier mesh networking architecture can achieve a per-node throughput of  in both schemes, as long as certain conditions on network parameters including bandwidth, the number of antennas, and the number of nodes of each tier are satisfied. A case study on a realistic deployment of 10,000 nodes is then carried out, which demonstrates that a scalable throughput of  is achievable with a reasonable assumption on bandwidth and the number of antennas.",Networking,achiev scalabl capac wireless mesh network,wireless mesh network critic enabl key network scenario beyond5g b5g 6g network includ integr access backhaul iab multihop sidelink v2x howev still pose challeng deliv scalabl pernod throughput via mesh network shown gupta kumar semin research gupta kumar 2000 multihop transmiss result pernod throughput wireless network node significantli limit potenti largescal deploy wireless mesh network followup research achiev pernod throughput dens network achiev scalabl remain unresolv issu extend wireless network network size increas constant node densiti issu prevent wireless mesh network largescal deploy end paper aim develop theoret approach achiev scalabl pernod throughput wireless mesh network first key factor limit pernod throughput wireless mesh network analyz two major one identifi ie link share interfer next multiti hierarch architectur propos overcom linkshar issu interti interfer architectur mitig util orthogon frequenc alloc adjac tier intrati interfer reduc consid two specif transmiss scheme one mimo spatial multiplex timedivis mimo beamform theoret analysi show multiti mesh network architectur achiev pernod throughput scheme long certain condit network paramet includ bandwidth number antenna number node tier satisfi case studi realist deploy 10000 node carri demonstr scalabl throughput achiev reason assumpt bandwidth number antenna
Efficient load distribution in heterogeneous vehicular networks using hierarchical controllers,"Vehicle movement poses significant challenges in vehicular networks, often resulting in uneven traffic distribution. Fog computing (FC) addresses this by operating at the network edge, handling specific tasks locally instead of relying solely on cloud computing (CC) facilities. There are instances where FC may need additional resources and must delegate tasks to CC, leading to increased delay and response time. This work conducts a thorough examination of previous load balancing (LB) strategies, with a specific focus on software-defined networking (SDN) and machine learning (ML) based LB within the internet of vehicles (IoV). The insights derived from this research expedite the development of SDN controller-based LB solutions in the IoV network. The authors proposes the integration of a local SDN controller (LSDNC) within the FC tier to enable localized LB, addressing delay concerns. However, the information will be available to the main SDN controller (MSDNC) too. The authors explore the concept mathematically and simulates the formulated model and subjecting it to a comprehensive performance analysis. The simulation results demonstrate a significant reduction in delay, with a 125 ms difference when 200 onboard units (OBUs) are used, compared to conventional software-defined vehicular networks (SDVN). This improvement continues to increase as the number of OBUs grows. Our model achieves the same maximum throughput as the previous model but delivers faster response times, as decisions are made locally without the need to wait for the main controller.",Networking,effici load distribut heterogen vehicular network use hierarch control,vehicl movement pose signific challeng vehicular network often result uneven traffic distribut fog comput fc address oper network edg handl specif task local instead reli sole cloud comput cc facil instanc fc may need addit resourc must deleg task cc lead increas delay respons time work conduct thorough examin previou load balanc lb strategi specif focu softwaredefin network sdn machin learn ml base lb within internet vehicl iov insight deriv research expedit develop sdn controllerbas lb solut iov network author propos integr local sdn control lsdnc within fc tier enabl local lb address delay concern howev inform avail main sdn control msdnc author explor concept mathemat simul formul model subject comprehens perform analysi simul result demonstr signific reduct delay 125 ms differ 200 onboard unit obu use compar convent softwaredefin vehicular network sdvn improv continu increas number obu grow model achiev maximum throughput previou model deliv faster respons time decis made local without need wait main control
Neural quantile optimization for edge–cloud networking,"We seek the best traffic allocation scheme for the edge–cloud networking subject to SD-WAN architecture and burstable billing. First, we formulate a family of quantile-based integer programming problems for a fixed network topology with random parameters describing the traffic demands. Then, to overcome the difficulty caused by the discrete feature, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem. Finally, we introduce the Gumbel-softmax sampling neural network to solve optimization problems via unsupervised learning. The neural network structure reflects the edge–cloud networking topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems. The trained network works as an efficient traffic allocation scheme sampler, outperforming the random strategy in feasibility and cost value. Besides testing the quality of the output allocation scheme, we examine the generalization property of the network by increasing the time steps and the number of users. We also feed the solution to existing integer optimization solvers as initial conditions and verify the warm-starts can accelerate the short-time iteration process. The framework is general, and the decoupled feature of the random neural networks is adequate for practical implementations.",Networking,neural quantil optim edgecloud network,seek best traffic alloc scheme edgecloud network subject sdwan architectur burstabl bill first formul famili quantilebas integ program problem fix network topolog random paramet describ traffic demand overcom difficulti caus discret featur gener gumbelsoftmax reparameter method induc unconstrain continu optim problem regular continu discret problem final introduc gumbelsoftmax sampl neural network solv optim problem via unsupervis learn neural network structur reflect edgecloud network topolog train minim expect cost function unconstrain continu optim problem train network work effici traffic alloc scheme sampler outperform random strategi feasibl cost valu besid test qualiti output alloc scheme examin gener properti network increas time step number user also feed solut exist integ optim solver initi condit verifi warmstart acceler shorttim iter process framework gener decoupl featur random neural network adequ practic implement
PETIT: PUF-enabled trust evaluation framework for IoT networks,"Internet-of-Things (IoT) is characterized by the incorporation of resource constrained devices that are inter-networked in an ad-hoc manner. Given the diversity of the devices and the operating conditions, it is important to assess the trustworthiness of IoT nodes and factor it in the network management. Contemporary trust evaluation and management schemes found in the literature mostly consider observable network-level behavior parameters and initially assume that all nodes are equally trustworthy owing to the absence of historical data or background. Such an equal trust initialization approach raises concerns in terms of accuracy, fairness, and adaptability. This paper aims to mitigate these shortcomings by proposing a novel trust evaluation and aggregation framework. Our framework leverages hardware primitives such as Physical Unclonable Functions (PUFs) to assign trust scores at the network bootstrapping phase. The paper explores the establishment of both direct and recommendation based indirect trust score evaluation and detection of irregularities to ensure the dynamic, safe, and reliable operation of the network. Simulation outcomes demonstrate that the trust value computed through this mechanism effectively and precisely mirrors the node’s credibility.",Networking,petit pufen trust evalu framework iot network,internetofth iot character incorpor resourc constrain devic internetwork adhoc manner given divers devic oper condit import assess trustworthi iot node factor network manag contemporari trust evalu manag scheme found literatur mostli consid observ networklevel behavior paramet initi assum node equal trustworthi owe absenc histor data background equal trust initi approach rais concern term accuraci fair adapt paper aim mitig shortcom propos novel trust evalu aggreg framework framework leverag hardwar primit physic unclon function puf assign trust score network bootstrap phase paper explor establish direct recommend base indirect trust score evalu detect irregular ensur dynam safe reliabl oper network simul outcom demonstr trust valu comput mechan effect precis mirror node credibl
Novel modeling and optimization for joint Cybersecurity-vs-QoS Intrusion Detection Mechanisms in 5G networks,"The rapid emergence of 5G technology brings new cybersecurity challenges that hold significant implications for our economy, society, and environment. Among these challenges, ensuring the effectiveness of Intrusion Detection Mechanisms (IDMs) in monitoring networks and detecting 5G-related cyberattacks is of utmost importance. However, optimizing cybersecurity levels and selecting appropriate IDMs remain as critical and ongoing challenges. This work considers multiple pre-deployed distributed Security Agents (SAs) across the network, each capable of running various IDMs, where they differ by their effectiveness in detecting the attacks (referred to as security term) and the consumption of resources (referred to as Quality of Service (QoS) costs). We formulate a joint security and QoS utility function leveraging the Cobb–Douglas production utility function. There are several parameters that impact the joint objective problem, including the set of elasticity parameters, that reflect the importance of the two objectives. We derive an optimal set of elasticity parameters in closed form to identify the balancing point where both objectives have equal utility values. Through comprehensive simulations, we demonstrate that increasing the detection level of SAs enhances the security utility while simultaneously diminishing the QoS utility, as more computational, bandwidth, and monetary resources are utilized for IDM processing. After optimization, our mechanism can strike an effective balance between cybersecurity and QoS overhead while demonstrating the importance of different parameters in the joint problem.",Networking,novel model optim joint cybersecurityvsqo intrus detect mechan 5g network,rapid emerg 5g technolog bring new cybersecur challeng hold signific implic economi societi environ among challeng ensur effect intrus detect mechan idm monitor network detect 5grelat cyberattack utmost import howev optim cybersecur level select appropri idm remain critic ongo challeng work consid multipl predeploy distribut secur agent sa across network capabl run variou idm differ effect detect attack refer secur term consumpt resourc refer qualiti servic qo cost formul joint secur qo util function leverag cobbdougla product util function sever paramet impact joint object problem includ set elast paramet reflect import two object deriv optim set elast paramet close form identifi balanc point object equal util valu comprehens simul demonstr increas detect level sa enhanc secur util simultan diminish qo util comput bandwidth monetari resourc util idm process optim mechan strike effect balanc cybersecur qo overhead demonstr import differ paramet joint problem
Surveying cybersecurity vulnerabilities and countermeasures for enhancing UAV security,"Drones and other forms of Unmanned Aerial Vehicles (UAVs) usages are increasing with time from military operations like surveillance, reconnaissance to commercial operations such as transportation, agriculture. The Drone market will be grown to around 43 billion USD by the 2025. With the increase in the usages, there is increase in cyber-attacks. So, Drone security and privacy are of major concern as they are used to perform the critical operations. The rapid adoption of UAVs in various sectors has prompted the need for robust and comprehensive cyber security measures. By implementing robust countermeasures, such as authentication, encryption etc. the risks posed by cyber threats can be significantly mitigated. The study investigates cyber security vulnerabilities and countermeasures in UAV systems within the scope of Authentication techniques, Physical Layer Security, Covert Communication. Relaying and Trajectory Optimization techniques are also discussed so that the flight trajectory can be optimized. It also discusses the communication modes used in UAV communication. An analysis of communication protocols is also carried out in this survey. The goal of the survey is to get the idea of cyber security threats involved in UAV communication. Our analysis emphasizes the importance of a holistic approach to UAV cyber security, leveraging the synergies among these domains to enhance overall resilience.",Networking,survey cybersecur vulner countermeasur enhanc uav secur,drone form unman aerial vehicl uav usag increas time militari oper like surveil reconnaiss commerci oper transport agricultur drone market grown around 43 billion usd 2025 increas usag increas cyberattack drone secur privaci major concern use perform critic oper rapid adopt uav variou sector prompt need robust comprehens cyber secur measur implement robust countermeasur authent encrypt etc risk pose cyber threat significantli mitig studi investig cyber secur vulner countermeasur uav system within scope authent techniqu physic layer secur covert commun relay trajectori optim techniqu also discuss flight trajectori optim also discuss commun mode use uav commun analysi commun protocol also carri survey goal survey get idea cyber secur threat involv uav commun analysi emphas import holist approach uav cyber secur leverag synergi among domain enhanc overal resili
"A survey on deep learning for cybersecurity: Progress, challenges, and opportunities","As the number of Internet-connected systems rises, cyber analysts find it increasingly difficult to effectively monitor the produced volume of data, its velocity and diversity. Signature-based cybersecurity strategies are unlikely to achieve the required performance for detecting new attack vectors. Moreover, technological advances enable attackers to develop sophisticated attack strategies that can avoid detection by current security systems. As the cyber-threat landscape worsens, we need advanced tools and technologies to detect, investigate, and make quick decisions regarding emerging attacks and threats. Applications of artificial intelligence (AI) have the potential to analyze and automatically classify vast amounts of Internet traffic. AI-based solutions that automate the detection of attacks and tackle complex cybersecurity problems are gaining increasing attention. This paper comprehensively presents the promising applications of deep learning, a subfield of AI based on multiple layers of artificial neural networks, in a wide variety of security tasks. Before critically and comparatively surveying state-of-the-art solutions from the literature, we discuss the key characteristics of representative deep learning architectures employed in cybersecurity applications, we introduce the emerging trends in deep learning, and we provide an overview of necessary resources like a generic framework and suitable datasets. We identify the limitations of the reviewed works, and we bring forth a vision of the current challenges of the area, providing valuable insights and good practices for researchers and developers working on related problems. Finally, we uncover current pain points and outline directions for future research to address them.",Networking,survey deep learn cybersecur progress challeng opportun,number internetconnect system rise cyber analyst find increasingli difficult effect monitor produc volum data veloc divers signaturebas cybersecur strategi unlik achiev requir perform detect new attack vector moreov technolog advanc enabl attack develop sophist attack strategi avoid detect current secur system cyberthreat landscap worsen need advanc tool technolog detect investig make quick decis regard emerg attack threat applic artifici intellig ai potenti analyz automat classifi vast amount internet traffic aibas solut autom detect attack tackl complex cybersecur problem gain increas attent paper comprehens present promis applic deep learn subfield ai base multipl layer artifici neural network wide varieti secur task critic compar survey stateoftheart solut literatur discuss key characterist repres deep learn architectur employ cybersecur applic introduc emerg trend deep learn provid overview necessari resourc like gener framework suitabl dataset identifi limit review work bring forth vision current challeng area provid valuabl insight good practic research develop work relat problem final uncov current pain point outlin direct futur research address
A differential game view of antagonistic dynamics for cybersecurity,"A fundamental understanding of the dynamics of the antagonism between the attacker and the defender can strengthen forward-looking and strategic planning for the cybersecurity. Most of the existing works have one or more of the following limitations: static modeling, non co-evolved strategy and synchronized setting, not accounting for the dynamic nature of the cyber arms race between the defensive and offensive sides. In this paper, we apply differential game theory to analyze the antagonistic dynamics between the attacker and the defender and study two complementary game theoretic formulations, namely the black-box game the and the white-box game: The black-box game can cogently model the opaque-information confrontation and the white-box one is capable to depict the partially transparent-information confrontation. To gain insight into the evolution rule of the antagonistic dynamics between the defensive and the offensive sides, we establish the existence of the Nash equilibrium and the corresponding conditions in the black-box game. Moreover, an iteration algorithm is designed to find the numerical solutions of the Nash equilibrium in the general case and further its analytical solutions in a special case for the white-box game. Extensive numerical simulations have been carried out to evaluate the performance of the proposed games.",Networking,differenti game view antagonist dynam cybersecur,fundament understand dynam antagon attack defend strengthen forwardlook strateg plan cybersecur exist work one follow limit static model non coevolv strategi synchron set account dynam natur cyber arm race defens offens side paper appli differenti game theori analyz antagonist dynam attack defend studi two complementari game theoret formul name blackbox game whitebox game blackbox game cogent model opaqueinform confront whitebox one capabl depict partial transparentinform confront gain insight evolut rule antagonist dynam defens offens side establish exist nash equilibrium correspond condit blackbox game moreov iter algorithm design find numer solut nash equilibrium gener case analyt solut special case whitebox game extens numer simul carri evalu perform propos game
"Cybersecurity in industrial control systems: Issues, technologies, and challenges","Industrial Control Systems (ICSs) play an important role in today’s industry by providing process automation, distributed control, and process monitoring. ICS was designed to be used in an isolated area or connected to other systems via specialised communication mechanisms or protocols. This setup allows manufacturers to manage their production processes with great flexibility and safety. However, this design does not meet today’s business requirements to work with state-of-the-art technologies such as Internet-of-Things (IoT) and big data analytics. In order to fulfil industry requirements, many ICSs have been connected to enterprise networks that allow business users to access real-time data generated by power plants. At the same time, this new design opens up several cybersecurity challenges for ICSs.
We review possible cyber attacks on ICSs, identify typical threats and vulnerabilities, and we discuss unresolved security issues with existing ICS cybersecurity solutions. Then, we discuss how to secure ICSs (e.g., using risk assessment methodologies) and other protection measures. We also identify open security research challenges for ICSs, and we present a classification of existing security solutions along with their strengths and weaknesses. Finally, we provide future research directions in ICS security.",Networking,cybersecur industri control system issu technolog challeng,industri control system icss play import role today industri provid process autom distribut control process monitor ic design use isol area connect system via specialis commun mechan protocol setup allow manufactur manag product process great flexibl safeti howev design meet today busi requir work stateoftheart technolog internetofth iot big data analyt order fulfil industri requir mani icss connect enterpris network allow busi user access realtim data gener power plant time new design open sever cybersecur challeng icss review possibl cyber attack icss identifi typic threat vulner discuss unresolv secur issu exist ic cybersecur solut discuss secur icss eg use risk assess methodolog protect measur also identifi open secur research challeng icss present classif exist secur solut along strength weak final provid futur research direct ic secur
A systematic evaluation of cybersecurity metrics for dynamic networks,"It is difficult to assess the security of modern networks because they are usually dynamic with configuration changes (such as changes in topology, firewall rules, etc). Graphical security models (e.g., Attack Graphs and Attack Trees) are widely used to systematically analyse the security posture of network systems using security metrics. However, there are problems using them to assess the security of dynamic networks. First, most models are unable to capture dynamic changes occurring in the networks over time. Second, the existing security metrics are not designed for the analysis of dynamic networks and hence their effectiveness to the dynamic changes in the network still remains unclear.
In this paper, we systematically categorise network changes into two categories (i.e., changes in hosts and changes in edges). We conduct a comprehensive analysis to evaluate the effectiveness of security metrics using a Temporal Hierarchical Attack Representation Model, which can capture and analyse the changes in the security of network systems. Further, we investigate the varying effects of security metrics when changes are observed in the dynamic networks.
Our simulation results show that different security metrics (except the shortest attack path) have varying security posture changes with respect to changes in the network (when we introduce time to them). However, none of the security metrics consistently changes for all the network changes that we observe in our scenarios. Hence, the results provide some insights into what security metrics can change (accordingly) when a particular network change is observed. It also provides a foundation for further research in this area.",Networking,systemat evalu cybersecur metric dynam network,difficult assess secur modern network usual dynam configur chang chang topolog firewal rule etc graphic secur model eg attack graph attack tree wide use systemat analys secur postur network system use secur metric howev problem use assess secur dynam network first model unabl captur dynam chang occur network time second exist secur metric design analysi dynam network henc effect dynam chang network still remain unclear paper systemat categoris network chang two categori ie chang host chang edg conduct comprehens analysi evalu effect secur metric use tempor hierarch attack represent model captur analys chang secur network system investig vari effect secur metric chang observ dynam network simul result show differ secur metric except shortest attack path vari secur postur chang respect chang network introduc time howev none secur metric consist chang network chang observ scenario henc result provid insight secur metric chang accordingli particular network chang observ also provid foundat research area
CGAN-based cyber deception framework against reconnaissance attacks in ICS,"In recent years, Industrial Control Systems (ICSs) have faced increasing vulnerability to cyber attacks due to their integration with the Internet. Despite efforts to enhance cybersecurity, reconnaissance attacks remain a significant threat, prompting the need for innovative defensive strategies. This paper introduces a novel approach to strengthen the defensive capabilities of ICS networks against reconnaissance attacks using machine learning-driven cyber deception techniques. Leveraging Conditional Generative Adversarial Networks (CGANs), the proposed framework dynamically generates defensive network topologies to network shuffling and implement deception strategies, prioritizing system availability. Extensive simulations demonstrate the superior efficacy of the proposed framework in enhancing cybersecurity while minimizing computational overhead. By effectively mitigating reconnaissance attacks, this solution reinforces the resilience of ICS networks, safeguarding critical industrial infrastructure from evolving cyber threats. These findings underscore the significance of adopting machine learning-based cyber deception as a pragmatic security measure for protecting ICS networks in real-world industrial contexts.",Networking,cganbas cyber decept framework reconnaiss attack ic,recent year industri control system icss face increas vulner cyber attack due integr internet despit effort enhanc cybersecur reconnaiss attack remain signific threat prompt need innov defens strategi paper introduc novel approach strengthen defens capabl ic network reconnaiss attack use machin learningdriven cyber decept techniqu leverag condit gener adversari network cgan propos framework dynam gener defens network topolog network shuffl implement decept strategi priorit system avail extens simul demonstr superior efficaci propos framework enhanc cybersecur minim comput overhead effect mitig reconnaiss attack solut reinforc resili ic network safeguard critic industri infrastructur evolv cyber threat find underscor signific adopt machin learningbas cyber decept pragmat secur measur protect ic network realworld industri context
A generic learning simulation framework to assess security strategies in cyber-physical production systems,"Connected systems through computerized networks are at the heart of the Industry of the future. As they merge physical entities with cyber spaces, they fall under the paradigm of cyber-physical production systems. Cybersecurity is a key challenge for such systems, as they are subject to daily attempts of intruders to gain unauthorized access to their internal resources or to compromise their integrity. The fast increase of new attack strategies requires the rapid design and assessment of new defense strategies. It entails a complex, error-prone and time-consuming process, including the clear specification of the attack and defense strategies involved, and the design and implementation of the simulation model allowing to evaluate the performances of the defense strategy. This work intends to make such a process transparent to cybersecurity managers by limiting their workload to the sole specification of the characteristics of the system and the logic of the attack and the defense. It provides a generic hybrid simulation framework for flexible evaluation of cybersecurity policies, which is demonstrated on a SYN flooding application. Therefore, the contribution is twofold: (1) The proposed framework offers a high-level environment allowing various experts to collaborate by graphically modeling a given attack strategy and the envisioned defense strategy, without engaging in heavy implementation efforts. Then the framework's executable infrastructure, which combines simulation with machine learning to understanding the interactions between the attackers & the defender, will allow them assessing the performances of these strategies. The proposed framework differs from state-of-the-art cybersecurity simulation environments in its uniqueness to combining the expressive power of a universal simulation modeling formalism with the user-friendliness of a visual simulation tool. Therefore, it offers at one side, a very high modeling flexibility for easy exploration of various cybersecurity strategies, and at the other side, integrated learning capabilities for allowing self-adaptive user-based cybersecurity strategy design. (2) The application demonstrating the framework focuses on the most encountered and still uncontrolled threats in cybersecurity, i.e. the SYN-Flooding based Denial of Service (DoS) attack. The application targeted is not meant to propose yet another SYN flood detection algorithm or to improve the state-of-the-art in that domain, but to prove the framework operationality. The experimental results obtained showcase the ability of the framework to support learning simulation-based SYN flood defense algorithm design and validation.",Networking,gener learn simul framework assess secur strategi cyberphys product system,connect system computer network heart industri futur merg physic entiti cyber space fall paradigm cyberphys product system cybersecur key challeng system subject daili attempt intrud gain unauthor access intern resourc compromis integr fast increas new attack strategi requir rapid design assess new defens strategi entail complex errorpron timeconsum process includ clear specif attack defens strategi involv design implement simul model allow evalu perform defens strategi work intend make process transpar cybersecur manag limit workload sole specif characterist system logic attack defens provid gener hybrid simul framework flexibl evalu cybersecur polici demonstr syn flood applic therefor contribut twofold 1 propos framework offer highlevel environ allow variou expert collabor graphic model given attack strategi envis defens strategi without engag heavi implement effort framework execut infrastructur combin simul machin learn understand interact attack defend allow assess perform strategi propos framework differ stateoftheart cybersecur simul environ uniqu combin express power univers simul model formal userfriendli visual simul tool therefor offer one side high model flexibl easi explor variou cybersecur strategi side integr learn capabl allow selfadapt userbas cybersecur strategi design 2 applic demonstr framework focus encount still uncontrol threat cybersecur ie synflood base denial servic do attack applic target meant propos yet anoth syn flood detect algorithm improv stateoftheart domain prove framework operation experiment result obtain showcas abil framework support learn simulationbas syn flood defens algorithm design valid
SPEAR SIEM: A Security Information and Event Management system for the Smart Grid,"The technological leap of smart technologies has brought the conventional electrical grid in a new digital era called Smart Grid (SG), providing multiple benefits, such as two-way communication, pervasive control and self-healing. However, this new reality generates significant cybersecurity risks due to the heterogeneous and insecure nature of SG. In particular, SG relies on legacy communication protocols that have not been implemented having cybersecurity in mind. Moreover, the advent of the Internet of Things (IoT) creates severe cybersecurity challenges. The Security Information and Event Management (SIEM) systems constitute an emerging technology in the cybersecurity area, having the capability to detect, normalise and correlate a vast amount of security events. They can orchestrate the entire security of a smart ecosystem, such as SG. Nevertheless, the current SIEM systems do not take into account the unique SG peculiarities and characteristics like the legacy communication protocols. In this paper, we present the Secure and PrivatE smArt gRid (SPEAR) SIEM, which focuses on SG. The main contribution of our work is the design and implementation of a SIEM system capable of detecting, normalising and correlating cyberattacks and anomalies against a plethora of SG application-layer protocols. It is noteworthy that the detection performance of the SPEAR SIEM is demonstrated with real data originating from four real SG use case (a) hydropower plant, (b) substation, (c) power plant and (d) smart home.",Networking,spear siem secur inform event manag system smart grid,technolog leap smart technolog brought convent electr grid new digit era call smart grid sg provid multipl benefit twoway commun pervas control selfheal howev new realiti gener signific cybersecur risk due heterogen insecur natur sg particular sg reli legaci commun protocol implement cybersecur mind moreov advent internet thing iot creat sever cybersecur challeng secur inform event manag siem system constitut emerg technolog cybersecur area capabl detect normalis correl vast amount secur event orchestr entir secur smart ecosystem sg nevertheless current siem system take account uniqu sg peculiar characterist like legaci commun protocol paper present secur privat smart grid spear siem focus sg main contribut work design implement siem system capabl detect normalis correl cyberattack anomali plethora sg applicationlay protocol noteworthi detect perform spear siem demonstr real data origin four real sg use case hydropow plant b substat c power plant smart home
AAE-DSVDD: A one-class classification model for VPN traffic identification,"Virtual Private Network(VPN) can provide a concealed transmission channel for communication and protect the privacy of users. However, it also brings hidden dangers to cybersecurity with its wide application. Malicious behavior or harmful information can be transmitted secretly through VPN tunnels to avoid firewall censorship. Therefore, VPN traffic identification is an important part of ensure cybersecurity. Although many efforts have been made for VPN traffic identification, existing methods mainly focus on supervised learning models. In this paper, we propose an one-class classification model called AAE-DSVDD for VPN traffic identification. First, we introduce Adversarial AutoEncoder(AAE) for preliminary modeling of VPN traffic. AAE can match the aggregated posterior distribution of the hidden layer to an arbitrary prior distribution. It associates the samples with a normal distribution in the hidden space. Secondly, We implement representation learning for VPN traffic via Deep Support Vector Data Description(DSVDD). A standardized method is designed to match the output distribution of DSVDD with the aggregated posterior distribution of AAE. It alleviates the hypersphere collapse problem of DSVDD and improves identification performance. Finally, we verify the abilities of the AAE-DSVDD model on the public dataset ISCXVPN. Compared with other one-class models, AAE-DSVDD achieved the best identification ability for VPN traffic identification. It also improves the recognition ability when identifying strange classes that are not included in the training data.",Networking,aaedsvdd oneclass classif model vpn traffic identif,virtual privat networkvpn provid conceal transmiss channel commun protect privaci user howev also bring hidden danger cybersecur wide applic malici behavior harm inform transmit secretli vpn tunnel avoid firewal censorship therefor vpn traffic identif import part ensur cybersecur although mani effort made vpn traffic identif exist method mainli focu supervis learn model paper propos oneclass classif model call aaedsvdd vpn traffic identif first introduc adversari autoencoderaa preliminari model vpn traffic aae match aggreg posterior distribut hidden layer arbitrari prior distribut associ sampl normal distribut hidden space secondli implement represent learn vpn traffic via deep support vector data descriptiondsvdd standard method design match output distribut dsvdd aggreg posterior distribut aae allevi hyperspher collaps problem dsvdd improv identif perform final verifi abil aaedsvdd model public dataset iscxvpn compar oneclass model aaedsvdd achiev best identif abil vpn traffic identif also improv recognit abil identifi strang class includ train data
DoS/DDoS-MQTT-IoT: A dataset for evaluating intrusions in IoT networks using the MQTT protocol,"Adversaries may exploit a range of vulnerabilities in Internet of Things (IoT) environments. These vulnerabilities are typically exploited to carry out attacks, such as denial-of-service (DoS) attacks, either against the IoT devices themselves, or using the devices to perform the attacks. These attacks are often successful due to the nature of the protocols used in the IoT. One popular protocol used for machine-to-machine IoT communications is the Message Queueing Telemetry Protocol (MQTT). Countermeasures for attacks against MQTT include testing defenses with existing datasets. However, there is a lack of real-world test datasets in this area. For this reason, this paper introduces a DoS/DDoS-MQTT-IoT dataset—that contains various DoS/DDoS attack scenarios using MQTT traffic—to help develop and test countermeasures against such attacks. To this end, a physical IoT testbed was constructed and a large volume of IoT data was generated that included standard MQTT traffic as well as 10 DoS scenarios. The usability of the dataset has been evaluated via machine learning.",Networking,dosddosmqttiot dataset evalu intrus iot network use mqtt protocol,adversari may exploit rang vulner internet thing iot environ vulner typic exploit carri attack denialofservic do attack either iot devic use devic perform attack attack often success due natur protocol use iot one popular protocol use machinetomachin iot commun messag queue telemetri protocol mqtt countermeasur attack mqtt includ test defens exist dataset howev lack realworld test dataset area reason paper introduc dosddosmqttiot datasetthat contain variou dosddo attack scenario use mqtt trafficto help develop test countermeasur attack end physic iot testb construct larg volum iot data gener includ standard mqtt traffic well 10 do scenario usabl dataset evalu via machin learn
Towards a Cyber Resilience Quantification Framework (CRQF) for IT infrastructure,"Cyber resilience quantification is the process of evaluating and measuring an organisation’s ability to withstand, adapt to, and recover from cyber-attacks. It involves estimating IT systems, networks, and response strategies to ensure robust defence and effective recovery mechanisms in the event of a cyber-attack. Quantifying cyber resilience can be difficult due to the constantly changing components of IT infrastructure. Traditional methods like vulnerability assessments and penetration testing may not be effective. Measuring cyber resilience is essential to evaluate and strengthen an organisation’s preparedness against evolving cyber-attacks. It helps identify weaknesses, allocate resources, and ensure the uninterrupted operation of critical systems and information. There are various methods for measuring cyber resilience, such as evaluating, teaming and testing, and creating simulated models. This article proposes a cyber resilience quantification framework for IT infrastructure that utilises a simulation approach. This approach enables organisations to simulate different attack scenarios, identify vulnerabilities, and improve their cyber resilience. The comparative analysis of cyber resilience factors highlights pre-configuration’s robust planning and adaptation (61.44%), buffering supported’s initial readiness (44.53%), and network topologies’ robust planning but weak recovery and adaptation (60.04% to 77.86%), underscoring the need for comprehensive enhancements across all phases. The utilisation of the proposed factors is crucial in conducting a comprehensive evaluation of IT infrastructure in the event of a cyber-attack.",Networking,toward cyber resili quantif framework crqf infrastructur,cyber resili quantif process evalu measur organis abil withstand adapt recov cyberattack involv estim system network respons strategi ensur robust defenc effect recoveri mechan event cyberattack quantifi cyber resili difficult due constantli chang compon infrastructur tradit method like vulner assess penetr test may effect measur cyber resili essenti evalu strengthen organis prepared evolv cyberattack help identifi weak alloc resourc ensur uninterrupt oper critic system inform variou method measur cyber resili evalu team test creat simul model articl propos cyber resili quantif framework infrastructur utilis simul approach approach enabl organis simul differ attack scenario identifi vulner improv cyber resili compar analysi cyber resili factor highlight preconfigur robust plan adapt 6144 buffer support initi readi 4453 network topolog robust plan weak recoveri adapt 6004 7786 underscor need comprehens enhanc across phase utilis propos factor crucial conduct comprehens evalu infrastructur event cyberattack
Dataset of attacks on a live enterprise VoIP network for machine learning based intrusion detection and prevention systems,This data article presents a dataset which can be used to train machine learning (ML) algorithms towards intrusion detection and prevention systems (IDS/IPS). The dataset applies to the field of unified communications in voice over internet protocol (VoIP) networks. Information related to the design and implementation of a real enterprise VoIP network is provided along with the specific protocols used. The attack tools used to disrupt the VoIP communications and the resulting data collected are uniquely presented in sub-datasets. Guidance on how to use the dataset and benefit from the raw packet captures is provided to support research and development in IDS/IPS systems.,Networking,dataset attack live enterpris voip network machin learn base intrus detect prevent system,data articl present dataset use train machin learn ml algorithm toward intrus detect prevent system idsip dataset appli field unifi commun voic internet protocol voip network inform relat design implement real enterpris voip network provid along specif protocol use attack tool use disrupt voip commun result data collect uniqu present subdataset guidanc use dataset benefit raw packet captur provid support research develop idsip system
FTG-Net-E: A hierarchical ensemble graph neural network for DDoS attack detection,"Distributed Denial-of-Service (DDoS) attacks are a major threat to computer networks. These attacks can be carried out by flooding a network with malicious traffic, overwhelming its resources, and/or making it unavailable to legitimate users. Existing machine learning methods for DDoS attack detection typically use statistical features of network traffic, such as packet sizes and inter-arrival times. However, these methods often fail to capture the complex relationships between different traffic flows. This paper proposes a new DDoS attack detection approach that uses Graph Neural Networks (GNN) ensemble learning. GNN ensemble learning is a type of machine learning that combines multiple GNN models to improve the detection accuracy. We evaluated our approach on the Canadian Institute for Cybersecurity Intrusion Detection Evaluation Dataset (CICIDS2018) and CICIDS2017 datasets, a benchmark dataset for DDoS attack detection. Our work provides two main contributions. First, we extend our DDoS attack detection approach using GNN ensemble learning. Second, we explore the evaluation and fine-tuning of hyperparameter metrics through ensemble learning, significantly enhancing accuracy compared to a single GNN model and achieving an average 3.2% higher F1-score. Additionally, our approach effectively reduces overfitting by incorporating regularization techniques, such as dropout and early stopping. Specifically, we use a hierarchical ensemble of GNN, where each GNN learns the relationships between traffic flows at a different granularity level. We then use bagging and boosting to combine the predictions of the individual GNN, further improving detection accuracy. Results show that our system can achieve 99.67% accuracy, with a F1-score of 99.29%, which is better than state-of-the-art methods, even using single traffic architecture.",Networking,ftgnete hierarch ensembl graph neural network ddo attack detect,distribut denialofservic ddo attack major threat comput network attack carri flood network malici traffic overwhelm resourc andor make unavail legitim user exist machin learn method ddo attack detect typic use statist featur network traffic packet size interarriv time howev method often fail captur complex relationship differ traffic flow paper propos new ddo attack detect approach use graph neural network gnn ensembl learn gnn ensembl learn type machin learn combin multipl gnn model improv detect accuraci evalu approach canadian institut cybersecur intrus detect evalu dataset cicids2018 cicids2017 dataset benchmark dataset ddo attack detect work provid two main contribut first extend ddo attack detect approach use gnn ensembl learn second explor evalu finetun hyperparamet metric ensembl learn significantli enhanc accuraci compar singl gnn model achiev averag 32 higher f1score addit approach effect reduc overfit incorpor regular techniqu dropout earli stop specif use hierarch ensembl gnn gnn learn relationship traffic flow differ granular level use bag boost combin predict individu gnn improv detect accuraci result show system achiev 9967 accuraci f1score 9929 better stateoftheart method even use singl traffic architectur
"Distributed denial of service attack prediction: Challenges, open issues and opportunities","Distributed Denial of Service (DDoS) attack is one of the biggest cyber threats. DDoS attacks have evolved in quantity and volume to evade detection and increase damage. Changes during the COVID-19 pandemic have left traditional perimeter-based security measures vulnerable to attackers that have diversified their activities by targeting health services, e-commerce, and educational services. DDoS attack prediction searches for signals of attack preparation to warn about the imminence of the attack. Prediction is necessary to handle high-volumetric DDoS attacks and to increase the time to defend against them. This survey article presents the classification of studies from the literature comprising the current state-of-the-art on DDoS attack prediction. It highlights the results of this extensive literature review categorizing the works by prediction time, architecture, employed methodology, and the type of data utilized to predict attacks. Further, this survey details each identified study and, finally, it emphasizes the research opportunities to evolve the DDoS attack prediction state-of-the-art.",Networking,distribut denial servic attack predict challeng open issu opportun,distribut denial servic ddo attack one biggest cyber threat ddo attack evolv quantiti volum evad detect increas damag chang covid19 pandem left tradit perimeterbas secur measur vulner attack diversifi activ target health servic ecommerc educ servic ddo attack predict search signal attack prepar warn immin attack predict necessari handl highvolumetr ddo attack increas time defend survey articl present classif studi literatur compris current stateoftheart ddo attack predict highlight result extens literatur review categor work predict time architectur employ methodolog type data util predict attack survey detail identifi studi final emphas research opportun evolv ddo attack predict stateoftheart
Passive operating system fingerprinting revisited: Evaluation and current challenges,"Fingerprinting a host's operating system is a very common yet precarious task in network, asset, and vulnerability management. Estimating the operating system via network traffic analysis may leverage TCP/IP header parameters or complex analysis of hosts' behavior using machine learning. However, the existing approaches are becoming obsolete as network traffic evolves which makes the problem still open. This paper discusses various approaches to passive OS fingerprinting and their evolution in the past twenty years. We illustrate their usage, compare their results in an experiment, and list challenges faced by the current fingerprinting approaches. The hosts' differences in network stack settings were initially the most important information source for OS fingerprinting, which is now complemented by hosts' behavioral analysis and combined approaches backed by machine learning. The most impactful reasons for this evolution were the Internet-wide network traffic encryption and the general adoption of privacy-preserving concepts in application protocols. Other changes, such as the increasing proliferation of web applications on handheld devices, raised the need to identify these devices in the networks, for which we may use the techniques of OS fingerprinting.",Networking,passiv oper system fingerprint revisit evalu current challeng,fingerprint host oper system common yet precari task network asset vulner manag estim oper system via network traffic analysi may leverag tcpip header paramet complex analysi host behavior use machin learn howev exist approach becom obsolet network traffic evolv make problem still open paper discuss variou approach passiv os fingerprint evolut past twenti year illustr usag compar result experi list challeng face current fingerprint approach host differ network stack set initi import inform sourc os fingerprint complement host behavior analysi combin approach back machin learn impact reason evolut internetwid network traffic encrypt gener adopt privacypreserv concept applic protocol chang increas prolifer web applic handheld devic rais need identifi devic network may use techniqu os fingerprint
"Satellite-based communications security: A survey of threats, solutions, and research challenges ","Satellite-based Communication (SATCOM) systems are gaining renewed momentum in Industry and Academia, thanks to innovative services introduced by leading tech companies and the promising impact they can deliver towards the global connectivity objective tackled by early 6G initiatives. On the one hand, the emergence of new manufacturing processes and radio technologies promises to reduce service costs while guaranteeing outstanding communication latency, available bandwidth, flexibility, and coverage range. On the other hand, cybersecurity techniques and solutions applied in SATCOM links should be updated to reflect the substantial advancements in attacker capabilities characterizing the last two decades. However, business urgency and opportunities are leading operators towards challenging system trade-offs, resulting in an increased attack surface and a general relaxation of the available security services.
In this paper, we tackle the cited problems and present a comprehensive survey on the link-layer security threats, solutions, and challenges faced when deploying and operating SATCOM systems. Specifically, we classify the literature on security for SATCOM systems into two main branches, i.e., physical-layer security and cryptography schemes. Then, we further identify specific research domains for each of the identified branches, focusing on dedicated security issues, including, e.g., physical-layer confidentiality, anti-jamming schemes, anti-spoofing strategies, and quantum-based key distribution schemes. For each of the above domains, we highlight the most essential techniques, peculiarities, advantages, disadvantages, lessons learned, and future directions. Finally, we also identify emerging research topics whose additional investigation by Academia and Industry could further attract researchers and investors, ultimately unleashing the full potential behind ubiquitous satellite communications.",Networking,satellitebas commun secur survey threat solut research challeng,satellitebas commun satcom system gain renew momentum industri academia thank innov servic introduc lead tech compani promis impact deliv toward global connect object tackl earli 6g initi one hand emerg new manufactur process radio technolog promis reduc servic cost guarante outstand commun latenc avail bandwidth flexibl coverag rang hand cybersecur techniqu solut appli satcom link updat reflect substanti advanc attack capabl character last two decad howev busi urgenc opportun lead oper toward challeng system tradeoff result increas attack surfac gener relax avail secur servic paper tackl cite problem present comprehens survey linklay secur threat solut challeng face deploy oper satcom system specif classifi literatur secur satcom system two main branch ie physicallay secur cryptographi scheme identifi specif research domain identifi branch focus dedic secur issu includ eg physicallay confidenti antijam scheme antispoof strategi quantumbas key distribut scheme domain highlight essenti techniqu peculiar advantag disadvantag lesson learn futur direct final also identifi emerg research topic whose addit investig academia industri could attract research investor ultim unleash full potenti behind ubiquit satellit commun
"A survey on network simulators, emulators, and testbeds used for research and education","Network operators and researchers constantly search for platforms to evaluate future deployments and test new research ideas. When experimenting, they usually face challenges in deciding on an appropriate platform to validate the advantages and limitations of their proposed system. These challenges include finding an experimentation environment that balances traffic realism, scalability, and cost. An experimenter can evaluate systems, protocols, and security implementations using simulators, emulators, or testbeds to validate the expected behavior of the proposed idea. Simulators and emulators provide a controlled environment to conduct reproducible experiments but lack realism. Testbeds provide realism and scale depending on the available resources. However, real equipment can be costly and unavailable for many experimenters. The inability to test networking ideas in a realistic environment at a large scale presents a barrier for companies, institutions, and network vendors to implement new features, thus, slowing down innovation. In the past few decades, the networking community developed new platforms to test new ideas and deployments at scale, with realism, and at lower costs. These platforms also enable the instruction of networking concepts, cybersecurity, distributed computing, storage systems, and science applications. From the learner’s side, practical hands-on experience is required to internalize concepts and improve troubleshooting skills. Learning these concepts can be challenging due to the multidisciplinary nature of networking instruction, where a learner must have a background in several computing areas (e.g., operating systems, programming languages, and computer architecture). This paper presents experimentation platforms used to conduct research in computer networks and evaluates the potential of these platforms for instructing networking courses. This paper examines the literature and presents a taxonomy of network experimentation platforms. It also discusses challenges, analyzes the limitations, and suggests future perspectives by providing an overview of the tools, a description of the underlying resources (i.e., hardware and software), and a summary of the supported experiments. The paper aims to assist experimenters and educators in deciding which platform is more suitable for their experimentation needs and discuss the challenges and future directions related to the network experimentation platforms.",Networking,survey network simul emul testb use research educ,network oper research constantli search platform evalu futur deploy test new research idea experi usual face challeng decid appropri platform valid advantag limit propos system challeng includ find experiment environ balanc traffic realism scalabl cost experiment evalu system protocol secur implement use simul emul testb valid expect behavior propos idea simul emul provid control environ conduct reproduc experi lack realism testb provid realism scale depend avail resourc howev real equip costli unavail mani experiment inabl test network idea realist environ larg scale present barrier compani institut network vendor implement new featur thu slow innov past decad network commun develop new platform test new idea deploy scale realism lower cost platform also enabl instruct network concept cybersecur distribut comput storag system scienc applic learner side practic handson experi requir intern concept improv troubleshoot skill learn concept challeng due multidisciplinari natur network instruct learner must background sever comput area eg oper system program languag comput architectur paper present experiment platform use conduct research comput network evalu potenti platform instruct network cours paper examin literatur present taxonomi network experiment platform also discuss challeng analyz limit suggest futur perspect provid overview tool descript underli resourc ie hardwar softwar summari support experi paper aim assist experiment educ decid platform suitabl experiment need discuss challeng futur direct relat network experiment platform
Efficient intrusion detection toward IoT networks using cloud–edge collaboration,"The Internet of Things (IoT) is increasingly utilized in daily life and industrial production, particularly in critical infrastructures. IoT cybersecurity has an effect on people’s safety, national security, and economic growth. However, the traditional cloud-based centralized intrusion detection methods cannot satisfy the demands for data privacy, network load, and timely response. In this article, we proposed an efficient intrusion detection method based on cloud–edge collaboration. The approach can reduce computational workload to speed up model training, prevent data privacy leakage, enrich training data, and detect attacks unknown to local edge devices. Specifically, stacked sparse autoencoder (SSAE) is first utilized for data dimensionality reduction to overcome the bottleneck of resource constraints on edge devices. Second, considering the long-term serial characteristics of IoT traffic data, the temporal convolutional network (TCN) model is employed to detect attack; Finally, the cloud–edge collaboration architecture based on federated learning is used to coordinate multi-party training intrusion detection models. It fills the gap of missed detection by intrusion detection models due to data silos. Experiments show that our method reduced the training time, storage and memory required for the model training process by more than 50%, respectively, but the detection accuracy is close to centralized trained models. The model trained based on cloud–edge collaboration can identify attacks unknown to local edge devices.",Networking,effici intrus detect toward iot network use cloudedg collabor,internet thing iot increasingli util daili life industri product particularli critic infrastructur iot cybersecur effect peopl safeti nation secur econom growth howev tradit cloudbas central intrus detect method satisfi demand data privaci network load time respons articl propos effici intrus detect method base cloudedg collabor approach reduc comput workload speed model train prevent data privaci leakag enrich train data detect attack unknown local edg devic specif stack spars autoencod ssae first util data dimension reduct overcom bottleneck resourc constraint edg devic second consid longterm serial characterist iot traffic data tempor convolut network tcn model employ detect attack final cloudedg collabor architectur base feder learn use coordin multiparti train intrus detect model fill gap miss detect intrus detect model due data silo experi show method reduc train time storag memori requir model train process 50 respect detect accuraci close central train model model train base cloudedg collabor identifi attack unknown local edg devic
Implementing zero trust security with dual fuzzy methodology for trust-aware authentication and task offloading in Multi-access Edge Computing,"This paper proposes an efficient trust-aware authentication and task offloading scheme for Multi-Access Edge Computing (MEC) using the Zero Trust Security (ZTS) principles. The proposed method uses a dual fuzzy logic system to evaluate the trustworthiness of edge servers. Devices connected to the edge servers are authenticated using identity, biometrics and Physical Unclonable Function (PUF) measures. After authentication, tasks can be offloaded from the devices to the most trustworthy edge server. The proposed scheme also considers the resource constraints of the edge servers and aims to minimise the overall task completion time. The experimental results show that the proposed scheme outperforms existing schemes regarding authentication accuracy, task completion time, and energy consumption.",Networking,implement zero trust secur dual fuzzi methodolog trustawar authent task offload multiaccess edg comput,paper propos effici trustawar authent task offload scheme multiaccess edg comput mec use zero trust secur zt principl propos method use dual fuzzi logic system evalu trustworthi edg server devic connect edg server authent use ident biometr physic unclon function puf measur authent task offload devic trustworthi edg server propos scheme also consid resourc constraint edg server aim minimis overal task complet time experiment result show propos scheme outperform exist scheme regard authent accuraci task complet time energi consumpt
"A practical framework for cyber defense generation, enforcement and evaluation","It is challenging to enforce and evaluate cyber-defenses for large networks. The current state-of-the-art approaches on defense enforcement and evaluations are manually performed by a security expert and they are executed separately without using a defined workflow. Moreover, the cyber defense generation, enforcement, and evaluation require costs, time, and effort of the security experts to effectively identify attacks and the best set of defenses to deploy. To effectively address these challenges, we propose a novel defense automation framework named CD-GEE (Cyber Defense Generation, Enforcement and Evaluation). CD-GEE can perform automated Cyber defense generation, defense enforcement and security evaluation without human intervention. CD-GEE works in the following five phases: (1) host/network data collection, (2) graphical security models construction, (3) cyber defense generation & selection, (4) defense deployment, and (5) defense evaluation and report. To show the usability and applicability of CD-GEE, we perform experiments on different networks including Amazon’s Elastic Compute Cloud. Besides, we also evaluate the performance and the effectiveness of the defenses deployed via our proposed framework and tool. We show that the CD-GEE measures the security posture of the network before and after defense deployment and evaluation. We also showed that our proposed framework performed better than without the framework in enhancing the security of networks on both the Amazon Elastic Compute Cloud and simulation network environment.",Networking,practic framework cyber defens gener enforc evalu,challeng enforc evalu cyberdefens larg network current stateoftheart approach defens enforc evalu manual perform secur expert execut separ without use defin workflow moreov cyber defens gener enforc evalu requir cost time effort secur expert effect identifi attack best set defens deploy effect address challeng propos novel defens autom framework name cdgee cyber defens gener enforc evalu cdgee perform autom cyber defens gener defens enforc secur evalu without human intervent cdgee work follow five phase 1 hostnetwork data collect 2 graphic secur model construct 3 cyber defens gener select 4 defens deploy 5 defens evalu report show usabl applic cdgee perform experi differ network includ amazon elast comput cloud besid also evalu perform effect defens deploy via propos framework tool show cdgee measur secur postur network defens deploy evalu also show propos framework perform better without framework enhanc secur network amazon elast comput cloud simul network environ
Designing and implementing an AUTOSAR-based Basic Software Module for enhanced security,"Electronic Control Units (ECUs) communicate with each other to accomplish the functionalities of modern vehicles. ECUs form an in-vehicle network that is precisely regulated and must be adequately protected from malicious activity, which has had several outbreaks in recent years. Therefore, we present CINNAMON, an AUTOSAR-based Basic Software Module that aims at confidentiality, integrity and authentication, all at the same time, for the traffic exchanged over the bus protocols that AUTOSAR supports. CINNAMON in fact stands for Confidential, INtegral aNd Authentic onboard coMmunicatiON.
This article introduces the requirements and specification of CINNAMON in a differential fashion with respect to the existing Secure Onboard Communication Basic Software Module, which does not include confidentiality. As a result, CINNAMON exceeds SecOC at least against information gathering attacks. The article then defines three security profiles, regulating also the freshness attribute appropriately. Most importantly, CINNAMON is not a simple academic exercise because it is implemented in a laboratory environment on commercial ECUs, thus reaching the level of TRL 4, “Component and/or breadboard validation in laboratory environment”. The runtimes obtained on inexpensive devices are reassuring, paving the way for a possible large-scale application.",Networking,design implement autosarbas basic softwar modul enhanc secur,electron control unit ecu commun accomplish function modern vehicl ecu form invehicl network precis regul must adequ protect malici activ sever outbreak recent year therefor present cinnamon autosarbas basic softwar modul aim confidenti integr authent time traffic exchang bu protocol autosar support cinnamon fact stand confidenti integr authent onboard commun articl introduc requir specif cinnamon differenti fashion respect exist secur onboard commun basic softwar modul includ confidenti result cinnamon exce secoc least inform gather attack articl defin three secur profil regul also fresh attribut appropri importantli cinnamon simpl academ exercis implement laboratori environ commerci ecu thu reach level trl 4 compon andor breadboard valid laboratori environ runtim obtain inexpens devic reassur pave way possibl largescal applic
Game theoretic solution for an Unmanned Aerial Vehicle network host under DDoS attack,"Game theory is being used in cybersecurity to observe different attacks as it can provide a mathematical representation of the interactions between system admins, hackers, and users. The game-theoretic solution determines the favorable parameters (strategies), predicts the player’s behavior, and suggests the best settings for minimizing the attack’s effect. To this end, our paper attempts to study the usefulness of game-theoretic applications for the prevention of Distributed Denial of Service (DDoS) attacks on a drone by deriving the information from conventional game solutions and augmenting that with the bounded rationality concept called Quantal response equilibrium (QRE). In this process, we identify feasible strategies for each player through simulations and formulate five non-cooperative game scenarios for two variants of DDoS attacks. In these games, the traditional game-theoretic solution or Nash Equilibrium (NashE) provides information about the drone’s recommended settings, the hacker’s preferred strategy, and the game-theoretic threshold assuming that all participants are highly intelligent. We augment this information by considering the participant’s tendency to make errors and the evolution in their behavioral pattern from zero to high-values of rationality using QRE. The information coupled from NashE and QRE provides better clarity to a drone operator, thus improving the drone’s security by two levels and allowing the drone operator to take timely precautions. Inspired by this multilevel process, we propose an equivalent real-world framework for protecting Unmanned Aerial Vehicle (UAV) nodes against a DDoS attack.",Networking,game theoret solut unman aerial vehicl network host ddo attack,game theori use cybersecur observ differ attack provid mathemat represent interact system admin hacker user gametheoret solut determin favor paramet strategi predict player behavior suggest best set minim attack effect end paper attempt studi use gametheoret applic prevent distribut denial servic ddo attack drone deriv inform convent game solut augment bound ration concept call quantal respons equilibrium qre process identifi feasibl strategi player simul formul five noncoop game scenario two variant ddo attack game tradit gametheoret solut nash equilibrium nash provid inform drone recommend set hacker prefer strategi gametheoret threshold assum particip highli intellig augment inform consid particip tendenc make error evolut behavior pattern zero highvalu ration use qre inform coupl nash qre provid better clariti drone oper thu improv drone secur two level allow drone oper take time precaut inspir multilevel process propos equival realworld framework protect unman aerial vehicl uav node ddo attack
Evaluating Federated Learning for intrusion detection in Internet of Things: Review and challenges,"The application of Machine Learning (ML) techniques to the well-known intrusion detection systems (IDS) is key to cope with increasingly sophisticated cybersecurity attacks through an effective and efficient detection process. In the context of the Internet of Things (IoT), most ML-enabled IDS approaches use centralized approaches where IoT devices share their data with data centers for further analysis. To mitigate privacy concerns associated with centralized approaches, in recent years the use of Federated Learning (FL) has attracted a significant interest in different sectors, including healthcare and transport systems. However, the development of FL-enabled IDS for IoT is in its infancy, and still requires research efforts from various areas, in order to identify the main challenges for the deployment in real-world scenarios. In this direction, our work evaluates a FL-enabled IDS approach based on a multiclass classifier considering different data distributions for the detection of different attacks in an IoT scenario. In particular, we use three different settings that are obtained by partitioning the recent ToN_IoT dataset according to IoT devices’ IP address and types of attack. Furthermore, we evaluate the impact of different aggregation functions according to such setting by using the recent IBMFL framework as FL implementation. Additionally, we identify a set of challenges and future directions based on the existing literature and the analysis of our evaluation results.",Networking,evalu feder learn intrus detect internet thing review challeng,applic machin learn ml techniqu wellknown intrus detect system id key cope increasingli sophist cybersecur attack effect effici detect process context internet thing iot mlenabl id approach use central approach iot devic share data data center analysi mitig privaci concern associ central approach recent year use feder learn fl attract signific interest differ sector includ healthcar transport system howev develop flenabl id iot infanc still requir research effort variou area order identifi main challeng deploy realworld scenario direct work evalu flenabl id approach base multiclass classifi consid differ data distribut detect differ attack iot scenario particular use three differ set obtain partit recent ton_iot dataset accord iot devic ip address type attack furthermor evalu impact differ aggreg function accord set use recent ibmfl framework fl implement addit identifi set challeng futur direct base exist literatur analysi evalu result
FedSBS: Federated-Learning participant-selection method for Intrusion Detection Systems,"Federated Learning (FL) is a decentralized machine learning approach in which multiple participants collaboratively train a model. Participants keep data locally, train their local models, and aggregate them in a single global model in a federated server. Collaborative FL-based Intrusion Detection Systems face challenges on an uneven statistical distribution of data and malicious participants trying to subvert the learning process. The statistical hurdles associated with imbalanced data and malicious participants pose a risk of skewing the training with biased or random data. The inability to effectively manage these statistical inconsistencies may degrade system performance, leading to false intrusion detection or opening avenues for cybersecurity breaches. To overcome these challenges, we propose a training method that employs score-based participant selection and utilizes global momentum for model aggregation. Our method improves the global model performance while mitigating the risks posed by malicious participants. The proposal incorporates a scoring system based on an information gain variant to evaluate each participant’s contribution. The scoring system and an epsilon greedy selection method ensure robust participant selection in each aggregation round. Furthermore, incorporating a global momentum term helps preserve previous knowledge at each aggregation round, contributing to model stability and overall learning. The proposed solution has demonstrated superior performance, delivering 80% F1-Score and 90% accuracy on experiments even in the presence of malicious participants, revealing the robustness and effectiveness of the proposal in mitigating statistical challenges. Consequently, the proposed method significantly enhances the performance of federated learning models, leading to more secure and efficient collaborative intrusion detection systems.",Networking,fedsb federatedlearn participantselect method intrus detect system,feder learn fl decentr machin learn approach multipl particip collabor train model particip keep data local train local model aggreg singl global model feder server collabor flbase intrus detect system face challeng uneven statist distribut data malici particip tri subvert learn process statist hurdl associ imbalanc data malici particip pose risk skew train bias random data inabl effect manag statist inconsist may degrad system perform lead fals intrus detect open avenu cybersecur breach overcom challeng propos train method employ scorebas particip select util global momentum model aggreg method improv global model perform mitig risk pose malici particip propos incorpor score system base inform gain variant evalu particip contribut score system epsilon greedi select method ensur robust particip select aggreg round furthermor incorpor global momentum term help preserv previou knowledg aggreg round contribut model stabil overal learn propos solut demonstr superior perform deliv 80 f1score 90 accuraci experi even presenc malici particip reveal robust effect propos mitig statist challeng consequ propos method significantli enhanc perform feder learn model lead secur effici collabor intrus detect system
Development of Image Processing Based on Deep Learning Algorithm,"With the rapid development of computer technology and information technology, the development of deep learning has been greatly promoted, and as the mainstream trend of the development of deep learning, there is a great technological breakthrough in the field of image processing. This paper mainly focuses on the development of image processing technology supported by deep learning algorithm, using particle swarm algorithms, image matching algorithms and deletion strategies to optimize image processing technology, and it is found that each of these methods plays a role in pattern recognition, obtaining deeper meaning of images and deleting unimportant information. Deep learning algorithm enable the processing of a large amount of stored information and ensure the integrity of the image in the process of optimizing image processing.",Image Processing,develop imag process base deep learn algorithm,rapid develop comput technolog inform technolog develop deep learn greatli promot mainstream trend develop deep learn great technolog breakthrough field imag process paper mainli focus develop imag process technolog support deep learn algorithm use particl swarm algorithm imag match algorithm delet strategi optim imag process technolog found method play role pattern recognit obtain deeper mean imag delet unimport inform deep learn algorithm enabl process larg amount store inform ensur integr imag process optim imag process
On the Present Situation and Future of Digital Image Intelligent Processing System,"Computer technology is widely used in various industries. Since the digital image processing technology is realized based on computer technology, its future development space is very large. For some users who are new to digital image processing technology, the theoretical knowledge of this technology will be more difficult to understand. For the purpose of better solving this problem, the system is based on the Windows processing system in the design period so that it can further process images and import or export them. Thus, this system can be easier to understand and operate for people.",Image Processing,present situat futur digit imag intellig process system,comput technolog wide use variou industri sinc digit imag process technolog realiz base comput technolog futur develop space larg user new digit imag process technolog theoret knowledg technolog difficult understand purpos better solv problem system base window process system design period process imag import export thu system easier understand oper peopl
Recent Trends of Granular Computing Approaches for Image Processing in Medical Imaging,"The objective of this study is to provide a comprehensive analysis of the current developments in the application of granular computing techniques in the field of biomedical image processing. Granular computing is a methodological approach that utilises granules as a means to effectively tackle a diverse array of diseases. The granules mentioned below represent the elemental constituents of granular computing. This paper offers an academic exploration of the application of granular computing in addressing image processing problems. It encompasses a comprehensive examination of the parameters associated with granular computing, the significance of granular computing in the field of image processing, a diverse range of image processing techniques, and the contributions made by granular computing in overcoming challenges encountered in medical image processing. Furthermore, this study examines the accomplishments of granular computing, performs a comprehensive review of the existing literature on granular computing and image processing methodologies, emphasises the benefits of employing granular computing in contrast to alternative image processing approaches, and ultimately investigates the various health concerns and obstacles associated with the utilisation of granular computing in the context of image processing.",Image Processing,recent trend granular comput approach imag process medic imag,object studi provid comprehens analysi current develop applic granular comput techniqu field biomed imag process granular comput methodolog approach utilis granul mean effect tackl divers array diseas granul mention repres element constitu granular comput paper offer academ explor applic granular comput address imag process problem encompass comprehens examin paramet associ granular comput signific granular comput field imag process divers rang imag process techniqu contribut made granular comput overcom challeng encount medic imag process furthermor studi examin accomplish granular comput perform comprehens review exist literatur granular comput imag process methodolog emphasis benefit employ granular comput contrast altern imag process approach ultim investig variou health concern obstacl associ utilis granular comput context imag process
Performance of Some Image Processing Algorithms in Tensorflow,"Signal, image and Synthetic Aperture Radar imagery algorithms in recent time are used in a daily routine. Due to huge data and complexity, their processing is almost impossible in a real time. Often image processing algorithms are inherently parallel in nature, so they fit nicely into parallel architectures multicore Central Processing Unit (CPU) and Graphics Processing Unit GPUs. In this paper image processing algorithms were evaluated, which are capable to execute in parallel manner on several platforms CPU and GPU. All algorithms were tested in TensorFlow, which is a novel framework for deep learning, but also for image processing. Relative speedups compared to CPU were given for all algorithms. TensorFlow GPU implementation can outperform multi-core CPUs for tested algorithms, obtained speedups range from 3.6 to 15 times.",Image Processing,perform imag process algorithm tensorflow,signal imag synthet apertur radar imageri algorithm recent time use daili routin due huge data complex process almost imposs real time often imag process algorithm inher parallel natur fit nice parallel architectur multicor central process unit cpu graphic process unit gpu paper imag process algorithm evalu capabl execut parallel manner sever platform cpu gpu algorithm test tensorflow novel framework deep learn also imag process rel speedup compar cpu given algorithm tensorflow gpu implement outperform multicor cpu test algorithm obtain speedup rang 36 15 time
Face Recognition Implementation Based on Image Processing Techniques,"Image processing is a technique that involves the manipulation and enhancement of digital images. It encompasses various aspects such as image acquisition, image preprocessing, image enhancement, image segmentation, feature extraction, and object detection. The purpose of image processing is to process and enhance images using computer technology, aiming to achieve higher image quality for subsequent processing and applications. In practical applications, image processing techniques employ different algorithms and methods to achieve various processing and enhancement effects. Additionally, image processing techniques can be applied in fields such as medicine, security, autonomous driving, VR/AR, etc. For instance, in the medical field, image processing techniques can be utilized for image segmentation and diagnosis of medical images, thereby improving the accuracy and analytical capabilities of medical imaging. In the security field, image processing techniques can be employed for tasks such as video surveillance and facial recognition, enhancing the efficiency and accuracy of security monitoring. In the domain of autonomous driving, image processing techniques can be used for detecting and recognizing the surrounding environment of vehicles, thereby enhancing the safety and performance of autonomous driving systems. In conclusion, image processing technology is a very important technology. By continuously improving the accuracy and reliability of the algorithm, it is expected to be applied in a wider range of applications.",Image Processing,face recognit implement base imag process techniqu,imag process techniqu involv manipul enhanc digit imag encompass variou aspect imag acquisit imag preprocess imag enhanc imag segment featur extract object detect purpos imag process process enhanc imag use comput technolog aim achiev higher imag qualiti subsequ process applic practic applic imag process techniqu employ differ algorithm method achiev variou process enhanc effect addit imag process techniqu appli field medicin secur autonom drive vrar etc instanc medic field imag process techniqu util imag segment diagnosi medic imag therebi improv accuraci analyt capabl medic imag secur field imag process techniqu employ task video surveil facial recognit enhanc effici accuraci secur monitor domain autonom drive imag process techniqu use detect recogn surround environ vehicl therebi enhanc safeti perform autonom drive system conclus imag process technolog import technolog continu improv accuraci reliabl algorithm expect appli wider rang applic
An Automatic Nuclei Cells Counting Approach Using Effective Image Processing Methods,"Manual counting of nuclei cells from histological images is considered tedious process, time-consuming and subjected to human errors. Therefore, automated the process of nuclei cells counting is become important and necessary for effective analyzing of histological images. Current systems and approaches of nuclei cells counting are based on color or grayscale images leading to inaccurate results and have several limitations. In this paper, we propose a novel accurate approach for automatic nuclei cells counting using effective image processing methods. The new techniques are designed based on image thresholding method, morphological image processing operations, and connected component algorithm. The new approach was evaluated experimentally on 37 images of a public data set of 100 histological images. The experimental results demonstrated that the approach achieved a high accuracy up to 89.5% compared with previous works. We concluded the effectiveness of the proposed approach for automatic counting of nuclei cells from histological images.",Image Processing,automat nuclei cell count approach use effect imag process method,manual count nuclei cell histolog imag consid tediou process timeconsum subject human error therefor autom process nuclei cell count becom import necessari effect analyz histolog imag current system approach nuclei cell count base color grayscal imag lead inaccur result sever limit paper propos novel accur approach automat nuclei cell count use effect imag process method new techniqu design base imag threshold method morpholog imag process oper connect compon algorithm new approach evalu experiment 37 imag public data set 100 histolog imag experiment result demonstr approach achiev high accuraci 895 compar previou work conclud effect propos approach automat count nuclei cell histolog imag
A Comprehensive Study: Image Forensic Analysis Traditional to Cognitive Image Processing,Image and video forensics is the biggest challenge in current digital era due to rapid change or modification in digital content by using lots of available free software and editing tools. Authenticity of image and video is explained from image acquisition phase to storage phase through various forensics methods. The aim of this survey is to investigate image and video forensics methods using the intrinsic footprints of digital image during the entire image processing life cycle in the traditional image processing paradigm to cognitive image processing paradigm.,Image Processing,comprehens studi imag forens analysi tradit cognit imag process,imag video forens biggest challeng current digit era due rapid chang modif digit content use lot avail free softwar edit tool authent imag video explain imag acquisit phase storag phase variou forens method aim survey investig imag video forens method use intrins footprint digit imag entir imag process life cycl tradit imag process paradigm cognit imag process paradigm
Analysis of X-Ray Images with Image Processing Techniques: A Review,"Human body suffers from various problems it consists of different parts such as legs, hands, bones, bones get cracked or discontinuity most of the times due to pressure applied on it which may be due to the accident, sports while playing etc. Osteoporosis is one of the major problems occurs due to extra use of bones radiologist suggests the patients take x-ray images of the bones for diagnosis purpose. This study is a tutorial review on medical imaging processing and repository techniques appeared in the literature. Many times, it is difficult and time-consuming to find out the location of fracture in the patient who is suffering from pain. Today medical imaging technique played the significant role in research and diagnosis field. X-ray imaging technique is used to diagnose and also used to represent anatomical structures such as bones, in human beings. This paper is a tutorial review of X-ray imaging technique which is used to detect bone fractures and then the obtained image is processed by different image processing methods such as Computer Aided Diagnosis, Edge Detection, segmentation which are beneficial for technicians.",Image Processing,analysi xray imag imag process techniqu review,human bodi suffer variou problem consist differ part leg hand bone bone get crack discontinu time due pressur appli may due accid sport play etc osteoporosi one major problem occur due extra use bone radiologist suggest patient take xray imag bone diagnosi purpos studi tutori review medic imag process repositori techniqu appear literatur mani time difficult timeconsum find locat fractur patient suffer pain today medic imag techniqu play signific role research diagnosi field xray imag techniqu use diagnos also use repres anatom structur bone human be paper tutori review xray imag techniqu use detect bone fractur obtain imag process differ imag process method comput aid diagnosi edg detect segment benefici technician
OpenCLIPER: An OpenCL-Based C++ Framework for Overhead-Reduced Medical Image Processing and Reconstruction on Heterogeneous Devices,"Medical image processing is often limited by the computational cost of the involved algorithms. Whereas dedicated computing devices (GPUs in particular) exist and do provide significant efficiency boosts, they have an extra cost of use in terms of housekeeping tasks (device selection and initialization, data streaming, synchronization with the CPU, and others), which may hinder developers from using them. This paper describes an OpenCL-based framework that is capable of handling dedicated computing devices seamlessly and that allows the developer to concentrate on image processing tasks. The framework handles automatically device discovery and initialization, data transfers to and from the device and the file system and kernel loading and compiling. Data structures need to be defined only once independently of the computing device; code is unique, consequently, for every device, including the host CPU. Pinned memory/buffer mapping is used to achieve maximum performance in data transfers. Code fragments included in the paper show how the computing device is almost immediately and effortlessly available to the users algorithms, so they can focus on productive work. Code required for device selection and initialization, data loading and streaming and kernel compilation is minimal and systematic. Algorithms can be thought of as mathematical operators (called processes), with input, output and parameters, and they may be chained one after another easily and efficiently. Also for efficiency, processes can have their initialization work split from their core workload, so process chains and loops do not incur in performance penalties. Algorithm code is independent of the device type targeted.",Image Processing,openclip openclbas c framework overheadreduc medic imag process reconstruct heterogen devic,medic imag process often limit comput cost involv algorithm wherea dedic comput devic gpu particular exist provid signific effici boost extra cost use term housekeep task devic select initi data stream synchron cpu other may hinder develop use paper describ openclbas framework capabl handl dedic comput devic seamlessli allow develop concentr imag process task framework handl automat devic discoveri initi data transfer devic file system kernel load compil data structur need defin independ comput devic code uniqu consequ everi devic includ host cpu pin memorybuff map use achiev maximum perform data transfer code fragment includ paper show comput devic almost immedi effortlessli avail user algorithm focu product work code requir devic select initi data load stream kernel compil minim systemat algorithm thought mathemat oper call process input output paramet may chain one anoth easili effici also effici process initi work split core workload process chain loop incur perform penalti algorithm code independ devic type target
Image Segmentation Technology and Its Application in Digital Image Processing,"In recent years, the level of science and technology in China has been significantly improved. Digital image processing technology is the product of the times with the rapid development of science and technology, and has been applied in many fields. Digital image processing technology includes many practical technologies, and image segmentation technology is one of them. At present, the widely used pattern recognition technology is realized by image segmentation technology. Based on this, this paper mainly studies the application of image segmentation technology in digital image processing. This paper first introduces the methods of image segmentation, including threshold segmentation, clustering segmentation and edge detection segmentation. Secondly, according to a need to extract a road image for image segmentation, and achieved very good results. Finally, this paper mainly analyzes the application of image segmentation in digital image processing. It is found that image segmentation has good application in automatic license plate recognition, biomedical engineering, remote sensing engineering and fire prevention and detection.",Image Processing,imag segment technolog applic digit imag process,recent year level scienc technolog china significantli improv digit imag process technolog product time rapid develop scienc technolog appli mani field digit imag process technolog includ mani practic technolog imag segment technolog one present wide use pattern recognit technolog realiz imag segment technolog base paper mainli studi applic imag segment technolog digit imag process paper first introduc method imag segment includ threshold segment cluster segment edg detect segment secondli accord need extract road imag imag segment achiev good result final paper mainli analyz applic imag segment digit imag process found imag segment good applic automat licens plate recognit biomed engin remot sens engin fire prevent detect
Design of Image Processing System Based on DSP Core,"In recent years, with the development of digital technology, digital image processing has been widely and deeply applied in the field of computer graphics. Digital image processing system is a complex real-time system, it from the camera, fax machine and other scanning equipment to obtain image information, after digital transformation, digital image information coding, filtering, enhancement, recovery, compression, storage and other processing, finally generate visual image. This design uses TMS320C6748 as the core processor of the system, SAA7113 as the video decoding chip of the system, CPLD as the sampling controller, DDR2 chip as the external expansion memory. The ROM expansion uses NAND flash memory chip. On the basis of hardware design, combined with software algorithm to complete image processing. The system can be used in information communication, image recognition, news scene and other fields of image processing and transmission. This paper analyzes the hardware structure and data processing algorithm of the system in detail. The experimental results show that the system can not only obtain higher compression ratio, but also reduce the distortion of the reconstructed image. The image processing system has certain practicability.",Image Processing,design imag process system base dsp core,recent year develop digit technolog digit imag process wide deepli appli field comput graphic digit imag process system complex realtim system camera fax machin scan equip obtain imag inform digit transform digit imag inform code filter enhanc recoveri compress storag process final gener visual imag design use tms320c6748 core processor system saa7113 video decod chip system cpld sampl control ddr2 chip extern expans memori rom expans use nand flash memori chip basi hardwar design combin softwar algorithm complet imag process system use inform commun imag recognit news scene field imag process transmiss paper analyz hardwar structur data process algorithm system detail experiment result show system obtain higher compress ratio also reduc distort reconstruct imag imag process system certain practic
Research on Applications of Image Processing Technologies in Interior Design Information System,"Due to the limitations of software technology and weak image processing technology, traditional interior design has limited scene reproduction ability and supports a small range of scenes. Therefore, this article proposes an interior design system based on image processing and virtual reality technology. The system hardware consists of an image processing module, a virtual simulation module, and a database support module, mainly responsible for processing image information. The system software mainly consists of program loading module, data storage and reading/writing module, bus transmission module, etc. After testing, the system's denoised image is smooth and clear, with obvious edge segmentation. After image rendering, the reflection and light effects are good, and the image processing time is short, which has a good indoor design effect.",Image Processing,research applic imag process technolog interior design inform system,due limit softwar technolog weak imag process technolog tradit interior design limit scene reproduct abil support small rang scene therefor articl propos interior design system base imag process virtual realiti technolog system hardwar consist imag process modul virtual simul modul databas support modul mainli respons process imag inform system softwar mainli consist program load modul data storag readingwrit modul bu transmiss modul etc test system denois imag smooth clear obviou edg segment imag render reflect light effect good imag process time short good indoor design effect
Analysis of Computer Image Processing Technology based on Intelligent Optimization Algorithm,"With the continuous development of computer technology, computer image processing technology has become an indispensable part of modern society. Computer image processing technology mainly includes image acquisition, image enhancement, image compression, image recognition and so on. In this process, intelligent optimization algorithm plays a very important role. This paper will introduce the application of intelligent optimization algorithm in computer image processing technology in detail.",Image Processing,analysi comput imag process technolog base intellig optim algorithm,continu develop comput technolog comput imag process technolog becom indispens part modern societi comput imag process technolog mainli includ imag acquisit imag enhanc imag compress imag recognit process intellig optim algorithm play import role paper introduc applic intellig optim algorithm comput imag process technolog detail
OpenCV Implementation of Image Processing Optimization Architecture of Deep Learning Algorithm based on Big Data Processing Technology,"Through the in-depth analysis of artificial neural network algorithm technology, particle swarm algorithm technology, and image matching algorithm, the article briefly analyzes the theoretical principle of the algorithm, analyzes the characteristics of the algorithm, and analyzes the application of the algorithm in image processing optimization technology. Using the iterative process of neurons to process image data, after dimensionality reduction, reduction, and white point removal, the optimization of image processing is increased by 7.5%. Using deep learning algorithms to optimize the processing of the drawbacks and difficulties of big data, build an image optimization architecture framework and OpenCV modeling, the results showthat image noise is reduced by 12%.",Image Processing,opencv implement imag process optim architectur deep learn algorithm base big data process technolog,indepth analysi artifici neural network algorithm technolog particl swarm algorithm technolog imag match algorithm articl briefli analyz theoret principl algorithm analyz characterist algorithm analyz applic algorithm imag process optim technolog use iter process neuron process imag data dimension reduct reduct white point remov optim imag process increas 75 use deep learn algorithm optim process drawback difficulti big data build imag optim architectur framework opencv model result showthat imag nois reduc 12
Design of an Access Control System for Unmanned Bathroom Based on Image Processing Technology,"In order to improve the efficiency and intelligence of the access control management system, this paper uses image processing technology to identify the ID card number, and uses the gender results after identification as the basis for identifying the switch of the male and female bathroom access control system, and compares the membership information in the database with the chip information in the second-generation ID card and the image recognition information to verify the authenticity of the ID card. Therefore, an intelligent bathroom access control system based on image processing technology is developed. The system is mainly composed of image acquisition and reading module, image processing module, magnetic card induction module, signal processing module and signal execution module. The image processing module is based on image processing technology, and uses the powerful mathematical processing ability of MATLAB to complete a series of image processing processes such as eigenvalue data extraction. Identify whether it is a registered member by comparing with the data in the database. The signal processing module is based on server processing technology to realize unmanned automatic door opening.",Image Processing,design access control system unman bathroom base imag process technolog,order improv effici intellig access control manag system paper use imag process technolog identifi id card number use gender result identif basi identifi switch male femal bathroom access control system compar membership inform databas chip inform secondgener id card imag recognit inform verifi authent id card therefor intellig bathroom access control system base imag process technolog develop system mainli compos imag acquisit read modul imag process modul magnet card induct modul signal process modul signal execut modul imag process modul base imag process technolog use power mathemat process abil matlab complet seri imag process process eigenvalu data extract identifi whether regist member compar data databas signal process modul base server process technolog realiz unman automat door open
Application of image processing technology in gas pipeline inner wall damage detection,"In order to detect the damage of gas pipeline, this paper uses image processing technology as a research tool, and proposes an image processing system for pipeline damage detection. Because the environment of the gas pipeline may be dark, humid, even sand and other complex environmental factors, as well as the noise generated by the electronic devices themselves, resulting in the quality of the image transmitted from the front end is greatly reduced, so this paper first uses image processing technology to denoise the image. In order to distinguish the damaged and intact inner wall of the pipeline and facilitate the measurement of the target area, the image segmentation processing is also needed for the image. The purpose is to provide high-quality and easy to process images for the follow-up work. Finally, the image features of gas pipeline inner wall damage can be extracted to complete the image processing.",Image Processing,applic imag process technolog ga pipelin inner wall damag detect,order detect damag ga pipelin paper use imag process technolog research tool propos imag process system pipelin damag detect environ ga pipelin may dark humid even sand complex environment factor well nois gener electron devic result qualiti imag transmit front end greatli reduc paper first use imag process technolog denois imag order distinguish damag intact inner wall pipelin facilit measur target area imag segment process also need imag purpos provid highqual easi process imag followup work final imag featur ga pipelin inner wall damag extract complet imag process
An Image Depth Processing Method Based On Parallel Computing and Multi-GPU,"An image depth processing method based on CPU+GPU hybrid heterogeneous programming and Multi-GPU parallel computing is proposed in this paper. With the gradual increase of the image data, image processing algorithms have higher and higher requirements for GPU and CPU. Firstly, this paper introduces the heterogeneous programming system of the combination of CPU and GPU clusters and the technical points of CUDA. Secondly, the technical points based on Multi-GPU parallel computing image depth processing algorithm is introduced. Finally, the effectiveness of this algorithm is verified through experimental simulation.",Image Processing,imag depth process method base parallel comput multigpu,imag depth process method base cpugpu hybrid heterogen program multigpu parallel comput propos paper gradual increas imag data imag process algorithm higher higher requir gpu cpu firstli paper introduc heterogen program system combin cpu gpu cluster technic point cuda secondli technic point base multigpu parallel comput imag depth process algorithm introduc final effect algorithm verifi experiment simul
Quantum Techniques for Image Processing,"Quantum computing is revolutionizing every field of knowledge. With advent of affordable quantum computing almost possible the importance and relevance of quantum computing for all the recent research trends cannot be overemphasized. We have already witnessed quantum analogues of most celebrated classical algorithms starting from Shor’s algorithm for integer factorization, Grover’s search algorithms for unstructured database. Quantum Image Processing is an emerging field borne by applying quantum computing techniques to the Image Processing problems. This paper attempts to summarize and review the state of development in the field of Quantum Image Processing. All major quantum computing platforms have been presented, At the end a discussion on the current trends and future possibilities is presented.",Image Processing,quantum techniqu imag process,quantum comput revolution everi field knowledg advent afford quantum comput almost possibl import relev quantum comput recent research trend overemphas alreadi wit quantum analogu celebr classic algorithm start shor algorithm integ factor grover search algorithm unstructur databas quantum imag process emerg field born appli quantum comput techniqu imag process problem paper attempt summar review state develop field quantum imag process major quantum comput platform present end discuss current trend futur possibl present
"Determining Image Deblur, Detecting Faces, and Enhancing Appearance Using Image Processing Techniques","This paper explores the utilization of MATLAB for digital signal processing (DSP) techniques in image processing tasks, focusing on image deblurring, face detection, and facial feature enhancement. Blind deconvolution methods are employed to address image blurriness, while face detection is facilitated using cascaded object detectors. Enhancements to detected facial features involve histogram equalization, smoothing filters, skin tone adjustment, and contrast enhancement techniques, followed by seamless integration using resizing methods. MATLAB serves as a robust platform for implementing and analyzing DSP algorithms, providing insights into practical solutions for common challenges in digital image processing.",Image Processing,determin imag deblur detect face enhanc appear use imag process techniqu,paper explor util matlab digit signal process dsp techniqu imag process task focus imag deblur face detect facial featur enhanc blind deconvolut method employ address imag blurri face detect facilit use cascad object detector enhanc detect facial featur involv histogram equal smooth filter skin tone adjust contrast enhanc techniqu follow seamless integr use resiz method matlab serv robust platform implement analyz dsp algorithm provid insight practic solut common challeng digit imag process
Evaluation of NVIDIA Xavier NX Platform for Real-Time Image Processing for Fusion Diagnostics,"Real-time image processing is the core component of image plasma diagnostics. Efficient algorithms enable machine protection, contributing to future steady-state operation in nuclear fusion devices. The paper evaluates the applicability of the newest low-power NVIDIA Jetson Xavier NX platform for fusion diagnostics. This embedded NVIDIA Tegra System-on-a-Chip (SoC) integrates a Graphics Processing Unit (GPU) and Central Processing Unit (CPU) on a single chip. General-Purpose computing on Graphics Processing Units (GPGPU) provides high parallelism that is advantageous in image-based calculations. The hardware differences in comparison to the previous NVIDIA Jetson TX2 based on Pascal architecture, including innovations introduced in the Volta architecture for NVIDIA Tegra, are signified. The evaluation is performed on the Wendelstein 7-X (W7-X) stellarator experimental data. Implemented algorithms detect and analyse thermal events in real-time utilising the embedded GPU. Investigated thermal events are strike-lines, overload hotspots, reflections and surface layers. Their detection allows the automated real-time risk evaluation incorporated in the feedback plasma control and interlock systems in the W7-X. The speedup resulting from the upgrade to the Xavier NX platform is presented in the paper, along with techniques pertaining to key hardware differences and programming aspects specific to the NVIDIA Tegra facilitating real-time computing on the low-power embedded device.",Image Processing,evalu nvidia xavier nx platform realtim imag process fusion diagnost,realtim imag process core compon imag plasma diagnost effici algorithm enabl machin protect contribut futur steadyst oper nuclear fusion devic paper evalu applic newest lowpow nvidia jetson xavier nx platform fusion diagnost embed nvidia tegra systemonachip soc integr graphic process unit gpu central process unit cpu singl chip generalpurpos comput graphic process unit gpgpu provid high parallel advantag imagebas calcul hardwar differ comparison previou nvidia jetson tx2 base pascal architectur includ innov introduc volta architectur nvidia tegra signifi evalu perform wendelstein 7x w7x stellar experiment data implement algorithm detect analys thermal event realtim utilis embed gpu investig thermal event strikelin overload hotspot reflect surfac layer detect allow autom realtim risk evalu incorpor feedback plasma control interlock system w7x speedup result upgrad xavier nx platform present paper along techniqu pertain key hardwar differ program aspect specif nvidia tegra facilit realtim comput lowpow embed devic
Fast controling autonomous vehicle based on real time image processing,"In this paper, a method for Autonomous Vehicle is presented based on real time image processing. The system detects the road by sobel edge detection, and it recognizes the obstacles, humans, and traffic lights by heuristic techniques. For this aim, some features are defined based on key points. One important technique is that every frame is divided in some sections which significantly affect time of processing. The system is able to analyze 30 frames per second to get the best decision for controlling the vehicle. The achieved structure is optimized on accuracy and the number of logic cells. The algorithms completely describes in hardware by VHDL, then implemented on DE1-SOC board which uses cyclone V FPGA.",Image Processing,fast control autonom vehicl base real time imag process,paper method autonom vehicl present base real time imag process system detect road sobel edg detect recogn obstacl human traffic light heurist techniqu aim featur defin base key point one import techniqu everi frame divid section significantli affect time process system abl analyz 30 frame per second get best decis control vehicl achiev structur optim accuraci number logic cell algorithm complet describ hardwar vhdl implement de1soc board use cyclon v fpga
Hotspot Detection of Solar Photovoltaic System: A Perspective from Image Processing,"Research in solar energy has rapidly grown since its significant and contributes to the advancement in clean renewable energy technology. Effective energy management such as fault detection impacts the early-stage monitoring for the efficiency, reliability, and safety of solar photovoltaic (PV) systems. The formation of a hotspot is one of the issues commonly occurred in a PV system. However, the main limitation of hotspot detection is the difficulty to interpret specific components with erratic temperatures in the thermographic images for attributes in the intelligence detection model. In this study, a review of hotspot detection in solar PV panels using the image processing method is established based on the image processing field. The integration of image processing approach can further assist in developing automated fault detection in solar PV farms for effective preventive monitoring methods. Therefore, several aspects need to be categorized and considered accordingly for achieving accurate prediction. Several ways were discussed, and future research is suggested in this study.",Image Processing,hotspot detect solar photovolta system perspect imag process,research solar energi rapidli grown sinc signific contribut advanc clean renew energi technolog effect energi manag fault detect impact earlystag monitor effici reliabl safeti solar photovolta pv system format hotspot one issu commonli occur pv system howev main limit hotspot detect difficulti interpret specif compon errat temperatur thermograph imag attribut intellig detect model studi review hotspot detect solar pv panel use imag process method establish base imag process field integr imag process approach assist develop autom fault detect solar pv farm effect prevent monitor method therefor sever aspect need categor consid accordingli achiev accur predict sever way discuss futur research suggest studi
Weakly Supervised Estimation of Shadow Confidence Maps in Fetal Ultrasound Imaging,"Detecting acoustic shadows in ultrasound images is important in many clinical and engineering applications. Real-time feedback of acoustic shadows can guide sonographers to a standardized diagnostic viewing plane with minimal artifacts and can provide additional information for other automatic image analysis algorithms. However, automatically detecting shadow regions using learning-based algorithms is challenging because pixel-wise ground truth annotation of acoustic shadows is subjective and time consuming. In this paper, we propose a weakly supervised method for automatic confidence estimation of acoustic shadow regions. Our method is able to generate a dense shadow-focused confidence map. In our method, a shadow-seg module is built to learn general shadow features for shadow segmentation, based on global image-level annotations as well as a small number of coarse pixel-wise shadow annotations. A transfer function is introduced to extend the obtained binary shadow segmentation to a reference confidence map. In addition, a confidence estimation network is proposed to learn the mapping between input images and the reference confidence maps. This network is able to predict shadow confidence maps directly from input images during inference. We use evaluation metrics such as DICE, inter-class correlation, and so on, to verify the effectiveness of our method. Our method is more consistent than human annotation and outperforms the state-of-the-art quantitatively in shadow segmentation and qualitatively in confidence estimation of shadow regions. Furthermore, we demonstrate the applicability of our method by integrating shadow confidence maps into tasks such as ultrasound image classification, multi-view image fusion, and automated biometric measurements.",Image Processing,weakli supervis estim shadow confid map fetal ultrasound imag,detect acoust shadow ultrasound imag import mani clinic engin applic realtim feedback acoust shadow guid sonograph standard diagnost view plane minim artifact provid addit inform automat imag analysi algorithm howev automat detect shadow region use learningbas algorithm challeng pixelwis ground truth annot acoust shadow subject time consum paper propos weakli supervis method automat confid estim acoust shadow region method abl gener dens shadowfocus confid map method shadowseg modul built learn gener shadow featur shadow segment base global imagelevel annot well small number coars pixelwis shadow annot transfer function introduc extend obtain binari shadow segment refer confid map addit confid estim network propos learn map input imag refer confid map network abl predict shadow confid map directli input imag infer use evalu metric dice interclass correl verifi effect method method consist human annot outperform stateoftheart quantit shadow segment qualit confid estim shadow region furthermor demonstr applic method integr shadow confid map task ultrasound imag classif multiview imag fusion autom biometr measur
SAR real-time imaging pipeline technology based on multi-core DSP,"According to the characteristics of SAR imaging, a SAR real-time imaging pipeline technology based on multicore DSP is proposed. The algorithm uses sub-aperture complex image stitching technology for real-time data processing, uses ECS operation to process the range direction of each subaperture data, and uses the Dechirp operation to process the azimuth direction. This technology is based on TI’s multi-core DSP C6678 chip to achieve a pipeline design, and coherently splices the processed sub-aperture image data in the complex image domain to obtain a full-resolution image of the recorded data. The algorithm is optimized for the KeystoneII hardware architecture of the chip to meet the performance requirements of real-time processing of SAR imaging. In this paper, the banding mode of SAR is verified, and the imaging results of the measured data test the effectiveness and real-time performance of the technology.",Image Processing,sar realtim imag pipelin technolog base multicor dsp,accord characterist sar imag sar realtim imag pipelin technolog base multicor dsp propos algorithm use subapertur complex imag stitch technolog realtim data process use ec oper process rang direct subapertur data use dechirp oper process azimuth direct technolog base ti multicor dsp c6678 chip achiev pipelin design coher splice process subapertur imag data complex imag domain obtain fullresolut imag record data algorithm optim keystoneii hardwar architectur chip meet perform requir realtim process sar imag paper band mode sar verifi imag result measur data test effect realtim perform technolog
Modelling and Simulation of Smart Traffic Light System for Emergency Vehicle using Image Processing Techniques,"traffic signal due to the drawbacks of the conventional traffic light system. In the existing traffic light system, a defined timer system is used and it is working based on preset timing. Due to the preset timing, there is no flexibility of ON/OFF in the signal light based on the emergency vehicle and congestion of the vehicle. Sometimes emergency vehicle like an ambulance needs to wait at a traffic signal for a long time and this would lead to a risk to a patient's life. Traffic police must personally identify an ambulance and release the congestion, but this is not possible as there are an enormous number of vehicles present these days. This project aims to providea solution for the issue in the conventional system. The model was designed using an image processing system that reads the image and determines the presence of an emergency vehicle and the density of vehicles in each lane the ON/OFF signal for the particular lane will be given to the traffic light system which helpsto reduce the unnecessary waiting time of vehicles. The system calculates the vehicle's density and to detect the emergency vehicle using image processing to provide the green light signal tothe lane. This project used Open CV and Yolo (you only look once)algorithm in the image processing method to develop the system. The simulation has been done on the proposed smart traffic systemand it identifies that the proposed system is efficient. Multiple times of programming and testing have been done on the proposedsystem to ensure accuracy and for validation.",Image Processing,model simul smart traffic light system emerg vehicl use imag process techniqu,traffic signal due drawback convent traffic light system exist traffic light system defin timer system use work base preset time due preset time flexibl onoff signal light base emerg vehicl congest vehicl sometim emerg vehicl like ambul need wait traffic signal long time would lead risk patient life traffic polic must person identifi ambul releas congest possibl enorm number vehicl present day project aim providea solut issu convent system model design use imag process system read imag determin presenc emerg vehicl densiti vehicl lane onoff signal particular lane given traffic light system helpsto reduc unnecessari wait time vehicl system calcul vehicl densiti detect emerg vehicl use imag process provid green light signal toth lane project use open cv yolo look oncealgorithm imag process method develop system simul done propos smart traffic systemand identifi propos system effici multipl time program test done proposedsystem ensur accuraci valid
Quantitative evaluation of morphological characteristics of road coarse aggregates based on image processing technology,"To quantitatively characterize and evaluate the morphological features of coarse aggregate particles, 2D images of coarse aggregate particles were processed by using Image Pro-Plus software in this paper. Through image enhancement, correction, segmentation, and retrieval, six morphological indicators such as axial coefficient, rectangularity, roundness, roughness, angular parameters, and fractal dimension were obtained. Further statistical analysis of the distribution pattern of each morphological characteristic parameter was carried out in a large sample to provide a basis for the quality control of coarse aggregates processing. What’s more, further statistical analysis of the distribution pattern of each morphological characteristic parameter was carried out in a large sample to provide a basis for the quality control of coarse aggregates processing. The axial coefficient of coarse aggregates of different particle sizes, decreases with increasing particle size. The roundness decreases with increasing particle size, and the fractal dimension decreases with increasing particle size; the axial coefficient and rectangularity can be used to characterize the shape of coarse aggregates without the influence of angles. Roughness and fractal dimension can be used to characterize coarse aggregate angles independent of shape; angular parameters, roughness or angular parameters, and fractal dimension can be chosen when characterizing angles only. This study can provide a fast and efficient quantitative basis for the optimization of the coarse aggregate processing and process control of incoming quality.",Image Processing,quantit evalu morpholog characterist road coars aggreg base imag process technolog,quantit character evalu morpholog featur coars aggreg particl 2d imag coars aggreg particl process use imag proplu softwar paper imag enhanc correct segment retriev six morpholog indic axial coeffici rectangular round rough angular paramet fractal dimens obtain statist analysi distribut pattern morpholog characterist paramet carri larg sampl provid basi qualiti control coars aggreg process what statist analysi distribut pattern morpholog characterist paramet carri larg sampl provid basi qualiti control coars aggreg process axial coeffici coars aggreg differ particl size decreas increas particl size round decreas increas particl size fractal dimens decreas increas particl size axial coeffici rectangular use character shape coars aggreg without influenc angl rough fractal dimens use character coars aggreg angl independ shape angular paramet rough angular paramet fractal dimens chosen character angl studi provid fast effici quantit basi optim coars aggreg process process control incom qualiti
A Redesign Method for Embroidery Pattern Graphics Based on Multiscale Image Processing Technology,"This study aims to explore the basic principles and strategies of embroidery pattern graphic redesign based on multi-scale image processing technology, as well as how to apply this technology to improve the accuracy and complexity of embroidery graphic design. By comprehensively utilizing image information at different scales, this article aims to achieve innovative redesign of traditional embroidery patterns, making them better suited to the needs of modern design. Meanwhile, by utilizing multi-scale image processing techniques in embroidery design, this article aims to promote the integration of traditional handicrafts and modern design. Through innovative redesign methods, this article will seek a balance between tradition and modernity to promote the inheritance and development of traditional handicrafts. This article will first review the relevant literature in the field of embroidery pattern design and summarize and analyze it. Then, it will provide a detailed introduction to the proposed redesign method based on multi-scale image processing technology. Finally, this article will explore the potential applications and future development directions of this method. This study seeks to present a fresh perspective on merging traditional handicrafts with modern design, fostering advancements in embroidery pattern graphic design. It aspires to offer valuable insights for both academic exploration and practical applications in relevant domains.",Image Processing,redesign method embroideri pattern graphic base multiscal imag process technolog,studi aim explor basic principl strategi embroideri pattern graphic redesign base multiscal imag process technolog well appli technolog improv accuraci complex embroideri graphic design comprehens util imag inform differ scale articl aim achiev innov redesign tradit embroideri pattern make better suit need modern design meanwhil util multiscal imag process techniqu embroideri design articl aim promot integr tradit handicraft modern design innov redesign method articl seek balanc tradit modern promot inherit develop tradit handicraft articl first review relev literatur field embroideri pattern design summar analyz provid detail introduct propos redesign method base multiscal imag process technolog final articl explor potenti applic futur develop direct method studi seek present fresh perspect merg tradit handicraft modern design foster advanc embroideri pattern graphic design aspir offer valuabl insight academ explor practic applic relev domain
Image Correction Technology Supporting Power Device Monitoring,"An automatically monitoring method of electrical device is proposed in this paper. Various visualization monitoring systems are built to save maintenance costs and improve operation efficiency. Limited to the working principle of ordinary camera lens, obtained image is easy to produce image deformity and aberration. In order to solve this problem, an image correction method based on phase-only correlation (POC) in power industry is suggested to get the image information of the actual situation of device. Experimental results show that proposed method can effectively save testing cost and achieve a high correction accuracy of device image.",Image Processing,imag correct technolog support power devic monitor,automat monitor method electr devic propos paper variou visual monitor system built save mainten cost improv oper effici limit work principl ordinari camera len obtain imag easi produc imag deform aberr order solv problem imag correct method base phaseonli correl poc power industri suggest get imag inform actual situat devic experiment result show propos method effect save test cost achiev high correct accuraci devic imag
Study of GAN-based image reconstruction for diffractive optical systems,"In previous works, the generative adversarial network (GAN) was successfully used to remove the image-wise color distortion of images captured by diffractive optics. It showed good image quality for a test sample. However, some reconstruction artifacts were produced by GAN for real scene images. In the paper, we study the nature of these artifacts. We show how overexposure and cross-like markers affect the occurrence of artifacts.",Image Processing,studi ganbas imag reconstruct diffract optic system,previou work gener adversari network gan success use remov imagewis color distort imag captur diffract optic show good imag qualiti test sampl howev reconstruct artifact produc gan real scene imag paper studi natur artifact show overexposur crosslik marker affect occurr artifact
Signal Processing for Circular-Track Ringmap SAR Equipped on Multi-rotors UAV,"Due to the limited radius of the arc, traditional ground-based Arc SAR often suffers from poor azimuth resolution. To provide high-resolution wide-swath imaging, a circular-track ringmap SAR equipped on multi-rotors UAV platform is proposed in this paper. In this paper, we put forward “ring-map mode” for the first time to describe a new data collection geometry. We begin with a brief description of the geometry model and discussion of its achievable image resolution. Then, an image formation processing flowchart based on sub-aperture processing strategy is proposed. Finally, simulation data processing verifies the effectiveness of the proposed approach.",Image Processing,signal process circulartrack ringmap sar equip multirotor uav,due limit radiu arc tradit groundbas arc sar often suffer poor azimuth resolut provid highresolut wideswath imag circulartrack ringmap sar equip multirotor uav platform propos paper paper put forward ringmap mode first time describ new data collect geometri begin brief descript geometri model discuss achiev imag resolut imag format process flowchart base subapertur process strategi propos final simul data process verifi effect propos approach
Utilization of Image Processing Strategy to Detect Crack on Walls,"The development of cracks on wall structures has drawn researchers' attention to work on an effective way to detect and resolve it. This paper proposes a method of detecting cracks on wall structure through a series of image processing. The series of image processing included: image conversion from Red, Green, Blue (RGB) model to grayscale; image enhancement; image segmentation; and noise removal. The simulation was run using MATLAB software. The results obtained showed the crack on a wall as well as its length in meters.",Image Processing,util imag process strategi detect crack wall,develop crack wall structur drawn research attent work effect way detect resolv paper propos method detect crack wall structur seri imag process seri imag process includ imag convers red green blue rgb model grayscal imag enhanc imag segment nois remov simul run use matlab softwar result obtain show crack wall well length meter
Research on Fire Detection and Image Information Processing System Based on Image Processing,"In this paper, based on digital image processing technology, a series of preprocessing operations such as graying based on color components, filtering denoising, histogram equalization and so on are carried out to enhance the image effect. Then, the suspicious fire area obtained based on color component difference image is processed by threshold segmentation and edge detection. In the processing of flame image, pattern recognition technology is of great significance to the extraction of flame image. Computer vision theory is the key to fire location. With this scheme, the system can effectively eliminate the interference of distance and light intensity in fire detection, and improve the recognition accuracy.",Image Processing,research fire detect imag inform process system base imag process,paper base digit imag process technolog seri preprocess oper gray base color compon filter denois histogram equal carri enhanc imag effect suspici fire area obtain base color compon differ imag process threshold segment edg detect process flame imag pattern recognit technolog great signific extract flame imag comput vision theori key fire locat scheme system effect elimin interfer distanc light intens fire detect improv recognit accuraci
Design of High-Speed Image Processing System for Weak-Dim Target Based on FPGA,"A high-speed image processing system for weak-dim target is proposed which is implemented with FPGA. In order to achieve the parallel of image acquisition and processing, the capabilities of the CMV4000, such as exposing the next frame during the read-out of the previous one, windowing and 16 data- output channels, are sued. Besides this, the ping-pang memory structure is adopted. This design achieves long exposure time and high image frequency in a imaging system based on large-scale plane array detector. Experimental results show that the exposure time of image sensor is about 20ms and the image frequency is 50Hz. The system can meet high speed detection and tracking requirements for weak-dim targets. Currently, the system has been applied to the actual project.",Image Processing,design highspe imag process system weakdim target base fpga,highspe imag process system weakdim target propos implement fpga order achiev parallel imag acquisit process capabl cmv4000 expos next frame readout previou one window 16 data output channel su besid pingpang memori structur adopt design achiev long exposur time high imag frequenc imag system base largescal plane array detector experiment result show exposur time imag sensor 20m imag frequenc 50hz system meet high speed detect track requir weakdim target current system appli actual project
Predicting CT Image From MRI Data Through Feature Matching With Learned Nonlinear Local Descriptors,"Attenuation correction for positron-emission tomography (PET)/magnetic resonance (MR) hybrid imaging systems and dose planning for MR-based radiation therapy remain challenging due to insufficient high-energy photon attenuation information. We present a novel approach that uses the learned nonlinear local descriptors and feature matching to predict pseudo computed tomography (pCT) images from T1-weighted and T2-weighted magnetic resonance imaging (MRI) data. The nonlinear local descriptors are obtained by projecting the linear descriptors into the nonlinear high-dimensional space using an explicit feature map and low-rank approximation with supervised manifold regularization. The nearest neighbors of each local descriptor in the input MR images are searched in a constrained spatial range of the MR images among the training dataset. Then the pCT patches are estimated through k-nearest neighbor regression. The proposed method for pCT prediction is quantitatively analyzed on a dataset consisting of paired brain MRI and CT images from 13 subjects. Our method generates pCT images with a mean absolute error (MAE) of 75.25 ± 18.05 Hounsfield units, a peak signal-to-noise ratio of 30.87 ± 1.15 dB, a relative MAE of 1.56 ± 0.5% in PET attenuation correction, and a dose relative structure volume difference of 0.055 ± 0.107% in D98%, as compared with true CT. The experimental results also show that our method outperforms four state-of-the-art methods.",Image Processing,predict ct imag mri data featur match learn nonlinear local descriptor,attenu correct positronemiss tomographi petmagnet reson mr hybrid imag system dose plan mrbase radiat therapi remain challeng due insuffici highenergi photon attenu inform present novel approach use learn nonlinear local descriptor featur match predict pseudo comput tomographi pct imag t1weight t2weight magnet reson imag mri data nonlinear local descriptor obtain project linear descriptor nonlinear highdimension space use explicit featur map lowrank approxim supervis manifold regular nearest neighbor local descriptor input mr imag search constrain spatial rang mr imag among train dataset pct patch estim knearest neighbor regress propos method pct predict quantit analyz dataset consist pair brain mri ct imag 13 subject method gener pct imag mean absolut error mae 7525 1805 hounsfield unit peak signaltonois ratio 3087 115 db rel mae 156 05 pet attenu correct dose rel structur volum differ 0055 0107 d98 compar true ct experiment result also show method outperform four stateoftheart method
Research on the application of the computer vision technology in image processing,"With the rapid development of the economy, various new image processing technologies have emerged. The staff uses computer image processing technology to process and design various images. This technology has been widely used in the field of image processing. This paper uses the computer vision plane image processing technology, which processes images through a variety of artistic materials, pictures, and design concepts, and finally realizes the fusion of text content, image content, and design goals. The paper designed and researched a computer vision plane image processing system, which can realize the automatic processing of the whole process of image arrangement, correction, and material integration in the process of image processing. The influence of image processing technology in image processing is of great significance.",Image Processing,research applic comput vision technolog imag process,rapid develop economi variou new imag process technolog emerg staff use comput imag process technolog process design variou imag technolog wide use field imag process paper use comput vision plane imag process technolog process imag varieti artist materi pictur design concept final realiz fusion text content imag content design goal paper design research comput vision plane imag process system realiz automat process whole process imag arrang correct materi integr process imag process influenc imag process technolog imag process great signific
Eggs Separation Process by Image Processing,"The main objective of this research is the process of sorting chicken eggs using image processing techniques. The objective is to measure the size of chicken eggs. To control the separation of chicken eggs in the experiment, the researchers used the Edge Detection technique. The experiment took pictures of chicken eggs of different sizes, and the system was able to separate the eggs correctly.",Image Processing,egg separ process imag process,main object research process sort chicken egg use imag process techniqu object measur size chicken egg control separ chicken egg experi research use edg detect techniqu experi took pictur chicken egg differ size system abl separ egg correctli
Massive High-dimensional Image Processing System Based on Big Data Technology,"In order to overcome the difficulty of massive high-dimensional image processing, a massive high-dimensional image processing system based on big data technology is proposed in this paper. The system fully combines the big data technology, analyzes the differences between the current massive high-dimensional image processing work and the traditional image processing work, and discusses the difficulties of the current massive high-dimensional image processing work. Based on this, the system also includes parallel image processing model and low delay dynamic processing model, which greatly improves the accuracy and efficiency of static image processing and dynamic image processing. The results demonstrate that the system has strong feasibility, and can indeed process large-scale high-dimensional image data and save the time and cost of data processing.",Image Processing,massiv highdimension imag process system base big data technolog,order overcom difficulti massiv highdimension imag process massiv highdimension imag process system base big data technolog propos paper system fulli combin big data technolog analyz differ current massiv highdimension imag process work tradit imag process work discuss difficulti current massiv highdimension imag process work base system also includ parallel imag process model low delay dynam process model greatli improv accuraci effici static imag process dynam imag process result demonstr system strong feasibl inde process largescal highdimension imag data save time cost data process
Comprehensive Comparison of Image Quality Aspects Between Conventional and Plane-Wave Imaging Methods on a Commercial Scanner,"Coherent plane-wave compound imaging (CPWCI) is used as alternative for conventional focused imaging (CFI) to increase frame rates linearly with the ratio number of imaging lines to steering angles. In this study, the image quality was compared between CPWCI and CFI, and the effect of steering angles (range and number) and beamforming strategies was evaluated in CPWCI. In automated breast volume scanners (ABVSs), which suffer from reduced volume rates, CPWCI might be an excellent candidate to replace CFI. Therefore, the image quality of CFI currently in ABVS and CPWCI was also compared in an in vivo breast lesion. Images were obtained by a Siemens Sequoia ultrasound system, and two transducers (14L5 and 10L4) in a CIRS multipurpose phantom (040GSE) and a breast lesion. Phantom results showed that contrast sensitivity and resolution, axial resolution, and generalized contrast-to-noise ratio (gCNR; imaging depths <45 mm) were similar for most imaging sequences. CNR (imaging depths ≥45 mm), penetration, and lateral resolution were significantly improved for CPWCI (15 angles) compared to CFI for both transducers. In CPWCI, certain combinations of steering angles and beamforming methods yielded improved gCNR (small angles and delay-and-sum) or lateral resolution (large angles and Lu’s-fk). Image quality seemed similar between CPWCI and CFI (three angles incoherent compounded as in ABVS) by visual inspection of the in vivo breast lesion images.",Image Processing,comprehens comparison imag qualiti aspect convent planewav imag method commerci scanner,coher planewav compound imag cpwci use altern convent focus imag cfi increas frame rate linearli ratio number imag line steer angl studi imag qualiti compar cpwci cfi effect steer angl rang number beamform strategi evalu cpwci autom breast volum scanner abvss suffer reduc volum rate cpwci might excel candid replac cfi therefor imag qualiti cfi current abv cpwci also compar vivo breast lesion imag obtain siemen sequoia ultrasound system two transduc 14l5 10l4 cir multipurpos phantom 040gse breast lesion phantom result show contrast sensit resolut axial resolut gener contrasttonois ratio gcnr imag depth 45 mm similar imag sequenc cnr imag depth 45 mm penetr later resolut significantli improv cpwci 15 angl compar cfi transduc cpwci certain combin steer angl beamform method yield improv gcnr small angl delayandsum later resolut larg angl lusfk imag qualiti seem similar cpwci cfi three angl incoher compound abv visual inspect vivo breast lesion imag
Application of Image Processing and Industrial Robot Arm for Quality Assurance Process of Production,"Recently, in the new era of highly developed technology, image processing is becoming a more efficient and required sector in the industry, enterprise, monitoring, and other various systems. All these applications lead us to research the image processing techniques in the quality assurance process of production with an ABB IRB 120 industrial robot arm. There are numerous types of image processing methods that are used in the quality assurance process of products. Hence, in our experiment, we attempted to distinguish the normal and abnormal products, which are replaced by 3D objects using the image processing system and ABB industrial robot arm. Therefore, we used the OpenCV library to enhance our image processing system. This paper presents the combination work of image processing methods and usages with modeling the end-effector of ABB IRB 120 industrial robot, their programming and experimental results.",Image Processing,applic imag process industri robot arm qualiti assur process product,recent new era highli develop technolog imag process becom effici requir sector industri enterpris monitor variou system applic lead us research imag process techniqu qualiti assur process product abb irb 120 industri robot arm numer type imag process method use qualiti assur process product henc experi attempt distinguish normal abnorm product replac 3d object use imag process system abb industri robot arm therefor use opencv librari enhanc imag process system paper present combin work imag process method usag model endeffector abb irb 120 industri robot program experiment result
A Simple Measure for Acuity in Medical Images,"An automatic and objective assessment of image quality is important in an era, where large-scale processing of imaging data from multi-center studies becomes commonplace. Based on a comprehensive statistical image model that includes noise and blur, a measure for image acuity is derived here as the ratio of the maximal gradient magnitude and the intensity difference at a boundary. Acuity may be affected by the object under study, the image acquisition, reconstruction processes, and any post-processing steps. The acuity measure presented here is post-hoc, intuitive to understand, simple to compute, and easily integrates with other standard measures of image quality. Three applications in medical imaging are included where our acuity measure is useful in the objective and automatic assessment of image quality.",Image Processing,simpl measur acuiti medic imag,automat object assess imag qualiti import era largescal process imag data multicent studi becom commonplac base comprehens statist imag model includ nois blur measur imag acuiti deriv ratio maxim gradient magnitud intens differ boundari acuiti may affect object studi imag acquisit reconstruct process postprocess step acuiti measur present posthoc intuit understand simpl comput easili integr standard measur imag qualiti three applic medic imag includ acuiti measur use object automat assess imag qualiti
Open source image processing module to improve dental patient care in suburban and rural areas: Review Paper,"The study paper showcases a detailed study of dental imaging can be possible only under the technology of open-source image processing modules as it involves the use of the latest image processing tools and devices for improving both the quality and accuracy of dental images. Dental radiography delivers significant indications for medical diagnosis, treatment and superiority valuation. Many determinations have been prepared to mature numeral X-ray image investigation organizations to progress medical eminence. This paper has presented records, techniques, and consequences for assessing the superiority of dental caution-consuming circumferential dental radiographs occupied earlier and afterwards the technique. Dental experimental eminence assessment comprises many procedures, such as statistics of yearly dental appointments of kids and grownups of dissimilar eternities, different caries between kids with vigorous caries, endodontics to abstraction events etc. This eminence extent evaluates dental scientific superiority. At the macro level, It is needed to escalate the micro level equally well, such as thoughtful whether the treatment assumed to distinct teeth is operative or not. The original and developing ground in orthodonture is dental informatics, owing to its possibility to recover treatment and analysis, while exchangeable time and plummeting nervousness and exhaustion throughout the regular repetition. The usage of computer curriculums can comfort dental specialists in the creation of conclusions associated with deterrence, identification, as well as cure schedule, surrounded by others. This study paper will also discuss the efficiency of Image processing tools that help a dentist and dental professionals improve the treatment quality.",Image Processing,open sourc imag process modul improv dental patient care suburban rural area review paper,studi paper showcas detail studi dental imag possibl technolog opensourc imag process modul involv use latest imag process tool devic improv qualiti accuraci dental imag dental radiographi deliv signific indic medic diagnosi treatment superior valuat mani determin prepar matur numer xray imag investig organ progress medic emin paper present record techniqu consequ assess superior dental cautionconsum circumferenti dental radiograph occupi earlier afterward techniqu dental experiment emin assess compris mani procedur statist yearli dental appoint kid grownup dissimilar etern differ cari kid vigor cari endodont abstract event etc emin extent evalu dental scientif superior macro level need escal micro level equal well thought whether treatment assum distinct teeth oper origin develop ground orthodontur dental informat owe possibl recov treatment analysi exchang time plummet nervous exhaust throughout regular repetit usag comput curriculum comfort dental specialist creation conclus associ deterr identif well cure schedul surround other studi paper also discuss effici imag process tool help dentist dental profession improv treatment qualiti
A Study of Image Processing for Dual Telecentric Optical System,"In this paper, the image processing technology of the double telecentric optical imaging system on the inner surface of the compensation hole is studied in depth, based on the theory and practice of image enhancement, image smoothing, image sharpening and image edge detection in spatial domain, a set of applicable image processing algorithms is integrated, it is applied to the inner surface of the compensation hole for practical testing, which proves the validity of the research content.",Image Processing,studi imag process dual telecentr optic system,paper imag process technolog doubl telecentr optic imag system inner surfac compens hole studi depth base theori practic imag enhanc imag smooth imag sharpen imag edg detect spatial domain set applic imag process algorithm integr appli inner surfac compens hole practic test prove valid research content
An Analysis of how Computer Graphics and Image Processing Are Used in Art Design,"In the era of high-speed information, with the widespread use of networks and the rapid advancement of multimedia communication technologies, human society has entered a significant new evolutionary stage. As a result of the informationization of society, human productivity and lifestyles have been greatly impacted. Recent advances have also been made in the area of computer-aided image processing. Photoshop has become a must-have software for computer graphics and image processing professionals. Rich functionality, ease of use, and humanization lead to increased labor productivity in processing computer images and photographs. Photoshop is a very effective tool for graphics and image processing techniques. The software is so robust in its image processing capabilities that it can produce the highest quality graphics and image processing. This technology is commonly used in the workplace. Professionals must act skilfully and understand technology in detail. Photoshop is widely used in graphic design and image editing these days. It is used in a variety of image processing contexts in addition to meeting the relevant processing needs of consumers.",Image Processing,analysi comput graphic imag process use art design,era highspe inform widespread use network rapid advanc multimedia commun technolog human societi enter signific new evolutionari stage result information societi human product lifestyl greatli impact recent advanc also made area computeraid imag process photoshop becom musthav softwar comput graphic imag process profession rich function eas use human lead increas labor product process comput imag photograph photoshop effect tool graphic imag process techniqu softwar robust imag process capabl produc highest qualiti graphic imag process technolog commonli use workplac profession must act skil understand technolog detail photoshop wide use graphic design imag edit day use varieti imag process context addit meet relev process need consum
Diverse Image Processing Techniques for Machine Learning Methods,"Machine learning is a trending topic in the area of computer vision, which makes the machine able to learn about it without being expressly programmed using various algorithms. When a model is designed to predict future-based on historical data using machine learning algorithms, data is vital because high quality data is needed to run the model efficiently and to predict highly accurate results. Machine learning algorithms receive data in many forms, like audio (speech recognition), images (feature recognition), numeric data (like height and weight) etc. When it comes to computer vision, the machine learning algorithms must be able to understand the image data, and before the images are processed to the algorithms it should be pre-processed (cleaned and set to the desired format) using various image pre-processing techniques. This research paper presents various image processing techniques, literature reviews on various image processing techniques and image PSNR value after applying median filter, grayscale filter and box filters.",Image Processing,divers imag process techniqu machin learn method,machin learn trend topic area comput vision make machin abl learn without expressli program use variou algorithm model design predict futurebas histor data use machin learn algorithm data vital high qualiti data need run model effici predict highli accur result machin learn algorithm receiv data mani form like audio speech recognit imag featur recognit numer data like height weight etc come comput vision machin learn algorithm must abl understand imag data imag process algorithm preprocess clean set desir format use variou imag preprocess techniqu research paper present variou imag process techniqu literatur review variou imag process techniqu imag psnr valu appli median filter grayscal filter box filter
Cross-Modality Image Adaptation Based on Volumetric Intensity Gaussian Process Models (VIGPM),"Image-based diagnosis routinely depends on more that one image modality for exploiting the complementary information they provide. However, it is not always possible to obtain images from a secondary modality for several reasons such as cost, degree of invasiveness and non-availability of scanners. Three-dimensional (3D) morphable models have made a significant contribution to the field of medical imaging for feature-based analysis. Here we extend their use to encode 3D volumetric imaging modalities. Specifically, we build a Gaussian Process (GP) over transformations establishing anatomical correspondence between training images within a modality. Given, two different modalities, the GP's eigenspace (latent space) can then be used to provide a parametric representation of each image modality, and we provide an operator for cross-domain translation between the two. We show that the latent space yields samples that are representative of the encoded modality. We also demonstrate that a 3D volumetric image can be efficiently encoded in latent space and transferred to synthesize the corresponding image in another modality. The framework called VIGPM can be extended by designing a fitting process to learn an observation in a given modality and performing cross-modality synthesis. Clinical Relevance— The proposed method provides a way to access a multi modality image from one modality. Both the source and synthetic modalities are in anatomical correspondence giving access to registered complementary information",Image Processing,crossmod imag adapt base volumetr intens gaussian process model vigpm,imagebas diagnosi routin depend one imag modal exploit complementari inform provid howev alway possibl obtain imag secondari modal sever reason cost degre invas nonavail scanner threedimension 3d morphabl model made signific contribut field medic imag featurebas analysi extend use encod 3d volumetr imag modal specif build gaussian process gp transform establish anatom correspond train imag within modal given two differ modal gp eigenspac latent space use provid parametr represent imag modal provid oper crossdomain translat two show latent space yield sampl repres encod modal also demonstr 3d volumetr imag effici encod latent space transfer synthes correspond imag anoth modal framework call vigpm extend design fit process learn observ given modal perform crossmod synthesi clinic relev propos method provid way access multi modal imag one modal sourc synthet modal anatom correspond give access regist complementari inform
Techniques and Applications of Image and Signal Processing : A Theoretical Approach,"This paper comprehensively overviews image and signal processing, including their fundamentals, advanced techniques, and applications. Image processing involves analyzing and manipulating digital images, while signal processing focuses on analyzing and interpreting signals in various domains. The fundamentals encompass digital signal representation, Fourier analysis, wavelet transforms, filtering, and noise removal. Advanced techniques, such as deep learning for image classification and object detection, are explored. Image and signal processing applications include computer vision, medical imaging, audio processing, and communications. This paper is a valuable resource for understanding image and signal processing principles and applications, fostering further research and development in these fields.",Image Processing,techniqu applic imag signal process theoret approach,paper comprehens overview imag signal process includ fundament advanc techniqu applic imag process involv analyz manipul digit imag signal process focus analyz interpret signal variou domain fundament encompass digit signal represent fourier analysi wavelet transform filter nois remov advanc techniqu deep learn imag classif object detect explor imag signal process applic includ comput vision medic imag audio process commun paper valuabl resourc understand imag signal process principl applic foster research develop field
A Survey of Medical Image Processing and its Applications,"At present time, medical field is rapidly broadening as a result of inventive image processing technologies. Medical images are exclusively important because they provide a pathway for disease diagnosis and therapeutic analysis in the healthcare profession. Medical image acquisition, image enhancement, segmentation, feature extraction, and classification are the five major processing steps in a general medical image processing system. Based on image processing methodologies, this article presents a comprehensive study on medical image analysis and its application on various forms of medical images. We also discuss the challenges that scholars face during successful execution and provide an overview of the ups and downs of existing algorithms. This case study highlights the key terminologies to be noted or the future challenges that are to be considered in the image processing techniques.",Image Processing,survey medic imag process applic,present time medic field rapidli broaden result invent imag process technolog medic imag exclus import provid pathway diseas diagnosi therapeut analysi healthcar profess medic imag acquisit imag enhanc segment featur extract classif five major process step gener medic imag process system base imag process methodolog articl present comprehens studi medic imag analysi applic variou form medic imag also discuss challeng scholar face success execut provid overview up down exist algorithm case studi highlight key terminolog note futur challeng consid imag process techniqu
Flexible Prediction of CT Images From MRI Data Through Improved Neighborhood Anchored Regression for PET Attenuation Correction,"Given the complicated relationship between the magnetic resonance imaging (MRI) signals and the attenuation values, the attenuation correction in hybrid positron emission tomography (PET)/MRI systems remains a challenging task. Currently, existing methods are either time-consuming or require sufficient samples to train the models. In this paper, an efficient approach for predicting pseudo computed tomography (CT) images from T1- and T2-weighted MRI data with limited data is proposed. The proposed approach uses improved neighborhood anchored regression (INAR) as a baseline method to pre-calculate projected matrices to flexibly predict the pseudo CT patches. Techniques, including the augmentation of the MR/CT dataset, learning of the nonlinear descriptors of MR images, hierarchical search for nearest neighbors, data-driven optimization, and multi-regressor ensemble, are adopted to improve the effectiveness of the proposed approach. In total, 22 healthy subjects were enrolled in the study. The pseudo CT images obtained using INAR with multi-regressor ensemble yielded mean absolute error (MAE) of 92.73 ± 14.86 HU, peak signal-to-noise ratio of 29.77 ± 1.63 dB, Pearson linear correlation coefficient of 0.82 ± 0.05, dice similarity coefficient of 0.81 ± 0.03, and the relative mean absolute error (rMAE) in PET attenuation correction of 1.30 ± 0.20% compared with true CT images. Moreover, our proposed INAR method, without any refinement strategies, can achieve considerable results with only seven subjects (MAE 106.89 ± 14.43 HU, rMAE 1.51 ± 0.21%). The experiments prove the superior performance of the proposed method over the six innovative methods. Moreover, the proposed method can rapidly generate the pseudo CT images that are suitable for PET attenuation correction.",Image Processing,flexibl predict ct imag mri data improv neighborhood anchor regress pet attenu correct,given complic relationship magnet reson imag mri signal attenu valu attenu correct hybrid positron emiss tomographi petmri system remain challeng task current exist method either timeconsum requir suffici sampl train model paper effici approach predict pseudo comput tomographi ct imag t1 t2weight mri data limit data propos propos approach use improv neighborhood anchor regress inar baselin method precalcul project matric flexibl predict pseudo ct patch techniqu includ augment mrct dataset learn nonlinear descriptor mr imag hierarch search nearest neighbor datadriven optim multiregressor ensembl adopt improv effect propos approach total 22 healthi subject enrol studi pseudo ct imag obtain use inar multiregressor ensembl yield mean absolut error mae 9273 1486 hu peak signaltonois ratio 2977 163 db pearson linear correl coeffici 082 005 dice similar coeffici 081 003 rel mean absolut error rmae pet attenu correct 130 020 compar true ct imag moreov propos inar method without refin strategi achiev consider result seven subject mae 10689 1443 hu rmae 151 021 experi prove superior perform propos method six innov method moreov propos method rapidli gener pseudo ct imag suitabl pet attenu correct
Review on Digital Image Processing Techniques for Face Recognition,"In the world of advancement, security is an important aspect of everyone's life. Personal data is easily accessed by hackers. This kind of problem can be reduced by using biometric access. Fingerprint, voice recognition, iris recognition, face recognition, and dental data is the few biometric techniques that provide end to end security between the user and service provider. Digital image processing plays a major role in this biometric recognition technique. Advancement in the digital image processing technique with the intellectual computer allows the researcher to work on this recognition technique. All recognition techniques have its merit and demerits. Face recognition is the most preferred technique which is not only providing secure access and also helpful to limit unauthorized access. Nowadays, social sites are using this technique to identify the user. If one photo is uploaded on the website immediately it matches the images with the database and helps the user to tag their friends. Advanced applications help the user by capturing their mood and play the songs, images, and so on. In this work, the basic image processing techniques to advance image processing techniques in face recognition are presented in a meaningful way. It will be useful for beginners, and researchers who want to work in face recognition techniques.",Image Processing,review digit imag process techniqu face recognit,world advanc secur import aspect everyon life person data easili access hacker kind problem reduc use biometr access fingerprint voic recognit iri recognit face recognit dental data biometr techniqu provid end end secur user servic provid digit imag process play major role biometr recognit techniqu advanc digit imag process techniqu intellectu comput allow research work recognit techniqu recognit techniqu merit demerit face recognit prefer techniqu provid secur access also help limit unauthor access nowaday social site use techniqu identifi user one photo upload websit immedi match imag databas help user tag friend advanc applic help user captur mood play song imag work basic imag process techniqu advanc imag process techniqu face recognit present meaning way use beginn research want work face recognit techniqu
Enumeration of Ampicillin-Resistant E. Coli in Blood Using Droplet Microfluidics and High-Speed Image Processing,"Bacteria entering the bloodstream causes bloodstream infection (BSI). Without proper treatment, BSI can lead to sepsis which is a life-threatening condition. Detection of bacteria in blood at the early stages of BSI can effectively prevent the development of sepsis. Using microfluidic droplets for single bacterium encapsulation provides single-digit bacterial detection sensitivity. In this study, samples of ampicillin-resistant E. coli in human blood were partitioned into millions of 30 μm diameter microfluidic droplets and followed by 8-hour culturing. Thousands of fluorescent bacteria from a single colony filled up the positive droplets after the culturing process. A circle detection software based on Hough Transform was developed to count the number of positive droplets from fluorescence images. The period to process one image can be as short as 0.5 ms when the original image is pre-processed and binarized by the developed software.",Image Processing,enumer ampicillinresist e coli blood use droplet microfluid highspe imag process,bacteria enter bloodstream caus bloodstream infect bsi without proper treatment bsi lead sepsi lifethreaten condit detect bacteria blood earli stage bsi effect prevent develop sepsi use microfluid droplet singl bacterium encapsul provid singledigit bacteri detect sensit studi sampl ampicillinresist e coli human blood partit million 30 μm diamet microfluid droplet follow 8hour cultur thousand fluoresc bacteria singl coloni fill posit droplet cultur process circl detect softwar base hough transform develop count number posit droplet fluoresc imag period process one imag short 05 ms origin imag preprocess binar develop softwar
